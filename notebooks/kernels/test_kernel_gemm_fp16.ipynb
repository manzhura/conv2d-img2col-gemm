{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ecfb1a-e3a2-430c-9867-2612ee97b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e22470-ab3a-43e9-94cc-a9cdd443e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from conv_gemm.triton_kernels.fp16.gemm_kernel import triton_gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21629dba-d2a7-4564-979d-fcd01657e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_gemm_fp16_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "\n",
    "    A_f16 = torch.randn(M, K, device=device, dtype=torch.float16)\n",
    "    B_f16 = torch.randn(K, N, device=device, dtype=torch.float16)\n",
    "\n",
    "    # Torch FP16 GEMM\n",
    "    def _call_torch():\n",
    "        return (A_f16 @ B_f16).float()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "\n",
    "    # Triton FP16 GEMM\n",
    "    def _call_triton():\n",
    "        return triton_gemm(\n",
    "            A_f16, B_f16,\n",
    "            use_fp16=True,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "\n",
    "    for _ in range(5):\n",
    "        _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_tr = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Bandwidth\n",
    "    bytes_moved = (\n",
    "        A_f16.numel() * 2 +\n",
    "        B_f16.numel() * 2 +\n",
    "        C_tr.numel() * 4\n",
    "    )\n",
    "    bytes_moved = float(bytes_moved)\n",
    "\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a664907-fec2-4aeb-ad0a-9dd1cff3825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_gemm_fp16_tiles_for_shape(\n",
    "    M, K, N,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3, 4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BN in blocks_N:\n",
    "            for BK in blocks_K:\n",
    "                for W in warps:\n",
    "                    for S in stages:\n",
    "\n",
    "                        try:\n",
    "                            rec = bench_once_gemm_fp16_vs_torch(\n",
    "                                M, K, N,\n",
    "                                BLOCK_M=BM,\n",
    "                                BLOCK_N=BN,\n",
    "                                BLOCK_K=BK,\n",
    "                                num_warps=W,\n",
    "                                num_stages=S,\n",
    "                                iters=iters,\n",
    "                                device=device,\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        print(\n",
    "                            f\"BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: \"\n",
    "                            f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                            f\"speed_vs_torch={rec['speed_vs_torch']:.3f}x, \"\n",
    "                        )\n",
    "                        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this GEMM FP16 shape\")\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1144ba-76a6-4572-ba6b-bf11bfdaac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BN=32, BK=32, W=2, S=2: t_triton=0.340 ms, speed_vs_torch=0.920x, \n",
      "BM=32, BN=32, BK=32, W=2, S=3: t_triton=0.462 ms, speed_vs_torch=0.608x, \n",
      "BM=32, BN=32, BK=32, W=4, S=2: t_triton=0.340 ms, speed_vs_torch=0.789x, \n",
      "BM=32, BN=32, BK=32, W=4, S=3: t_triton=0.450 ms, speed_vs_torch=0.597x, \n",
      "BM=32, BN=32, BK=32, W=8, S=2: t_triton=0.472 ms, speed_vs_torch=0.563x, \n",
      "BM=32, BN=32, BK=32, W=8, S=3: t_triton=0.549 ms, speed_vs_torch=0.486x, \n",
      "BM=32, BN=32, BK=64, W=2, S=2: t_triton=0.321 ms, speed_vs_torch=0.870x, \n",
      "BM=32, BN=32, BK=64, W=2, S=3: t_triton=0.353 ms, speed_vs_torch=0.776x, \n",
      "BM=32, BN=32, BK=64, W=4, S=2: t_triton=0.344 ms, speed_vs_torch=0.788x, \n",
      "BM=32, BN=32, BK=64, W=4, S=3: t_triton=0.336 ms, speed_vs_torch=0.807x, \n",
      "BM=32, BN=32, BK=64, W=8, S=2: t_triton=0.357 ms, speed_vs_torch=0.772x, \n",
      "BM=32, BN=32, BK=64, W=8, S=3: t_triton=0.362 ms, speed_vs_torch=0.762x, \n",
      "BM=32, BN=32, BK=128, W=2, S=2: t_triton=0.415 ms, speed_vs_torch=0.663x, \n",
      "BM=32, BN=32, BK=128, W=2, S=3: t_triton=0.506 ms, speed_vs_torch=0.532x, \n",
      "BM=32, BN=32, BK=128, W=4, S=2: t_triton=0.386 ms, speed_vs_torch=0.707x, \n",
      "BM=32, BN=32, BK=128, W=4, S=3: t_triton=0.539 ms, speed_vs_torch=0.502x, \n",
      "BM=32, BN=32, BK=128, W=8, S=2: t_triton=0.430 ms, speed_vs_torch=0.688x, \n",
      "BM=32, BN=32, BK=128, W=8, S=3: t_triton=0.516 ms, speed_vs_torch=0.523x, \n",
      "BM=32, BN=64, BK=32, W=2, S=2: t_triton=0.278 ms, speed_vs_torch=0.959x, \n",
      "BM=32, BN=64, BK=32, W=2, S=3: t_triton=0.260 ms, speed_vs_torch=1.040x, \n",
      "BM=32, BN=64, BK=32, W=4, S=2: t_triton=0.285 ms, speed_vs_torch=1.023x, \n",
      "BM=32, BN=64, BK=32, W=4, S=3: t_triton=0.262 ms, speed_vs_torch=1.125x, \n",
      "BM=32, BN=64, BK=32, W=8, S=2: t_triton=0.319 ms, speed_vs_torch=0.864x, \n",
      "BM=32, BN=64, BK=32, W=8, S=3: t_triton=0.312 ms, speed_vs_torch=0.929x, \n",
      "BM=32, BN=64, BK=64, W=2, S=2: t_triton=0.265 ms, speed_vs_torch=1.038x, \n",
      "BM=32, BN=64, BK=64, W=2, S=3: t_triton=0.283 ms, speed_vs_torch=1.029x, \n",
      "BM=32, BN=64, BK=64, W=4, S=2: t_triton=0.260 ms, speed_vs_torch=1.124x, \n",
      "BM=32, BN=64, BK=64, W=4, S=3: t_triton=0.284 ms, speed_vs_torch=0.993x, \n",
      "BM=32, BN=64, BK=64, W=8, S=2: t_triton=0.283 ms, speed_vs_torch=1.047x, \n",
      "BM=32, BN=64, BK=64, W=8, S=3: t_triton=0.269 ms, speed_vs_torch=1.023x, \n",
      "BM=32, BN=64, BK=128, W=2, S=2: t_triton=0.290 ms, speed_vs_torch=0.948x, \n",
      "BM=32, BN=64, BK=128, W=2, S=3: t_triton=0.306 ms, speed_vs_torch=0.895x, \n",
      "BM=32, BN=64, BK=128, W=4, S=2: t_triton=0.303 ms, speed_vs_torch=0.952x, \n",
      "BM=32, BN=64, BK=128, W=4, S=3: t_triton=0.278 ms, speed_vs_torch=1.033x, \n",
      "BM=32, BN=64, BK=128, W=8, S=2: t_triton=0.279 ms, speed_vs_torch=0.973x, \n",
      "BM=32, BN=64, BK=128, W=8, S=3: t_triton=0.279 ms, speed_vs_torch=1.002x, \n",
      "BM=32, BN=128, BK=32, W=2, S=2: t_triton=0.240 ms, speed_vs_torch=1.142x, \n",
      "BM=32, BN=128, BK=32, W=2, S=3: t_triton=0.229 ms, speed_vs_torch=1.178x, \n",
      "BM=32, BN=128, BK=32, W=4, S=2: t_triton=0.229 ms, speed_vs_torch=1.175x, \n",
      "BM=32, BN=128, BK=32, W=4, S=3: t_triton=0.230 ms, speed_vs_torch=1.196x, \n",
      "BM=32, BN=128, BK=32, W=8, S=2: t_triton=0.267 ms, speed_vs_torch=1.015x, \n",
      "BM=32, BN=128, BK=32, W=8, S=3: t_triton=0.237 ms, speed_vs_torch=1.137x, \n",
      "BM=32, BN=128, BK=64, W=2, S=2: t_triton=0.257 ms, speed_vs_torch=1.077x, \n",
      "BM=32, BN=128, BK=64, W=2, S=3: t_triton=0.258 ms, speed_vs_torch=1.044x, \n",
      "BM=32, BN=128, BK=64, W=4, S=2: t_triton=0.246 ms, speed_vs_torch=1.097x, \n",
      "BM=32, BN=128, BK=64, W=4, S=3: t_triton=0.232 ms, speed_vs_torch=1.179x, \n",
      "BM=32, BN=128, BK=64, W=8, S=2: t_triton=0.233 ms, speed_vs_torch=1.154x, \n",
      "BM=32, BN=128, BK=64, W=8, S=3: t_triton=0.234 ms, speed_vs_torch=1.180x, \n",
      "BM=32, BN=128, BK=128, W=2, S=2: t_triton=0.337 ms, speed_vs_torch=0.803x, \n",
      "BM=32, BN=128, BK=128, W=2, S=3: t_triton=0.560 ms, speed_vs_torch=0.476x, \n",
      "BM=32, BN=128, BK=128, W=4, S=2: t_triton=0.286 ms, speed_vs_torch=0.962x, \n",
      "BM=32, BN=128, BK=128, W=4, S=3: t_triton=0.306 ms, speed_vs_torch=0.872x, \n",
      "BM=32, BN=128, BK=128, W=8, S=2: t_triton=0.281 ms, speed_vs_torch=0.950x, \n",
      "BM=32, BN=128, BK=128, W=8, S=3: t_triton=0.283 ms, speed_vs_torch=0.950x, \n",
      "BM=64, BN=32, BK=32, W=2, S=2: t_triton=0.297 ms, speed_vs_torch=0.897x, \n",
      "BM=64, BN=32, BK=32, W=2, S=3: t_triton=0.403 ms, speed_vs_torch=0.669x, \n",
      "BM=64, BN=32, BK=32, W=4, S=2: t_triton=0.283 ms, speed_vs_torch=0.947x, \n",
      "BM=64, BN=32, BK=32, W=4, S=3: t_triton=0.380 ms, speed_vs_torch=0.719x, \n",
      "BM=64, BN=32, BK=32, W=8, S=2: t_triton=0.311 ms, speed_vs_torch=0.868x, \n",
      "BM=64, BN=32, BK=32, W=8, S=3: t_triton=0.315 ms, speed_vs_torch=0.863x, \n",
      "BM=64, BN=32, BK=64, W=2, S=2: t_triton=0.335 ms, speed_vs_torch=0.820x, \n",
      "BM=64, BN=32, BK=64, W=2, S=3: t_triton=0.342 ms, speed_vs_torch=0.793x, \n",
      "BM=64, BN=32, BK=64, W=4, S=2: t_triton=0.309 ms, speed_vs_torch=0.868x, \n",
      "BM=64, BN=32, BK=64, W=4, S=3: t_triton=0.287 ms, speed_vs_torch=0.942x, \n",
      "BM=64, BN=32, BK=64, W=8, S=2: t_triton=0.293 ms, speed_vs_torch=0.931x, \n",
      "BM=64, BN=32, BK=64, W=8, S=3: t_triton=0.302 ms, speed_vs_torch=0.900x, \n",
      "BM=64, BN=32, BK=128, W=2, S=2: t_triton=0.351 ms, speed_vs_torch=0.781x, \n",
      "BM=64, BN=32, BK=128, W=2, S=3: t_triton=0.435 ms, speed_vs_torch=0.614x, \n",
      "BM=64, BN=32, BK=128, W=4, S=2: t_triton=0.346 ms, speed_vs_torch=0.771x, \n",
      "BM=64, BN=32, BK=128, W=4, S=3: t_triton=0.455 ms, speed_vs_torch=0.593x, \n",
      "BM=64, BN=32, BK=128, W=8, S=2: t_triton=0.340 ms, speed_vs_torch=0.785x, \n",
      "BM=64, BN=32, BK=128, W=8, S=3: t_triton=0.440 ms, speed_vs_torch=0.614x, \n",
      "BM=64, BN=64, BK=32, W=2, S=2: t_triton=0.249 ms, speed_vs_torch=1.070x, \n",
      "BM=64, BN=64, BK=32, W=2, S=3: t_triton=0.273 ms, speed_vs_torch=0.980x, \n",
      "BM=64, BN=64, BK=32, W=4, S=2: t_triton=0.280 ms, speed_vs_torch=0.956x, \n",
      "BM=64, BN=64, BK=32, W=4, S=3: t_triton=0.254 ms, speed_vs_torch=1.051x, \n",
      "BM=64, BN=64, BK=32, W=8, S=2: t_triton=0.251 ms, speed_vs_torch=1.076x, \n",
      "BM=64, BN=64, BK=32, W=8, S=3: t_triton=0.276 ms, speed_vs_torch=0.999x, \n",
      "BM=64, BN=64, BK=64, W=2, S=2: t_triton=0.263 ms, speed_vs_torch=1.029x, \n",
      "BM=64, BN=64, BK=64, W=2, S=3: t_triton=0.281 ms, speed_vs_torch=0.954x, \n",
      "BM=64, BN=64, BK=64, W=4, S=2: t_triton=0.266 ms, speed_vs_torch=1.003x, \n",
      "BM=64, BN=64, BK=64, W=4, S=3: t_triton=0.271 ms, speed_vs_torch=0.990x, \n",
      "BM=64, BN=64, BK=64, W=8, S=2: t_triton=0.263 ms, speed_vs_torch=1.018x, \n",
      "BM=64, BN=64, BK=64, W=8, S=3: t_triton=0.269 ms, speed_vs_torch=1.012x, \n",
      "BM=64, BN=64, BK=128, W=2, S=2: t_triton=0.292 ms, speed_vs_torch=0.925x, \n",
      "BM=64, BN=64, BK=128, W=2, S=3: t_triton=0.494 ms, speed_vs_torch=0.542x, \n",
      "BM=64, BN=64, BK=128, W=4, S=2: t_triton=0.273 ms, speed_vs_torch=1.013x, \n",
      "BM=64, BN=64, BK=128, W=4, S=3: t_triton=0.294 ms, speed_vs_torch=0.909x, \n",
      "BM=64, BN=64, BK=128, W=8, S=2: t_triton=0.282 ms, speed_vs_torch=0.947x, \n",
      "BM=64, BN=64, BK=128, W=8, S=3: t_triton=0.282 ms, speed_vs_torch=0.949x, \n",
      "BM=64, BN=128, BK=32, W=2, S=2: t_triton=0.245 ms, speed_vs_torch=1.094x, \n",
      "BM=64, BN=128, BK=32, W=2, S=3: t_triton=0.238 ms, speed_vs_torch=1.144x, \n",
      "BM=64, BN=128, BK=32, W=4, S=2: t_triton=0.229 ms, speed_vs_torch=1.169x, \n",
      "BM=64, BN=128, BK=32, W=4, S=3: t_triton=0.213 ms, speed_vs_torch=1.306x, \n",
      "BM=64, BN=128, BK=32, W=8, S=2: t_triton=0.241 ms, speed_vs_torch=1.119x, \n",
      "BM=64, BN=128, BK=32, W=8, S=3: t_triton=0.231 ms, speed_vs_torch=1.191x, \n",
      "BM=64, BN=128, BK=64, W=2, S=2: t_triton=0.260 ms, speed_vs_torch=1.026x, \n",
      "BM=64, BN=128, BK=64, W=2, S=3: t_triton=0.269 ms, speed_vs_torch=1.037x, \n",
      "BM=64, BN=128, BK=64, W=4, S=2: t_triton=0.228 ms, speed_vs_torch=1.173x, \n",
      "BM=64, BN=128, BK=64, W=4, S=3: t_triton=0.221 ms, speed_vs_torch=1.220x, \n",
      "BM=64, BN=128, BK=64, W=8, S=2: t_triton=0.241 ms, speed_vs_torch=1.117x, \n",
      "BM=64, BN=128, BK=64, W=8, S=3: t_triton=0.229 ms, speed_vs_torch=1.205x, \n",
      "BM=64, BN=128, BK=128, W=2, S=2: t_triton=0.378 ms, speed_vs_torch=0.716x, \n",
      "BM=64, BN=128, BK=128, W=2, S=3: t_triton=0.502 ms, speed_vs_torch=0.536x, \n",
      "BM=64, BN=128, BK=128, W=4, S=2: t_triton=0.259 ms, speed_vs_torch=1.042x, \n",
      "BM=64, BN=128, BK=128, W=4, S=3: t_triton=0.266 ms, speed_vs_torch=1.004x, \n",
      "BM=64, BN=128, BK=128, W=8, S=2: t_triton=0.291 ms, speed_vs_torch=0.924x, \n",
      "BM=64, BN=128, BK=128, W=8, S=3: t_triton=0.251 ms, speed_vs_torch=1.120x, \n",
      "BM=128, BN=32, BK=32, W=2, S=2: t_triton=0.259 ms, speed_vs_torch=1.031x, \n",
      "BM=128, BN=32, BK=32, W=2, S=3: t_triton=0.275 ms, speed_vs_torch=1.107x, \n",
      "BM=128, BN=32, BK=32, W=4, S=2: t_triton=0.323 ms, speed_vs_torch=0.840x, \n",
      "BM=128, BN=32, BK=32, W=4, S=3: t_triton=0.318 ms, speed_vs_torch=0.842x, \n",
      "BM=128, BN=32, BK=32, W=8, S=2: t_triton=0.322 ms, speed_vs_torch=0.837x, \n",
      "BM=128, BN=32, BK=32, W=8, S=3: t_triton=0.338 ms, speed_vs_torch=0.791x, \n",
      "BM=128, BN=32, BK=64, W=2, S=2: t_triton=0.251 ms, speed_vs_torch=1.073x, \n",
      "BM=128, BN=32, BK=64, W=2, S=3: t_triton=0.280 ms, speed_vs_torch=0.961x, \n",
      "BM=128, BN=32, BK=64, W=4, S=2: t_triton=0.394 ms, speed_vs_torch=0.679x, \n",
      "BM=128, BN=32, BK=64, W=4, S=3: t_triton=0.368 ms, speed_vs_torch=0.725x, \n",
      "BM=128, BN=32, BK=64, W=8, S=2: t_triton=0.353 ms, speed_vs_torch=0.754x, \n",
      "BM=128, BN=32, BK=64, W=8, S=3: t_triton=0.387 ms, speed_vs_torch=0.693x, \n",
      "BM=128, BN=32, BK=128, W=2, S=2: t_triton=0.367 ms, speed_vs_torch=0.726x, \n",
      "BM=128, BN=32, BK=128, W=2, S=3: t_triton=0.521 ms, speed_vs_torch=0.515x, \n",
      "BM=128, BN=32, BK=128, W=4, S=2: t_triton=0.340 ms, speed_vs_torch=0.800x, \n",
      "BM=128, BN=32, BK=128, W=4, S=3: t_triton=0.414 ms, speed_vs_torch=0.645x, \n",
      "BM=128, BN=32, BK=128, W=8, S=2: t_triton=0.379 ms, speed_vs_torch=0.739x, \n",
      "BM=128, BN=32, BK=128, W=8, S=3: t_triton=0.446 ms, speed_vs_torch=0.661x, \n",
      "BM=128, BN=64, BK=32, W=2, S=2: t_triton=0.253 ms, speed_vs_torch=1.060x, \n",
      "BM=128, BN=64, BK=32, W=2, S=3: t_triton=0.243 ms, speed_vs_torch=1.101x, \n",
      "BM=128, BN=64, BK=32, W=4, S=2: t_triton=0.240 ms, speed_vs_torch=1.126x, \n",
      "BM=128, BN=64, BK=32, W=4, S=3: t_triton=0.224 ms, speed_vs_torch=1.243x, \n",
      "BM=128, BN=64, BK=32, W=8, S=2: t_triton=0.246 ms, speed_vs_torch=1.097x, \n",
      "BM=128, BN=64, BK=32, W=8, S=3: t_triton=0.234 ms, speed_vs_torch=1.163x, \n",
      "BM=128, BN=64, BK=64, W=2, S=2: t_triton=0.253 ms, speed_vs_torch=1.063x, \n",
      "BM=128, BN=64, BK=64, W=2, S=3: t_triton=0.262 ms, speed_vs_torch=1.040x, \n",
      "BM=128, BN=64, BK=64, W=4, S=2: t_triton=0.242 ms, speed_vs_torch=1.104x, \n",
      "BM=128, BN=64, BK=64, W=4, S=3: t_triton=0.225 ms, speed_vs_torch=1.212x, \n",
      "BM=128, BN=64, BK=64, W=8, S=2: t_triton=0.250 ms, speed_vs_torch=1.073x, \n",
      "BM=128, BN=64, BK=64, W=8, S=3: t_triton=0.223 ms, speed_vs_torch=1.220x, \n",
      "BM=128, BN=64, BK=128, W=2, S=2: t_triton=0.383 ms, speed_vs_torch=0.719x, \n",
      "BM=128, BN=64, BK=128, W=2, S=3: t_triton=0.539 ms, speed_vs_torch=0.511x, \n",
      "BM=128, BN=64, BK=128, W=4, S=2: t_triton=0.241 ms, speed_vs_torch=1.138x, \n",
      "BM=128, BN=64, BK=128, W=4, S=3: t_triton=0.265 ms, speed_vs_torch=1.007x, \n",
      "BM=128, BN=64, BK=128, W=8, S=2: t_triton=0.255 ms, speed_vs_torch=1.048x, \n",
      "BM=128, BN=64, BK=128, W=8, S=3: t_triton=0.262 ms, speed_vs_torch=1.045x, \n",
      "BM=128, BN=128, BK=32, W=2, S=2: t_triton=1.207 ms, speed_vs_torch=0.222x, \n",
      "BM=128, BN=128, BK=32, W=2, S=3: t_triton=1.172 ms, speed_vs_torch=0.229x, \n",
      "BM=128, BN=128, BK=32, W=4, S=2: t_triton=0.247 ms, speed_vs_torch=1.081x, \n",
      "BM=128, BN=128, BK=32, W=4, S=3: t_triton=0.236 ms, speed_vs_torch=1.173x, \n",
      "BM=128, BN=128, BK=32, W=8, S=2: t_triton=0.244 ms, speed_vs_torch=1.130x, \n",
      "BM=128, BN=128, BK=32, W=8, S=3: t_triton=0.237 ms, speed_vs_torch=1.166x, \n",
      "BM=128, BN=128, BK=64, W=2, S=2: t_triton=1.215 ms, speed_vs_torch=0.230x, \n",
      "BM=128, BN=128, BK=64, W=2, S=3: t_triton=1.021 ms, speed_vs_torch=0.267x, \n",
      "BM=128, BN=128, BK=64, W=4, S=2: t_triton=0.244 ms, speed_vs_torch=1.094x, \n",
      "BM=128, BN=128, BK=64, W=4, S=3: t_triton=0.253 ms, speed_vs_torch=1.057x, \n",
      "BM=128, BN=128, BK=64, W=8, S=2: t_triton=0.297 ms, speed_vs_torch=0.934x, \n",
      "BM=128, BN=128, BK=64, W=8, S=3: t_triton=0.250 ms, speed_vs_torch=1.081x, \n",
      "BM=128, BN=128, BK=128, W=2, S=2: t_triton=1.374 ms, speed_vs_torch=0.198x, \n",
      "[SKIP] BM=128, BN=128, BK=128, W=2, S=3: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "BM=128, BN=128, BK=128, W=4, S=2: t_triton=0.347 ms, speed_vs_torch=0.780x, \n",
      "[SKIP] BM=128, BN=128, BK=128, W=4, S=3: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "BM=128, BN=128, BK=128, W=8, S=2: t_triton=0.307 ms, speed_vs_torch=0.891x, \n",
      "[SKIP] BM=128, BN=128, BK=128, W=8, S=3: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n"
     ]
    }
   ],
   "source": [
    "df_fp16 = tune_gemm_fp16_tiles_for_shape(\n",
    "    M=4096, K=1024, N=1024,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c28f971-b3a5-4d09-92a7-9c84689da03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch_ms</th>\n",
       "      <th>speed_vs_torch</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_torch_GBs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.212696</td>\n",
       "      <td>0.277762</td>\n",
       "      <td>1.305912</td>\n",
       "      <td>128.178156</td>\n",
       "      <td>98.152237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.221131</td>\n",
       "      <td>0.269691</td>\n",
       "      <td>1.219598</td>\n",
       "      <td>123.288768</td>\n",
       "      <td>101.089643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.223396</td>\n",
       "      <td>0.272501</td>\n",
       "      <td>1.219810</td>\n",
       "      <td>122.038595</td>\n",
       "      <td>100.047226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.223889</td>\n",
       "      <td>0.278304</td>\n",
       "      <td>1.243042</td>\n",
       "      <td>121.769980</td>\n",
       "      <td>97.961284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.225016</td>\n",
       "      <td>0.272817</td>\n",
       "      <td>1.212434</td>\n",
       "      <td>121.159951</td>\n",
       "      <td>99.931194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.227905</td>\n",
       "      <td>0.267245</td>\n",
       "      <td>1.172617</td>\n",
       "      <td>119.624494</td>\n",
       "      <td>102.014990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.228575</td>\n",
       "      <td>0.267142</td>\n",
       "      <td>1.168728</td>\n",
       "      <td>119.273622</td>\n",
       "      <td>102.054244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.229122</td>\n",
       "      <td>0.270004</td>\n",
       "      <td>1.178428</td>\n",
       "      <td>118.988851</td>\n",
       "      <td>100.972493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.229209</td>\n",
       "      <td>0.269264</td>\n",
       "      <td>1.174753</td>\n",
       "      <td>118.943754</td>\n",
       "      <td>101.250053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.276581</td>\n",
       "      <td>1.205188</td>\n",
       "      <td>118.796873</td>\n",
       "      <td>98.571244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        M     K     N  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "93   4096  1024  1024       64      128       32          4           3   \n",
       "99   4096  1024  1024       64      128       64          4           3   \n",
       "137  4096  1024  1024      128       64       64          8           3   \n",
       "129  4096  1024  1024      128       64       32          4           3   \n",
       "135  4096  1024  1024      128       64       64          4           3   \n",
       "98   4096  1024  1024       64      128       64          4           2   \n",
       "92   4096  1024  1024       64      128       32          4           2   \n",
       "37   4096  1024  1024       32      128       32          2           3   \n",
       "38   4096  1024  1024       32      128       32          4           2   \n",
       "101  4096  1024  1024       64      128       64          8           3   \n",
       "\n",
       "     t_triton_ms  t_torch_ms  speed_vs_torch  bw_triton_GBs  bw_torch_GBs  \n",
       "93      0.212696    0.277762        1.305912     128.178156     98.152237  \n",
       "99      0.221131    0.269691        1.219598     123.288768    101.089643  \n",
       "137     0.223396    0.272501        1.219810     122.038595    100.047226  \n",
       "129     0.223889    0.278304        1.243042     121.769980     97.961284  \n",
       "135     0.225016    0.272817        1.212434     121.159951     99.931194  \n",
       "98      0.227905    0.267245        1.172617     119.624494    102.014990  \n",
       "92      0.228575    0.267142        1.168728     119.273622    102.054244  \n",
       "37      0.229122    0.270004        1.178428     118.988851    100.972493  \n",
       "38      0.229209    0.269264        1.174753     118.943754    101.250053  \n",
       "101     0.229492    0.276581        1.205188     118.796873     98.571244  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp16.sort_values(\"t_triton_ms\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caaa2e59-6fd1-46a2-aa6e-f6239acef9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP16_GEMM_BEST_BLOCK_M = 64\n",
    "FP16_GEMM_BEST_BLOCK_N = 128\n",
    "FP16_GEMM_BEST_BLOCK_K = 32\n",
    "FP16_GEMM_BEST_WARPS   = 4\n",
    "FP16_GEMM_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170a898-0db3-43db-94b8-7b2eeeed7b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
