{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ecfb1a-e3a2-430c-9867-2612ee97b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e22470-ab3a-43e9-94cc-a9cdd443e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from conv_gemm.triton_kernels.fp16.gemm_kernel import triton_gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21629dba-d2a7-4564-979d-fcd01657e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_gemm_fp16_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "\n",
    "    A_f16 = torch.randn(M, K, device=device, dtype=torch.float16)\n",
    "    B_f16 = torch.randn(K, N, device=device, dtype=torch.float16)\n",
    "\n",
    "    # Torch FP16 GEMM\n",
    "    def _call_torch():\n",
    "        return (A_f16 @ B_f16).float()\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Triton FP16 GEMM\n",
    "    def _call_triton():\n",
    "        return triton_gemm(\n",
    "            A_f16, B_f16,\n",
    "            use_fp16=True,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "\n",
    "    for _ in range(5):\n",
    "        C_tr = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_tr = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "\n",
    "    diff = (C_tr - C_ref).abs()\n",
    "    max_abs_err16 = diff.max().item()\n",
    "    mean_abs_err16 = diff.mean().item()\n",
    "\n",
    "    \n",
    "    # Bandwidth\n",
    "    bytes_moved = (\n",
    "        A_f16.numel() * 2 +\n",
    "        B_f16.numel() * 2 +\n",
    "        C_tr.numel() * 4\n",
    "    )\n",
    "    bytes_moved = float(bytes_moved)\n",
    "\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch16_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch16\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err16,\n",
    "        \"mean_abs_err\": mean_abs_err16,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a664907-fec2-4aeb-ad0a-9dd1cff3825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_gemm_fp16_tiles_for_shape(\n",
    "    M, K, N,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3, 4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BN in blocks_N:\n",
    "            for BK in blocks_K:\n",
    "                for W in warps:\n",
    "                    for S in stages:\n",
    "\n",
    "                        try:\n",
    "                            rec = bench_once_gemm_fp16_vs_torch(\n",
    "                                M, K, N,\n",
    "                                BLOCK_M=BM,\n",
    "                                BLOCK_N=BN,\n",
    "                                BLOCK_K=BK,\n",
    "                                num_warps=W,\n",
    "                                num_stages=S,\n",
    "                                iters=iters,\n",
    "                                device=device,\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        print(\n",
    "                            f\"BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: \"\n",
    "                            f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                            f\"speed_vs_torch={rec['speed_vs_torch16']:.3f}x, \"\n",
    "                        )\n",
    "                        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this GEMM FP16 shape\")\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef1144ba-76a6-4572-ba6b-bf11bfdaac8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BN=32, BK=32, W=2, S=2: t_triton=0.342 ms, speed_vs_torch=0.851x, \n",
      "BM=32, BN=32, BK=32, W=2, S=3: t_triton=0.437 ms, speed_vs_torch=0.628x, \n",
      "BM=32, BN=32, BK=32, W=4, S=2: t_triton=0.332 ms, speed_vs_torch=0.786x, \n",
      "BM=32, BN=32, BK=32, W=4, S=3: t_triton=0.435 ms, speed_vs_torch=0.609x, \n",
      "BM=32, BN=32, BK=32, W=8, S=2: t_triton=0.454 ms, speed_vs_torch=0.580x, \n",
      "BM=32, BN=32, BK=32, W=8, S=3: t_triton=0.542 ms, speed_vs_torch=0.484x, \n",
      "BM=32, BN=32, BK=64, W=2, S=2: t_triton=0.314 ms, speed_vs_torch=0.856x, \n",
      "BM=32, BN=32, BK=64, W=2, S=3: t_triton=0.339 ms, speed_vs_torch=0.786x, \n",
      "BM=32, BN=32, BK=64, W=4, S=2: t_triton=0.328 ms, speed_vs_torch=0.816x, \n",
      "BM=32, BN=32, BK=64, W=4, S=3: t_triton=0.328 ms, speed_vs_torch=0.815x, \n",
      "BM=32, BN=32, BK=64, W=8, S=2: t_triton=0.336 ms, speed_vs_torch=0.807x, \n",
      "BM=32, BN=32, BK=64, W=8, S=3: t_triton=0.355 ms, speed_vs_torch=0.764x, \n",
      "BM=32, BN=32, BK=128, W=2, S=2: t_triton=0.401 ms, speed_vs_torch=0.682x, \n",
      "BM=32, BN=32, BK=128, W=2, S=3: t_triton=0.498 ms, speed_vs_torch=0.526x, \n",
      "BM=32, BN=32, BK=128, W=4, S=2: t_triton=0.376 ms, speed_vs_torch=0.695x, \n",
      "BM=32, BN=32, BK=128, W=4, S=3: t_triton=0.498 ms, speed_vs_torch=0.537x, \n",
      "BM=32, BN=32, BK=128, W=8, S=2: t_triton=0.396 ms, speed_vs_torch=0.675x, \n",
      "BM=32, BN=32, BK=128, W=8, S=3: t_triton=0.500 ms, speed_vs_torch=0.535x, \n",
      "BM=32, BN=64, BK=32, W=2, S=2: t_triton=0.257 ms, speed_vs_torch=1.030x, \n",
      "BM=32, BN=64, BK=32, W=2, S=3: t_triton=0.254 ms, speed_vs_torch=1.053x, \n",
      "BM=32, BN=64, BK=32, W=4, S=2: t_triton=0.264 ms, speed_vs_torch=1.016x, \n",
      "BM=32, BN=64, BK=32, W=4, S=3: t_triton=0.265 ms, speed_vs_torch=1.030x, \n",
      "BM=32, BN=64, BK=32, W=8, S=2: t_triton=0.291 ms, speed_vs_torch=0.947x, \n",
      "BM=32, BN=64, BK=32, W=8, S=3: t_triton=0.282 ms, speed_vs_torch=0.968x, \n",
      "BM=32, BN=64, BK=64, W=2, S=2: t_triton=0.267 ms, speed_vs_torch=1.029x, \n",
      "BM=32, BN=64, BK=64, W=2, S=3: t_triton=0.255 ms, speed_vs_torch=1.045x, \n",
      "BM=32, BN=64, BK=64, W=4, S=2: t_triton=0.250 ms, speed_vs_torch=1.071x, \n",
      "BM=32, BN=64, BK=64, W=4, S=3: t_triton=0.257 ms, speed_vs_torch=1.048x, \n",
      "BM=32, BN=64, BK=64, W=8, S=2: t_triton=0.264 ms, speed_vs_torch=1.024x, \n",
      "BM=32, BN=64, BK=64, W=8, S=3: t_triton=0.264 ms, speed_vs_torch=1.027x, \n",
      "BM=32, BN=64, BK=128, W=2, S=2: t_triton=0.294 ms, speed_vs_torch=0.934x, \n",
      "BM=32, BN=64, BK=128, W=2, S=3: t_triton=0.301 ms, speed_vs_torch=0.954x, \n",
      "BM=32, BN=64, BK=128, W=4, S=2: t_triton=0.271 ms, speed_vs_torch=0.972x, \n",
      "BM=32, BN=64, BK=128, W=4, S=3: t_triton=0.276 ms, speed_vs_torch=0.982x, \n",
      "BM=32, BN=64, BK=128, W=8, S=2: t_triton=0.268 ms, speed_vs_torch=0.995x, \n",
      "BM=32, BN=64, BK=128, W=8, S=3: t_triton=0.278 ms, speed_vs_torch=1.014x, \n",
      "BM=32, BN=128, BK=32, W=2, S=2: t_triton=0.240 ms, speed_vs_torch=1.128x, \n",
      "BM=32, BN=128, BK=32, W=2, S=3: t_triton=0.227 ms, speed_vs_torch=1.168x, \n",
      "BM=32, BN=128, BK=32, W=4, S=2: t_triton=0.243 ms, speed_vs_torch=1.134x, \n",
      "BM=32, BN=128, BK=32, W=4, S=3: t_triton=0.227 ms, speed_vs_torch=1.209x, \n",
      "BM=32, BN=128, BK=32, W=8, S=2: t_triton=0.256 ms, speed_vs_torch=1.040x, \n",
      "BM=32, BN=128, BK=32, W=8, S=3: t_triton=0.234 ms, speed_vs_torch=1.190x, \n",
      "BM=32, BN=128, BK=64, W=2, S=2: t_triton=0.253 ms, speed_vs_torch=1.065x, \n",
      "BM=32, BN=128, BK=64, W=2, S=3: t_triton=0.255 ms, speed_vs_torch=1.042x, \n",
      "BM=32, BN=128, BK=64, W=4, S=2: t_triton=0.238 ms, speed_vs_torch=1.107x, \n",
      "BM=32, BN=128, BK=64, W=4, S=3: t_triton=0.230 ms, speed_vs_torch=1.166x, \n",
      "BM=32, BN=128, BK=64, W=8, S=2: t_triton=0.250 ms, speed_vs_torch=1.118x, \n",
      "BM=32, BN=128, BK=64, W=8, S=3: t_triton=0.229 ms, speed_vs_torch=1.198x, \n",
      "BM=32, BN=128, BK=128, W=2, S=2: t_triton=0.325 ms, speed_vs_torch=0.822x, \n",
      "BM=32, BN=128, BK=128, W=2, S=3: t_triton=0.554 ms, speed_vs_torch=0.476x, \n",
      "BM=32, BN=128, BK=128, W=4, S=2: t_triton=0.282 ms, speed_vs_torch=0.958x, \n",
      "BM=32, BN=128, BK=128, W=4, S=3: t_triton=0.302 ms, speed_vs_torch=0.870x, \n",
      "BM=32, BN=128, BK=128, W=8, S=2: t_triton=0.271 ms, speed_vs_torch=0.970x, \n",
      "BM=32, BN=128, BK=128, W=8, S=3: t_triton=0.278 ms, speed_vs_torch=0.948x, \n",
      "BM=64, BN=32, BK=32, W=2, S=2: t_triton=0.283 ms, speed_vs_torch=0.926x, \n",
      "BM=64, BN=32, BK=32, W=2, S=3: t_triton=0.389 ms, speed_vs_torch=0.689x, \n",
      "BM=64, BN=32, BK=32, W=4, S=2: t_triton=0.281 ms, speed_vs_torch=0.985x, \n",
      "BM=64, BN=32, BK=32, W=4, S=3: t_triton=0.366 ms, speed_vs_torch=0.732x, \n",
      "BM=64, BN=32, BK=32, W=8, S=2: t_triton=0.297 ms, speed_vs_torch=0.889x, \n",
      "BM=64, BN=32, BK=32, W=8, S=3: t_triton=0.308 ms, speed_vs_torch=0.865x, \n",
      "BM=64, BN=32, BK=64, W=2, S=2: t_triton=0.331 ms, speed_vs_torch=0.827x, \n",
      "BM=64, BN=32, BK=64, W=2, S=3: t_triton=0.331 ms, speed_vs_torch=0.794x, \n",
      "BM=64, BN=32, BK=64, W=4, S=2: t_triton=0.300 ms, speed_vs_torch=0.879x, \n",
      "BM=64, BN=32, BK=64, W=4, S=3: t_triton=0.275 ms, speed_vs_torch=0.964x, \n",
      "BM=64, BN=32, BK=64, W=8, S=2: t_triton=0.294 ms, speed_vs_torch=0.919x, \n",
      "BM=64, BN=32, BK=64, W=8, S=3: t_triton=0.291 ms, speed_vs_torch=0.922x, \n",
      "BM=64, BN=32, BK=128, W=2, S=2: t_triton=0.341 ms, speed_vs_torch=0.790x, \n",
      "BM=64, BN=32, BK=128, W=2, S=3: t_triton=0.420 ms, speed_vs_torch=0.626x, \n",
      "BM=64, BN=32, BK=128, W=4, S=2: t_triton=0.335 ms, speed_vs_torch=0.788x, \n",
      "BM=64, BN=32, BK=128, W=4, S=3: t_triton=0.456 ms, speed_vs_torch=0.579x, \n",
      "BM=64, BN=32, BK=128, W=8, S=2: t_triton=0.330 ms, speed_vs_torch=0.796x, \n",
      "BM=64, BN=32, BK=128, W=8, S=3: t_triton=0.441 ms, speed_vs_torch=0.602x, \n",
      "BM=64, BN=64, BK=32, W=2, S=2: t_triton=0.248 ms, speed_vs_torch=1.057x, \n",
      "BM=64, BN=64, BK=32, W=2, S=3: t_triton=0.268 ms, speed_vs_torch=0.981x, \n",
      "BM=64, BN=64, BK=32, W=4, S=2: t_triton=0.273 ms, speed_vs_torch=0.963x, \n",
      "BM=64, BN=64, BK=32, W=4, S=3: t_triton=0.249 ms, speed_vs_torch=1.066x, \n",
      "BM=64, BN=64, BK=32, W=8, S=2: t_triton=0.244 ms, speed_vs_torch=1.083x, \n",
      "BM=64, BN=64, BK=32, W=8, S=3: t_triton=0.274 ms, speed_vs_torch=0.987x, \n",
      "BM=64, BN=64, BK=64, W=2, S=2: t_triton=0.254 ms, speed_vs_torch=1.044x, \n",
      "BM=64, BN=64, BK=64, W=2, S=3: t_triton=0.274 ms, speed_vs_torch=0.976x, \n",
      "BM=64, BN=64, BK=64, W=4, S=2: t_triton=0.256 ms, speed_vs_torch=1.024x, \n",
      "BM=64, BN=64, BK=64, W=4, S=3: t_triton=0.280 ms, speed_vs_torch=1.008x, \n",
      "BM=64, BN=64, BK=64, W=8, S=2: t_triton=0.273 ms, speed_vs_torch=1.040x, \n",
      "BM=64, BN=64, BK=64, W=8, S=3: t_triton=0.279 ms, speed_vs_torch=1.021x, \n",
      "BM=64, BN=64, BK=128, W=2, S=2: t_triton=0.293 ms, speed_vs_torch=0.967x, \n",
      "BM=64, BN=64, BK=128, W=2, S=3: t_triton=0.494 ms, speed_vs_torch=0.534x, \n",
      "BM=64, BN=64, BK=128, W=4, S=2: t_triton=0.271 ms, speed_vs_torch=0.972x, \n",
      "BM=64, BN=64, BK=128, W=4, S=3: t_triton=0.289 ms, speed_vs_torch=0.910x, \n",
      "BM=64, BN=64, BK=128, W=8, S=2: t_triton=0.274 ms, speed_vs_torch=0.957x, \n",
      "BM=64, BN=64, BK=128, W=8, S=3: t_triton=0.295 ms, speed_vs_torch=0.946x, \n",
      "BM=64, BN=128, BK=32, W=2, S=2: t_triton=0.254 ms, speed_vs_torch=1.094x, \n",
      "BM=64, BN=128, BK=32, W=2, S=3: t_triton=0.234 ms, speed_vs_torch=1.188x, \n",
      "BM=64, BN=128, BK=32, W=4, S=2: t_triton=0.227 ms, speed_vs_torch=1.159x, \n",
      "BM=64, BN=128, BK=32, W=4, S=3: t_triton=0.209 ms, speed_vs_torch=1.275x, \n",
      "BM=64, BN=128, BK=32, W=8, S=2: t_triton=0.238 ms, speed_vs_torch=1.122x, \n",
      "BM=64, BN=128, BK=32, W=8, S=3: t_triton=0.240 ms, speed_vs_torch=1.200x, \n",
      "BM=64, BN=128, BK=64, W=2, S=2: t_triton=0.252 ms, speed_vs_torch=1.107x, \n",
      "BM=64, BN=128, BK=64, W=2, S=3: t_triton=0.282 ms, speed_vs_torch=0.979x, \n",
      "BM=64, BN=128, BK=64, W=4, S=2: t_triton=0.228 ms, speed_vs_torch=1.212x, \n",
      "BM=64, BN=128, BK=64, W=4, S=3: t_triton=0.218 ms, speed_vs_torch=1.203x, \n",
      "BM=64, BN=128, BK=64, W=8, S=2: t_triton=0.238 ms, speed_vs_torch=1.104x, \n",
      "BM=64, BN=128, BK=64, W=8, S=3: t_triton=0.226 ms, speed_vs_torch=1.159x, \n",
      "BM=64, BN=128, BK=128, W=2, S=2: t_triton=0.363 ms, speed_vs_torch=0.731x, \n",
      "BM=64, BN=128, BK=128, W=2, S=3: t_triton=0.496 ms, speed_vs_torch=0.529x, \n",
      "BM=64, BN=128, BK=128, W=4, S=2: t_triton=0.249 ms, speed_vs_torch=1.056x, \n",
      "BM=64, BN=128, BK=128, W=4, S=3: t_triton=0.262 ms, speed_vs_torch=1.003x, \n",
      "BM=64, BN=128, BK=128, W=8, S=2: t_triton=0.280 ms, speed_vs_torch=0.936x, \n",
      "BM=64, BN=128, BK=128, W=8, S=3: t_triton=0.247 ms, speed_vs_torch=1.063x, \n",
      "BM=128, BN=32, BK=32, W=2, S=2: t_triton=0.252 ms, speed_vs_torch=1.042x, \n",
      "BM=128, BN=32, BK=32, W=2, S=3: t_triton=0.254 ms, speed_vs_torch=1.040x, \n",
      "BM=128, BN=32, BK=32, W=4, S=2: t_triton=0.308 ms, speed_vs_torch=0.863x, \n",
      "BM=128, BN=32, BK=32, W=4, S=3: t_triton=0.313 ms, speed_vs_torch=0.841x, \n",
      "BM=128, BN=32, BK=32, W=8, S=2: t_triton=0.305 ms, speed_vs_torch=0.877x, \n",
      "BM=128, BN=32, BK=32, W=8, S=3: t_triton=0.327 ms, speed_vs_torch=0.810x, \n",
      "BM=128, BN=32, BK=64, W=2, S=2: t_triton=0.242 ms, speed_vs_torch=1.095x, \n",
      "BM=128, BN=32, BK=64, W=2, S=3: t_triton=0.274 ms, speed_vs_torch=0.965x, \n",
      "BM=128, BN=32, BK=64, W=4, S=2: t_triton=0.375 ms, speed_vs_torch=0.701x, \n",
      "BM=128, BN=32, BK=64, W=4, S=3: t_triton=0.370 ms, speed_vs_torch=0.709x, \n",
      "BM=128, BN=32, BK=64, W=8, S=2: t_triton=0.349 ms, speed_vs_torch=0.752x, \n",
      "BM=128, BN=32, BK=64, W=8, S=3: t_triton=0.388 ms, speed_vs_torch=0.676x, \n",
      "BM=128, BN=32, BK=128, W=2, S=2: t_triton=0.363 ms, speed_vs_torch=0.730x, \n",
      "BM=128, BN=32, BK=128, W=2, S=3: t_triton=0.547 ms, speed_vs_torch=0.514x, \n",
      "BM=128, BN=32, BK=128, W=4, S=2: t_triton=0.337 ms, speed_vs_torch=0.828x, \n",
      "BM=128, BN=32, BK=128, W=4, S=3: t_triton=0.402 ms, speed_vs_torch=0.653x, \n",
      "BM=128, BN=32, BK=128, W=8, S=2: t_triton=0.334 ms, speed_vs_torch=0.786x, \n",
      "BM=128, BN=32, BK=128, W=8, S=3: t_triton=0.405 ms, speed_vs_torch=0.648x, \n",
      "BM=128, BN=64, BK=32, W=2, S=2: t_triton=0.240 ms, speed_vs_torch=1.094x, \n",
      "BM=128, BN=64, BK=32, W=2, S=3: t_triton=0.238 ms, speed_vs_torch=1.104x, \n",
      "BM=128, BN=64, BK=32, W=4, S=2: t_triton=0.236 ms, speed_vs_torch=1.127x, \n",
      "BM=128, BN=64, BK=32, W=4, S=3: t_triton=0.222 ms, speed_vs_torch=1.184x, \n",
      "BM=128, BN=64, BK=32, W=8, S=2: t_triton=0.242 ms, speed_vs_torch=1.091x, \n",
      "BM=128, BN=64, BK=32, W=8, S=3: t_triton=0.238 ms, speed_vs_torch=1.149x, \n",
      "BM=128, BN=64, BK=64, W=2, S=2: t_triton=0.245 ms, speed_vs_torch=1.078x, \n",
      "BM=128, BN=64, BK=64, W=2, S=3: t_triton=0.258 ms, speed_vs_torch=1.017x, \n",
      "BM=128, BN=64, BK=64, W=4, S=2: t_triton=0.233 ms, speed_vs_torch=1.126x, \n",
      "BM=128, BN=64, BK=64, W=4, S=3: t_triton=0.221 ms, speed_vs_torch=1.186x, \n",
      "BM=128, BN=64, BK=64, W=8, S=2: t_triton=0.245 ms, speed_vs_torch=1.073x, \n",
      "BM=128, BN=64, BK=64, W=8, S=3: t_triton=0.220 ms, speed_vs_torch=1.191x, \n",
      "BM=128, BN=64, BK=128, W=2, S=2: t_triton=0.368 ms, speed_vs_torch=0.717x, \n",
      "BM=128, BN=64, BK=128, W=2, S=3: t_triton=0.507 ms, speed_vs_torch=0.517x, \n",
      "BM=128, BN=64, BK=128, W=4, S=2: t_triton=0.237 ms, speed_vs_torch=1.109x, \n",
      "BM=128, BN=64, BK=128, W=4, S=3: t_triton=0.272 ms, speed_vs_torch=0.965x, \n",
      "BM=128, BN=64, BK=128, W=8, S=2: t_triton=0.250 ms, speed_vs_torch=1.048x, \n",
      "BM=128, BN=64, BK=128, W=8, S=3: t_triton=0.256 ms, speed_vs_torch=1.024x, \n",
      "BM=128, BN=128, BK=32, W=2, S=2: t_triton=1.166 ms, speed_vs_torch=0.225x, \n",
      "BM=128, BN=128, BK=32, W=2, S=3: t_triton=1.185 ms, speed_vs_torch=0.222x, \n",
      "BM=128, BN=128, BK=32, W=4, S=2: t_triton=0.261 ms, speed_vs_torch=1.072x, \n",
      "BM=128, BN=128, BK=32, W=4, S=3: t_triton=0.248 ms, speed_vs_torch=1.134x, \n",
      "BM=128, BN=128, BK=32, W=8, S=2: t_triton=0.260 ms, speed_vs_torch=1.084x, \n",
      "BM=128, BN=128, BK=32, W=8, S=3: t_triton=0.244 ms, speed_vs_torch=1.139x, \n",
      "BM=128, BN=128, BK=64, W=2, S=2: t_triton=1.195 ms, speed_vs_torch=0.248x, \n",
      "BM=128, BN=128, BK=64, W=2, S=3: t_triton=0.987 ms, speed_vs_torch=0.266x, \n",
      "BM=128, BN=128, BK=64, W=4, S=2: t_triton=0.240 ms, speed_vs_torch=1.094x, \n",
      "BM=128, BN=128, BK=64, W=4, S=3: t_triton=0.250 ms, speed_vs_torch=1.052x, \n",
      "BM=128, BN=128, BK=64, W=8, S=2: t_triton=0.301 ms, speed_vs_torch=0.921x, \n",
      "BM=128, BN=128, BK=64, W=8, S=3: t_triton=0.260 ms, speed_vs_torch=1.077x, \n",
      "BM=128, BN=128, BK=128, W=2, S=2: t_triton=1.392 ms, speed_vs_torch=0.189x, \n",
      "[SKIP] BM=128, BN=128, BK=128, W=2, S=3: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "BM=128, BN=128, BK=128, W=4, S=2: t_triton=0.341 ms, speed_vs_torch=0.773x, \n",
      "[SKIP] BM=128, BN=128, BK=128, W=4, S=3: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "BM=128, BN=128, BK=128, W=8, S=2: t_triton=0.293 ms, speed_vs_torch=0.896x, \n",
      "[SKIP] BM=128, BN=128, BK=128, W=8, S=3: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n"
     ]
    }
   ],
   "source": [
    "df_fp16 = tune_gemm_fp16_tiles_for_shape(\n",
    "    M=4096, K=1024, N=1024,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb03eccb-85cd-4f24-aa3f-0327c0663985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_info</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch16_ms</th>\n",
       "      <th>speed_vs_torch16</th>\n",
       "      <th>mean_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.209433</td>\n",
       "      <td>0.267012</td>\n",
       "      <td>1.274928</td>\n",
       "      <td>0.004495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.227704</td>\n",
       "      <td>0.276041</td>\n",
       "      <td>1.212277</td>\n",
       "      <td>0.004497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226807</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>1.209349</td>\n",
       "      <td>0.004498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.218102</td>\n",
       "      <td>0.262456</td>\n",
       "      <td>1.203364</td>\n",
       "      <td>0.004496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240096</td>\n",
       "      <td>0.288143</td>\n",
       "      <td>1.200114</td>\n",
       "      <td>0.004501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shape_info  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "0  4096/1024/1024       64      128       32          4           3   \n",
       "1  4096/1024/1024       64      128       64          4           2   \n",
       "2  4096/1024/1024       32      128       32          4           3   \n",
       "3  4096/1024/1024       64      128       64          4           3   \n",
       "4  4096/1024/1024       64      128       32          8           3   \n",
       "\n",
       "   t_triton_ms  t_torch16_ms  speed_vs_torch16  mean_abs_err  \n",
       "0     0.209433      0.267012          1.274928      0.004495  \n",
       "1     0.227704      0.276041          1.212277      0.004497  \n",
       "2     0.226807      0.274289          1.209349      0.004498  \n",
       "3     0.218102      0.262456          1.203364      0.004496  \n",
       "4     0.240096      0.288143          1.200114      0.004501  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp16[\"shape_info\"] = (\n",
    "      \"4096/1024/1024\"\n",
    "\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"shape_info\",\n",
    "    \"BLOCK_M\", \"BLOCK_N\", \"BLOCK_K\",\n",
    "    \"num_warps\", \"num_stages\",\n",
    "    \"t_triton_ms\", \"t_torch16_ms\",\n",
    "    \"speed_vs_torch16\",\n",
    "     \"mean_abs_err\"\n",
    "]\n",
    "\n",
    "df_fp16_filtered = df_fp16[cols].sort_values(\"speed_vs_torch16\", ascending=False).head(5).reset_index(drop=True)\n",
    "df_fp16_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c28f971-b3a5-4d09-92a7-9c84689da03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_info</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch16_ms</th>\n",
       "      <th>speed_vs_torch16</th>\n",
       "      <th>mean_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GEMM_FP16</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.209433</td>\n",
       "      <td>0.267012</td>\n",
       "      <td>1.274928</td>\n",
       "      <td>0.004495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_FP16</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.227704</td>\n",
       "      <td>0.276041</td>\n",
       "      <td>1.212277</td>\n",
       "      <td>0.004497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_FP16</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226807</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>1.209349</td>\n",
       "      <td>0.004498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_FP16</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.218102</td>\n",
       "      <td>0.262456</td>\n",
       "      <td>1.203364</td>\n",
       "      <td>0.004496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_FP16</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240096</td>\n",
       "      <td>0.288143</td>\n",
       "      <td>1.200114</td>\n",
       "      <td>0.004501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               shape_info  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "GEMM_FP16  4096/1024/1024       64      128       32          4           3   \n",
       "GEMM_FP16  4096/1024/1024       64      128       64          4           2   \n",
       "GEMM_FP16  4096/1024/1024       32      128       32          4           3   \n",
       "GEMM_FP16  4096/1024/1024       64      128       64          4           3   \n",
       "GEMM_FP16  4096/1024/1024       64      128       32          8           3   \n",
       "\n",
       "           t_triton_ms  t_torch16_ms  speed_vs_torch16  mean_abs_err  \n",
       "GEMM_FP16     0.209433      0.267012          1.274928      0.004495  \n",
       "GEMM_FP16     0.227704      0.276041          1.212277      0.004497  \n",
       "GEMM_FP16     0.226807      0.274289          1.209349      0.004498  \n",
       "GEMM_FP16     0.218102      0.262456          1.203364      0.004496  \n",
       "GEMM_FP16     0.240096      0.288143          1.200114      0.004501  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp16_filtered.index = [\"GEMM_FP16\"] * len(df_fp16_filtered)\n",
    "df_fp16_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caaa2e59-6fd1-46a2-aa6e-f6239acef9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP16_GEMM_BEST_BLOCK_M = 64\n",
    "FP16_GEMM_BEST_BLOCK_N = 128\n",
    "FP16_GEMM_BEST_BLOCK_K = 32\n",
    "FP16_GEMM_BEST_WARPS   = 4\n",
    "FP16_GEMM_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170a898-0db3-43db-94b8-7b2eeeed7b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a0457-c2ca-445b-bce8-b548643cdd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8aa21e-1cd6-4013-aad7-ddc6d51382a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164f0a7-5e4f-4198-be23-383a6591d9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1e4b3-1429-4b36-a9db-ef73caca1a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b5d8c-1886-476d-912c-eb9ba7f12c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d20278-7e3f-4801-aab9-69b8d21055f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c4bc7-a1f5-4ee9-8328-798f03e066a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb12850-dd78-4347-8678-307c0f1c81a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3b07d-4a18-421e-b426-d70fffb87fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e16eae-376a-483e-9a1b-f38c5053c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ea04e-f3d0-4dc3-872a-f01ecde4a6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de5940-890d-4388-a53d-ca61ec9b8f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
