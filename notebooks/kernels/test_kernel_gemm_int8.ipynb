{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2544b4-e605-40a5-b5b0-6e9150e9266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd19731-11bd-4717-8cf0-d63c086bed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from conv_gemm.triton_kernels.int8.gemm_int8_kernel import gemm_int8_tc_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d9441b-f60b-4973-87ab-3f824eb054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemm_int8_tc(\n",
    "    A_q: torch.Tensor,   \n",
    "    B_q: torch.Tensor,  \n",
    "    *,\n",
    "    BLOCK_M: int = 64,\n",
    "    BLOCK_N: int = 64,\n",
    "    BLOCK_K: int = 32,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    if not A_q.is_contiguous():\n",
    "        A_q = A_q.contiguous()\n",
    "    if not B_q.is_contiguous():\n",
    "        B_q = B_q.contiguous()\n",
    "\n",
    "    M, K1 = A_q.shape\n",
    "    K2, N = B_q.shape\n",
    "    assert K1 == K2, f\"K mismatch: {K1} vs {K2}\"\n",
    "\n",
    "    assert K1 % 4 == 0, f\"K={K1} must be divisible by 4 for INT8 dot\"\n",
    "    assert BLOCK_K % 4 == 0, f\"BLOCK_K={BLOCK_K} must be divisible by 4\"\n",
    "\n",
    "    C_i32 = torch.empty((M, N), dtype=torch.int32, device=A_q.device)\n",
    "\n",
    "    a_m, a_k = A_q.stride()\n",
    "    b_k, b_n = B_q.stride()\n",
    "    c_m, c_n = C_i32.stride()\n",
    "\n",
    "    grid = (\n",
    "        triton.cdiv(M, BLOCK_M),\n",
    "        triton.cdiv(N, BLOCK_N),\n",
    "    )\n",
    "\n",
    "    gemm_int8_tc_kernel[grid](\n",
    "        A_q, B_q, C_i32,\n",
    "        M, N, K1,\n",
    "        a_m, a_k,\n",
    "        b_k, b_n,\n",
    "        c_m, c_n,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return C_i32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cceb33-ae6a-41c3-af13-4b6c12f62f2c",
   "metadata": {},
   "source": [
    "# title search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c24134f7-47f2-4d16-b951-2fb19db1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_gemm_int8_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    INT8 Triton GEMM vs torch FP16 matmul\n",
    "    \"\"\"\n",
    "    A_q = torch.randint(-128, 127, (M, K), device=device, dtype=torch.int8)\n",
    "    B_q = torch.randint(-128, 127, (K, N), device=device, dtype=torch.int8)\n",
    "\n",
    "    # масштаб для перевода int8 \n",
    "    scale = 128.0\n",
    "    s = 1.0 / scale\n",
    "\n",
    "    # FP16 baseline на отмасштабированных данных\n",
    "    A_f16 = (A_q.float() * s).half()\n",
    "    B_f16 = (B_q.float() * s).half()\n",
    "\n",
    "    # torch FP16 matmul\n",
    "    def _call_torch():\n",
    "        return A_f16 @ B_f16\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref2 = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Triton INT8 GEMM \n",
    "    def _call_triton():\n",
    "        C_i32 = gemm_int8_tc(\n",
    "            A_q, B_q,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return C_i32\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Error Triton INT32 vs Torch FP16\n",
    "    C_tr_fp32  = C_i32.float() * (s * s)\n",
    "    C_ref_fp32 = C_ref2.float()\n",
    "\n",
    "    diff = (C_tr_fp32 - C_ref_fp32).abs()\n",
    "    max_abs_err16 = diff.max().item()\n",
    "    mean_abs_err16 = diff.mean().item()\n",
    "\n",
    "    # bandwidth\n",
    "    bytes_moved = A_q.numel() + B_q.numel()\n",
    "    bytes_moved += C_i32.numel() * 4\n",
    "    bytes_moved = float(bytes_moved)\n",
    "\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch16_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch16\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch16_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err16,\n",
    "        \"mean_abs_err\": mean_abs_err16,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e230b854-ec14-467d-a3c6-d55a1937a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_gemm_int8_tiles_for_shape(\n",
    "    M, K, N,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3,4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BN in blocks_N:\n",
    "            for BK in blocks_K:\n",
    "                if (K % 4 != 0) or (BK % 4 != 0):\n",
    "                    print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}: K/BK not multiple of 4\")\n",
    "                    continue\n",
    "\n",
    "                for W in warps:\n",
    "                    for S in stages:\n",
    "                        try:\n",
    "                            rec = bench_once_gemm_int8_vs_torch(\n",
    "                                M, K, N,\n",
    "                                BLOCK_M=BM,\n",
    "                                BLOCK_N=BN,\n",
    "                                BLOCK_K=BK,\n",
    "                                num_warps=W,\n",
    "                                num_stages=S,\n",
    "                                iters=iters,\n",
    "                                device=device,\n",
    "                            )\n",
    "                        except RuntimeError as e:\n",
    "                            print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        print(\n",
    "                            f\"BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: \"\n",
    "                            f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                            f\"speed_vs_torch={rec['speed_vs_torch16']:.3f}x, \"\n",
    "                        )\n",
    "                        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this GEMM shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c363e02-f7c4-4911-a6da-ed7305f15f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BN=32, BK=32, W=2, S=2: t_triton=0.244 ms, speed_vs_torch=1.068x, \n",
      "BM=32, BN=32, BK=32, W=2, S=3: t_triton=0.272 ms, speed_vs_torch=0.865x, \n",
      "BM=32, BN=32, BK=32, W=4, S=2: t_triton=0.482 ms, speed_vs_torch=0.484x, \n",
      "BM=32, BN=32, BK=32, W=4, S=3: t_triton=0.402 ms, speed_vs_torch=0.542x, \n",
      "BM=32, BN=32, BK=32, W=8, S=2: t_triton=0.502 ms, speed_vs_torch=0.435x, \n",
      "BM=32, BN=32, BK=32, W=8, S=3: t_triton=0.538 ms, speed_vs_torch=0.440x, \n",
      "BM=32, BN=32, BK=64, W=2, S=2: t_triton=0.240 ms, speed_vs_torch=0.904x, \n",
      "BM=32, BN=32, BK=64, W=2, S=3: t_triton=0.227 ms, speed_vs_torch=0.962x, \n",
      "BM=32, BN=32, BK=64, W=4, S=2: t_triton=0.306 ms, speed_vs_torch=0.784x, \n",
      "BM=32, BN=32, BK=64, W=4, S=3: t_triton=0.357 ms, speed_vs_torch=0.702x, \n",
      "BM=32, BN=32, BK=64, W=8, S=2: t_triton=0.389 ms, speed_vs_torch=0.560x, \n",
      "BM=32, BN=32, BK=64, W=8, S=3: t_triton=0.387 ms, speed_vs_torch=0.565x, \n",
      "BM=32, BN=32, BK=128, W=2, S=2: t_triton=0.230 ms, speed_vs_torch=0.959x, \n",
      "BM=32, BN=32, BK=128, W=2, S=3: t_triton=0.253 ms, speed_vs_torch=1.078x, \n",
      "BM=32, BN=32, BK=128, W=4, S=2: t_triton=0.326 ms, speed_vs_torch=0.685x, \n",
      "BM=32, BN=32, BK=128, W=4, S=3: t_triton=0.369 ms, speed_vs_torch=0.590x, \n",
      "BM=32, BN=32, BK=128, W=8, S=2: t_triton=0.408 ms, speed_vs_torch=0.579x, \n",
      "BM=32, BN=32, BK=128, W=8, S=3: t_triton=0.355 ms, speed_vs_torch=0.628x, \n",
      "BM=32, BN=64, BK=32, W=2, S=2: t_triton=0.181 ms, speed_vs_torch=1.202x, \n",
      "BM=32, BN=64, BK=32, W=2, S=3: t_triton=0.180 ms, speed_vs_torch=1.360x, \n",
      "BM=32, BN=64, BK=32, W=4, S=2: t_triton=0.261 ms, speed_vs_torch=0.899x, \n",
      "BM=32, BN=64, BK=32, W=4, S=3: t_triton=0.212 ms, speed_vs_torch=1.128x, \n",
      "BM=32, BN=64, BK=32, W=8, S=2: t_triton=0.351 ms, speed_vs_torch=0.648x, \n",
      "BM=32, BN=64, BK=32, W=8, S=3: t_triton=0.354 ms, speed_vs_torch=0.650x, \n",
      "BM=32, BN=64, BK=64, W=2, S=2: t_triton=0.211 ms, speed_vs_torch=1.107x, \n",
      "BM=32, BN=64, BK=64, W=2, S=3: t_triton=0.176 ms, speed_vs_torch=1.310x, \n",
      "BM=32, BN=64, BK=64, W=4, S=2: t_triton=0.180 ms, speed_vs_torch=1.291x, \n",
      "BM=32, BN=64, BK=64, W=4, S=3: t_triton=0.184 ms, speed_vs_torch=1.202x, \n",
      "BM=32, BN=64, BK=64, W=8, S=2: t_triton=0.295 ms, speed_vs_torch=0.798x, \n",
      "BM=32, BN=64, BK=64, W=8, S=3: t_triton=0.365 ms, speed_vs_torch=0.629x, \n",
      "BM=32, BN=64, BK=128, W=2, S=2: t_triton=0.174 ms, speed_vs_torch=1.275x, \n",
      "BM=32, BN=64, BK=128, W=2, S=3: t_triton=0.183 ms, speed_vs_torch=1.226x, \n",
      "BM=32, BN=64, BK=128, W=4, S=2: t_triton=0.192 ms, speed_vs_torch=1.188x, \n",
      "BM=32, BN=64, BK=128, W=4, S=3: t_triton=0.196 ms, speed_vs_torch=1.202x, \n",
      "BM=32, BN=64, BK=128, W=8, S=2: t_triton=0.277 ms, speed_vs_torch=0.989x, \n",
      "BM=32, BN=64, BK=128, W=8, S=3: t_triton=0.286 ms, speed_vs_torch=0.789x, \n",
      "BM=32, BN=128, BK=32, W=2, S=2: t_triton=0.170 ms, speed_vs_torch=1.279x, \n",
      "BM=32, BN=128, BK=32, W=2, S=3: t_triton=0.165 ms, speed_vs_torch=1.381x, \n",
      "BM=32, BN=128, BK=32, W=4, S=2: t_triton=0.276 ms, speed_vs_torch=0.817x, \n",
      "BM=32, BN=128, BK=32, W=4, S=3: t_triton=0.174 ms, speed_vs_torch=1.259x, \n",
      "BM=32, BN=128, BK=32, W=8, S=2: t_triton=0.217 ms, speed_vs_torch=1.070x, \n",
      "BM=32, BN=128, BK=32, W=8, S=3: t_triton=0.216 ms, speed_vs_torch=1.067x, \n",
      "BM=32, BN=128, BK=64, W=2, S=2: t_triton=0.181 ms, speed_vs_torch=1.240x, \n",
      "BM=32, BN=128, BK=64, W=2, S=3: t_triton=0.173 ms, speed_vs_torch=1.564x, \n",
      "BM=32, BN=128, BK=64, W=4, S=2: t_triton=0.160 ms, speed_vs_torch=1.390x, \n",
      "BM=32, BN=128, BK=64, W=4, S=3: t_triton=0.164 ms, speed_vs_torch=1.358x, \n",
      "BM=32, BN=128, BK=64, W=8, S=2: t_triton=0.200 ms, speed_vs_torch=1.171x, \n",
      "BM=32, BN=128, BK=64, W=8, S=3: t_triton=0.209 ms, speed_vs_torch=1.040x, \n",
      "BM=32, BN=128, BK=128, W=2, S=2: t_triton=0.165 ms, speed_vs_torch=1.531x, \n",
      "BM=32, BN=128, BK=128, W=2, S=3: t_triton=0.193 ms, speed_vs_torch=1.168x, \n",
      "BM=32, BN=128, BK=128, W=4, S=2: t_triton=0.168 ms, speed_vs_torch=1.286x, \n",
      "BM=32, BN=128, BK=128, W=4, S=3: t_triton=0.167 ms, speed_vs_torch=1.358x, \n",
      "BM=32, BN=128, BK=128, W=8, S=2: t_triton=0.230 ms, speed_vs_torch=1.285x, \n",
      "BM=32, BN=128, BK=128, W=8, S=3: t_triton=0.187 ms, speed_vs_torch=1.200x, \n",
      "BM=64, BN=32, BK=32, W=2, S=2: t_triton=0.189 ms, speed_vs_torch=1.173x, \n",
      "BM=64, BN=32, BK=32, W=2, S=3: t_triton=0.200 ms, speed_vs_torch=1.163x, \n",
      "BM=64, BN=32, BK=32, W=4, S=2: t_triton=0.227 ms, speed_vs_torch=0.982x, \n",
      "BM=64, BN=32, BK=32, W=4, S=3: t_triton=0.210 ms, speed_vs_torch=1.316x, \n",
      "BM=64, BN=32, BK=32, W=8, S=2: t_triton=0.351 ms, speed_vs_torch=0.663x, \n",
      "BM=64, BN=32, BK=32, W=8, S=3: t_triton=0.361 ms, speed_vs_torch=0.610x, \n",
      "BM=64, BN=32, BK=64, W=2, S=2: t_triton=0.195 ms, speed_vs_torch=1.112x, \n",
      "BM=64, BN=32, BK=64, W=2, S=3: t_triton=0.185 ms, speed_vs_torch=1.388x, \n",
      "BM=64, BN=32, BK=64, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=1.235x, \n",
      "BM=64, BN=32, BK=64, W=4, S=3: t_triton=0.196 ms, speed_vs_torch=1.193x, \n",
      "BM=64, BN=32, BK=64, W=8, S=2: t_triton=0.281 ms, speed_vs_torch=0.842x, \n",
      "BM=64, BN=32, BK=64, W=8, S=3: t_triton=0.353 ms, speed_vs_torch=0.653x, \n",
      "BM=64, BN=32, BK=128, W=2, S=2: t_triton=0.195 ms, speed_vs_torch=1.110x, \n",
      "BM=64, BN=32, BK=128, W=2, S=3: t_triton=0.190 ms, speed_vs_torch=1.187x, \n",
      "BM=64, BN=32, BK=128, W=4, S=2: t_triton=0.221 ms, speed_vs_torch=1.032x, \n",
      "BM=64, BN=32, BK=128, W=4, S=3: t_triton=0.227 ms, speed_vs_torch=0.969x, \n",
      "BM=64, BN=32, BK=128, W=8, S=2: t_triton=0.298 ms, speed_vs_torch=0.822x, \n",
      "BM=64, BN=32, BK=128, W=8, S=3: t_triton=0.286 ms, speed_vs_torch=0.788x, \n",
      "BM=64, BN=64, BK=32, W=2, S=2: t_triton=0.121 ms, speed_vs_torch=1.793x, \n",
      "BM=64, BN=64, BK=32, W=2, S=3: t_triton=0.143 ms, speed_vs_torch=1.660x, \n",
      "BM=64, BN=64, BK=32, W=4, S=2: t_triton=0.184 ms, speed_vs_torch=1.251x, \n",
      "BM=64, BN=64, BK=32, W=4, S=3: t_triton=0.168 ms, speed_vs_torch=1.649x, \n",
      "BM=64, BN=64, BK=32, W=8, S=2: t_triton=0.213 ms, speed_vs_torch=1.113x, \n",
      "BM=64, BN=64, BK=32, W=8, S=3: t_triton=0.199 ms, speed_vs_torch=1.090x, \n",
      "BM=64, BN=64, BK=64, W=2, S=2: t_triton=0.112 ms, speed_vs_torch=2.129x, \n",
      "BM=64, BN=64, BK=64, W=2, S=3: t_triton=0.146 ms, speed_vs_torch=1.502x, \n",
      "BM=64, BN=64, BK=64, W=4, S=2: t_triton=0.157 ms, speed_vs_torch=1.718x, \n",
      "BM=64, BN=64, BK=64, W=4, S=3: t_triton=0.152 ms, speed_vs_torch=1.444x, \n",
      "BM=64, BN=64, BK=64, W=8, S=2: t_triton=0.178 ms, speed_vs_torch=1.254x, \n",
      "BM=64, BN=64, BK=64, W=8, S=3: t_triton=0.179 ms, speed_vs_torch=1.312x, \n",
      "BM=64, BN=64, BK=128, W=2, S=2: t_triton=0.130 ms, speed_vs_torch=1.702x, \n",
      "BM=64, BN=64, BK=128, W=2, S=3: t_triton=0.151 ms, speed_vs_torch=1.840x, \n",
      "BM=64, BN=64, BK=128, W=4, S=2: t_triton=0.162 ms, speed_vs_torch=1.372x, \n",
      "BM=64, BN=64, BK=128, W=4, S=3: t_triton=0.162 ms, speed_vs_torch=1.354x, \n",
      "BM=64, BN=64, BK=128, W=8, S=2: t_triton=0.219 ms, speed_vs_torch=1.035x, \n",
      "BM=64, BN=64, BK=128, W=8, S=3: t_triton=0.166 ms, speed_vs_torch=1.315x, \n",
      "BM=64, BN=128, BK=32, W=2, S=2: t_triton=0.138 ms, speed_vs_torch=2.059x, \n",
      "BM=64, BN=128, BK=32, W=2, S=3: t_triton=0.115 ms, speed_vs_torch=1.932x, \n",
      "BM=64, BN=128, BK=32, W=4, S=2: t_triton=0.122 ms, speed_vs_torch=1.783x, \n",
      "BM=64, BN=128, BK=32, W=4, S=3: t_triton=0.118 ms, speed_vs_torch=1.844x, \n",
      "BM=64, BN=128, BK=32, W=8, S=2: t_triton=0.182 ms, speed_vs_torch=1.217x, \n",
      "BM=64, BN=128, BK=32, W=8, S=3: t_triton=0.227 ms, speed_vs_torch=1.042x, \n",
      "BM=64, BN=128, BK=64, W=2, S=2: t_triton=0.117 ms, speed_vs_torch=1.931x, \n",
      "BM=64, BN=128, BK=64, W=2, S=3: t_triton=0.109 ms, speed_vs_torch=2.041x, \n",
      "BM=64, BN=128, BK=64, W=4, S=2: t_triton=0.114 ms, speed_vs_torch=1.950x, \n",
      "BM=64, BN=128, BK=64, W=4, S=3: t_triton=0.105 ms, speed_vs_torch=2.213x, \n",
      "BM=64, BN=128, BK=64, W=8, S=2: t_triton=0.176 ms, speed_vs_torch=1.247x, \n",
      "BM=64, BN=128, BK=64, W=8, S=3: t_triton=0.149 ms, speed_vs_torch=1.845x, \n",
      "BM=64, BN=128, BK=128, W=2, S=2: t_triton=0.124 ms, speed_vs_torch=1.779x, \n",
      "BM=64, BN=128, BK=128, W=2, S=3: t_triton=0.172 ms, speed_vs_torch=1.254x, \n",
      "BM=64, BN=128, BK=128, W=4, S=2: t_triton=0.114 ms, speed_vs_torch=2.028x, \n",
      "BM=64, BN=128, BK=128, W=4, S=3: t_triton=0.112 ms, speed_vs_torch=1.940x, \n",
      "BM=64, BN=128, BK=128, W=8, S=2: t_triton=0.180 ms, speed_vs_torch=1.437x, \n",
      "BM=64, BN=128, BK=128, W=8, S=3: t_triton=0.149 ms, speed_vs_torch=1.459x, \n",
      "BM=128, BN=32, BK=32, W=2, S=2: t_triton=0.133 ms, speed_vs_torch=1.649x, \n",
      "BM=128, BN=32, BK=32, W=2, S=3: t_triton=0.159 ms, speed_vs_torch=1.426x, \n",
      "BM=128, BN=32, BK=32, W=4, S=2: t_triton=0.183 ms, speed_vs_torch=1.212x, \n",
      "BM=128, BN=32, BK=32, W=4, S=3: t_triton=0.219 ms, speed_vs_torch=1.102x, \n",
      "BM=128, BN=32, BK=32, W=8, S=2: t_triton=0.210 ms, speed_vs_torch=1.074x, \n",
      "BM=128, BN=32, BK=32, W=8, S=3: t_triton=0.218 ms, speed_vs_torch=1.025x, \n",
      "BM=128, BN=32, BK=64, W=2, S=2: t_triton=0.123 ms, speed_vs_torch=1.848x, \n",
      "BM=128, BN=32, BK=64, W=2, S=3: t_triton=0.139 ms, speed_vs_torch=1.593x, \n",
      "BM=128, BN=32, BK=64, W=4, S=2: t_triton=0.187 ms, speed_vs_torch=1.440x, \n",
      "BM=128, BN=32, BK=64, W=4, S=3: t_triton=0.173 ms, speed_vs_torch=1.268x, \n",
      "BM=128, BN=32, BK=64, W=8, S=2: t_triton=0.184 ms, speed_vs_torch=1.265x, \n",
      "BM=128, BN=32, BK=64, W=8, S=3: t_triton=0.181 ms, speed_vs_torch=1.233x, \n",
      "BM=128, BN=32, BK=128, W=2, S=2: t_triton=0.135 ms, speed_vs_torch=1.638x, \n",
      "BM=128, BN=32, BK=128, W=2, S=3: t_triton=0.195 ms, speed_vs_torch=1.341x, \n",
      "BM=128, BN=32, BK=128, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=1.200x, \n",
      "BM=128, BN=32, BK=128, W=4, S=3: t_triton=0.190 ms, speed_vs_torch=1.174x, \n",
      "BM=128, BN=32, BK=128, W=8, S=2: t_triton=0.186 ms, speed_vs_torch=1.241x, \n",
      "BM=128, BN=32, BK=128, W=8, S=3: t_triton=0.199 ms, speed_vs_torch=1.152x, \n",
      "BM=128, BN=64, BK=32, W=2, S=2: t_triton=0.139 ms, speed_vs_torch=1.944x, \n",
      "BM=128, BN=64, BK=32, W=2, S=3: t_triton=0.115 ms, speed_vs_torch=1.935x, \n",
      "BM=128, BN=64, BK=32, W=4, S=2: t_triton=0.127 ms, speed_vs_torch=1.751x, \n",
      "BM=128, BN=64, BK=32, W=4, S=3: t_triton=0.129 ms, speed_vs_torch=1.738x, \n",
      "BM=128, BN=64, BK=32, W=8, S=2: t_triton=0.192 ms, speed_vs_torch=1.151x, \n",
      "BM=128, BN=64, BK=32, W=8, S=3: t_triton=0.176 ms, speed_vs_torch=1.500x, \n",
      "BM=128, BN=64, BK=64, W=2, S=2: t_triton=0.126 ms, speed_vs_torch=1.723x, \n",
      "BM=128, BN=64, BK=64, W=2, S=3: t_triton=0.118 ms, speed_vs_torch=1.854x, \n",
      "BM=128, BN=64, BK=64, W=4, S=2: t_triton=0.108 ms, speed_vs_torch=2.026x, \n",
      "BM=128, BN=64, BK=64, W=4, S=3: t_triton=0.117 ms, speed_vs_torch=1.897x, \n",
      "BM=128, BN=64, BK=64, W=8, S=2: t_triton=0.180 ms, speed_vs_torch=1.328x, \n",
      "BM=128, BN=64, BK=64, W=8, S=3: t_triton=0.162 ms, speed_vs_torch=1.677x, \n",
      "BM=128, BN=64, BK=128, W=2, S=2: t_triton=0.200 ms, speed_vs_torch=1.119x, \n",
      "BM=128, BN=64, BK=128, W=2, S=3: t_triton=0.212 ms, speed_vs_torch=1.066x, \n",
      "BM=128, BN=64, BK=128, W=4, S=2: t_triton=0.121 ms, speed_vs_torch=1.878x, \n",
      "BM=128, BN=64, BK=128, W=4, S=3: t_triton=0.129 ms, speed_vs_torch=1.741x, \n",
      "BM=128, BN=64, BK=128, W=8, S=2: t_triton=0.181 ms, speed_vs_torch=1.429x, \n",
      "BM=128, BN=64, BK=128, W=8, S=3: t_triton=0.155 ms, speed_vs_torch=1.408x, \n",
      "BM=128, BN=128, BK=32, W=2, S=2: t_triton=0.391 ms, speed_vs_torch=0.561x, \n",
      "BM=128, BN=128, BK=32, W=2, S=3: t_triton=0.472 ms, speed_vs_torch=0.481x, \n",
      "BM=128, BN=128, BK=32, W=4, S=2: t_triton=0.154 ms, speed_vs_torch=1.481x, \n",
      "BM=128, BN=128, BK=32, W=4, S=3: t_triton=0.114 ms, speed_vs_torch=2.189x, \n",
      "BM=128, BN=128, BK=32, W=8, S=2: t_triton=0.148 ms, speed_vs_torch=1.472x, \n",
      "BM=128, BN=128, BK=32, W=8, S=3: t_triton=0.139 ms, speed_vs_torch=1.593x, \n",
      "BM=128, BN=128, BK=64, W=2, S=2: t_triton=0.530 ms, speed_vs_torch=0.444x, \n",
      "BM=128, BN=128, BK=64, W=2, S=3: t_triton=0.601 ms, speed_vs_torch=0.441x, \n",
      "BM=128, BN=128, BK=64, W=4, S=2: t_triton=0.112 ms, speed_vs_torch=1.953x, \n",
      "BM=128, BN=128, BK=64, W=4, S=3: t_triton=0.101 ms, speed_vs_torch=2.261x, \n",
      "BM=128, BN=128, BK=64, W=8, S=2: t_triton=0.177 ms, speed_vs_torch=1.239x, \n",
      "BM=128, BN=128, BK=64, W=8, S=3: t_triton=0.124 ms, speed_vs_torch=2.205x, \n",
      "BM=128, BN=128, BK=128, W=2, S=2: t_triton=0.606 ms, speed_vs_torch=0.361x, \n",
      "BM=128, BN=128, BK=128, W=2, S=3: t_triton=0.647 ms, speed_vs_torch=0.349x, \n",
      "BM=128, BN=128, BK=128, W=4, S=2: t_triton=0.129 ms, speed_vs_torch=2.018x, \n",
      "BM=128, BN=128, BK=128, W=4, S=3: t_triton=0.166 ms, speed_vs_torch=1.319x, \n",
      "BM=128, BN=128, BK=128, W=8, S=2: t_triton=0.169 ms, speed_vs_torch=1.297x, \n",
      "BM=128, BN=128, BK=128, W=8, S=3: t_triton=0.123 ms, speed_vs_torch=1.896x, \n"
     ]
    }
   ],
   "source": [
    "df_gemm_tiles = tune_gemm_int8_tiles_for_shape(\n",
    "    M=4096, K=1024, N=1024,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88c29522-a1df-4062-adbc-9edc3a7e4709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch16_ms</th>\n",
       "      <th>speed_vs_torch16</th>\n",
       "      <th>mean_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100911</td>\n",
       "      <td>0.228124</td>\n",
       "      <td>2.260644</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104894</td>\n",
       "      <td>0.232098</td>\n",
       "      <td>2.212686</td>\n",
       "      <td>0.001486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.124323</td>\n",
       "      <td>0.274138</td>\n",
       "      <td>2.205039</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.114248</td>\n",
       "      <td>0.250047</td>\n",
       "      <td>2.188643</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.112178</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>2.129300</td>\n",
       "      <td>0.001486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BLOCK_M  BLOCK_K  num_warps  num_stages  t_triton_ms  t_torch16_ms  \\\n",
       "0      128       64          4           3     0.100911      0.228124   \n",
       "1       64       64          4           3     0.104894      0.232098   \n",
       "2      128       64          8           3     0.124323      0.274138   \n",
       "3      128       32          4           3     0.114248      0.250047   \n",
       "4       64       64          2           2     0.112178      0.238861   \n",
       "\n",
       "   speed_vs_torch16  mean_abs_err  \n",
       "0          2.260644      0.001489  \n",
       "1          2.212686      0.001486  \n",
       "2          2.205039      0.001489  \n",
       "3          2.188643      0.001489  \n",
       "4          2.129300      0.001486  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\n",
    "    \"BLOCK_M\", \"BLOCK_K\",\n",
    "    \"num_warps\", \"num_stages\",\n",
    "    \"t_triton_ms\", \"t_torch16_ms\",\n",
    "    \"speed_vs_torch16\",\n",
    "     \"mean_abs_err\"\n",
    "]\n",
    "\n",
    "df_gemm__filtered = df_gemm_tiles[cols].sort_values(\"speed_vs_torch16\", ascending=False).head(5).reset_index(drop=True)\n",
    "df_gemm__filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa596d30-56f6-4fce-ae4e-684416db22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_GEMM_BEST_BLOCK_M = 128\n",
    "INT8_GEMM_BEST_BLOCK_N = 0\n",
    "INT8_GEMM_BEST_BLOCK_K = 64\n",
    "INT8_GEMM_BEST_WARPS   = 4\n",
    "INT8_GEMM_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc783a-dfe5-4306-9f8b-b4e9eee2cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588e49e-84e6-4dd0-b9a0-73010622b059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de16cb-e4b7-4598-9f28-5bfd9c6045b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942eb61-f0a3-4b67-bd66-e82b70f809bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
