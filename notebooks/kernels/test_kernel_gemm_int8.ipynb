{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2544b4-e605-40a5-b5b0-6e9150e9266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd19731-11bd-4717-8cf0-d63c086bed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from conv_gemm.triton_kernels.int8.gemm_int8_kernel import gemm_int8_tc_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d9441b-f60b-4973-87ab-3f824eb054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemm_int8_tc(\n",
    "    A_q: torch.Tensor,   \n",
    "    B_q: torch.Tensor,  \n",
    "    *,\n",
    "    BLOCK_M: int = 64,\n",
    "    BLOCK_N: int = 64,\n",
    "    BLOCK_K: int = 32,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    if not A_q.is_contiguous():\n",
    "        A_q = A_q.contiguous()\n",
    "    if not B_q.is_contiguous():\n",
    "        B_q = B_q.contiguous()\n",
    "\n",
    "    M, K1 = A_q.shape\n",
    "    K2, N = B_q.shape\n",
    "    assert K1 == K2, f\"K mismatch: {K1} vs {K2}\"\n",
    "\n",
    "    assert K1 % 4 == 0, f\"K={K1} must be divisible by 4 for INT8 dot\"\n",
    "    assert BLOCK_K % 4 == 0, f\"BLOCK_K={BLOCK_K} must be divisible by 4\"\n",
    "\n",
    "    C_i32 = torch.empty((M, N), dtype=torch.int32, device=A_q.device)\n",
    "\n",
    "    a_m, a_k = A_q.stride()\n",
    "    b_k, b_n = B_q.stride()\n",
    "    c_m, c_n = C_i32.stride()\n",
    "\n",
    "    grid = (\n",
    "        triton.cdiv(M, BLOCK_M),\n",
    "        triton.cdiv(N, BLOCK_N),\n",
    "    )\n",
    "\n",
    "    gemm_int8_tc_kernel[grid](\n",
    "        A_q, B_q, C_i32,\n",
    "        M, N, K1,\n",
    "        a_m, a_k,\n",
    "        b_k, b_n,\n",
    "        c_m, c_n,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return C_i32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cceb33-ae6a-41c3-af13-4b6c12f62f2c",
   "metadata": {},
   "source": [
    "# title search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24134f7-47f2-4d16-b951-2fb19db1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_gemm_int8_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    INT8 Triton GEMM vs torch FP16 matmul\n",
    "    \"\"\"\n",
    "    A_q = torch.randint(-128, 127, (M, K), device=device, dtype=torch.int8)\n",
    "    B_q = torch.randint(-128, 127, (K, N), device=device, dtype=torch.int8)\n",
    "\n",
    "    # масштаб для перевода int8 \n",
    "    scale = 128.0\n",
    "    s = 1.0 / scale\n",
    "\n",
    "    # FP16 baseline на отмасштабированных данных\n",
    "    A_f16 = (A_q.float() * s).half()\n",
    "    B_f16 = (B_q.float() * s).half()\n",
    "\n",
    "    # torch FP16 matmul\n",
    "    def _call_torch():\n",
    "        return A_f16 @ B_f16\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref2 = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Triton INT8 GEMM \n",
    "    def _call_triton():\n",
    "        C_i32 = gemm_int8_tc(\n",
    "            A_q, B_q,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return C_i32\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Error Triton INT32 vs Torch FP16\n",
    "    C_tr_fp32  = C_i32.float() * (s * s)\n",
    "    C_ref_fp32 = C_ref2.float()\n",
    "\n",
    "    diff = (C_tr_fp32 - C_ref_fp32).abs()\n",
    "    max_abs_err16 = diff.max().item()\n",
    "    mean_abs_err16 = diff.mean().item()\n",
    "\n",
    "    # bandwidth\n",
    "    bytes_moved = A_q.numel() + B_q.numel()\n",
    "    bytes_moved += C_i32.numel() * 4\n",
    "    bytes_moved = float(bytes_moved)\n",
    "\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch16_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch16\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch16_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err16,\n",
    "        \"mean_abs_err\": mean_abs_err16,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e230b854-ec14-467d-a3c6-d55a1937a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_gemm_int8_tiles_for_shape(\n",
    "    M, K, N,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3,4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BN in blocks_N:\n",
    "            for BK in blocks_K:\n",
    "                if (K % 4 != 0) or (BK % 4 != 0):\n",
    "                    print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}: K/BK not multiple of 4\")\n",
    "                    continue\n",
    "\n",
    "                for W in warps:\n",
    "                    for S in stages:\n",
    "                        try:\n",
    "                            rec = bench_once_gemm_int8_vs_torch(\n",
    "                                M, K, N,\n",
    "                                BLOCK_M=BM,\n",
    "                                BLOCK_N=BN,\n",
    "                                BLOCK_K=BK,\n",
    "                                num_warps=W,\n",
    "                                num_stages=S,\n",
    "                                iters=iters,\n",
    "                                device=device,\n",
    "                            )\n",
    "                        except RuntimeError as e:\n",
    "                            print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        print(\n",
    "                            f\"BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: \"\n",
    "                            f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                            f\"speed_vs_torch={rec['speed_vs_torch16']:.3f}x, \"\n",
    "                        )\n",
    "                        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this GEMM shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c363e02-f7c4-4911-a6da-ed7305f15f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BN=32, BK=32, W=2, S=2: t_triton=0.233 ms, speed_vs_torch=0.967x, \n",
      "BM=32, BN=32, BK=32, W=2, S=3: t_triton=0.234 ms, speed_vs_torch=0.967x, \n",
      "BM=32, BN=32, BK=32, W=4, S=2: t_triton=0.398 ms, speed_vs_torch=0.541x, \n",
      "BM=32, BN=32, BK=32, W=4, S=3: t_triton=0.392 ms, speed_vs_torch=0.544x, \n",
      "BM=32, BN=32, BK=32, W=8, S=2: t_triton=0.476 ms, speed_vs_torch=0.445x, \n",
      "BM=32, BN=32, BK=32, W=8, S=3: t_triton=0.460 ms, speed_vs_torch=0.461x, \n",
      "BM=32, BN=32, BK=64, W=2, S=2: t_triton=0.222 ms, speed_vs_torch=0.953x, \n",
      "BM=32, BN=32, BK=64, W=2, S=3: t_triton=0.216 ms, speed_vs_torch=1.012x, \n",
      "BM=32, BN=32, BK=64, W=4, S=2: t_triton=0.296 ms, speed_vs_torch=0.754x, \n",
      "BM=32, BN=32, BK=64, W=4, S=3: t_triton=0.333 ms, speed_vs_torch=0.718x, \n",
      "BM=32, BN=32, BK=64, W=8, S=2: t_triton=0.374 ms, speed_vs_torch=0.583x, \n",
      "BM=32, BN=32, BK=64, W=8, S=3: t_triton=0.364 ms, speed_vs_torch=0.585x, \n",
      "BM=32, BN=32, BK=128, W=2, S=2: t_triton=0.214 ms, speed_vs_torch=0.991x, \n",
      "BM=32, BN=32, BK=128, W=2, S=3: t_triton=0.243 ms, speed_vs_torch=0.968x, \n",
      "BM=32, BN=32, BK=128, W=4, S=2: t_triton=0.307 ms, speed_vs_torch=0.715x, \n",
      "BM=32, BN=32, BK=128, W=4, S=3: t_triton=0.322 ms, speed_vs_torch=0.699x, \n",
      "BM=32, BN=32, BK=128, W=8, S=2: t_triton=0.332 ms, speed_vs_torch=0.645x, \n",
      "BM=32, BN=32, BK=128, W=8, S=3: t_triton=0.345 ms, speed_vs_torch=0.624x, \n",
      "BM=32, BN=64, BK=32, W=2, S=2: t_triton=0.178 ms, speed_vs_torch=1.204x, \n",
      "BM=32, BN=64, BK=32, W=2, S=3: t_triton=0.174 ms, speed_vs_torch=1.288x, \n",
      "BM=32, BN=64, BK=32, W=4, S=2: t_triton=0.207 ms, speed_vs_torch=1.060x, \n",
      "BM=32, BN=64, BK=32, W=4, S=3: t_triton=0.210 ms, speed_vs_torch=1.044x, \n",
      "BM=32, BN=64, BK=32, W=8, S=2: t_triton=0.334 ms, speed_vs_torch=0.657x, \n",
      "BM=32, BN=64, BK=32, W=8, S=3: t_triton=0.340 ms, speed_vs_torch=0.626x, \n",
      "BM=32, BN=64, BK=64, W=2, S=2: t_triton=0.166 ms, speed_vs_torch=1.286x, \n",
      "BM=32, BN=64, BK=64, W=2, S=3: t_triton=0.173 ms, speed_vs_torch=1.262x, \n",
      "BM=32, BN=64, BK=64, W=4, S=2: t_triton=0.177 ms, speed_vs_torch=1.254x, \n",
      "BM=32, BN=64, BK=64, W=4, S=3: t_triton=0.186 ms, speed_vs_torch=1.191x, \n",
      "BM=32, BN=64, BK=64, W=8, S=2: t_triton=0.287 ms, speed_vs_torch=0.766x, \n",
      "BM=32, BN=64, BK=64, W=8, S=3: t_triton=0.297 ms, speed_vs_torch=0.736x, \n",
      "BM=32, BN=64, BK=128, W=2, S=2: t_triton=0.169 ms, speed_vs_torch=1.263x, \n",
      "BM=32, BN=64, BK=128, W=2, S=3: t_triton=0.178 ms, speed_vs_torch=1.244x, \n",
      "BM=32, BN=64, BK=128, W=4, S=2: t_triton=0.179 ms, speed_vs_torch=1.239x, \n",
      "BM=32, BN=64, BK=128, W=4, S=3: t_triton=0.187 ms, speed_vs_torch=1.195x, \n",
      "BM=32, BN=64, BK=128, W=8, S=2: t_triton=0.268 ms, speed_vs_torch=0.820x, \n",
      "BM=32, BN=64, BK=128, W=8, S=3: t_triton=0.279 ms, speed_vs_torch=0.785x, \n",
      "BM=32, BN=128, BK=32, W=2, S=2: t_triton=0.171 ms, speed_vs_torch=1.292x, \n",
      "BM=32, BN=128, BK=32, W=2, S=3: t_triton=0.153 ms, speed_vs_torch=1.425x, \n",
      "BM=32, BN=128, BK=32, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=1.186x, \n",
      "BM=32, BN=128, BK=32, W=4, S=3: t_triton=0.170 ms, speed_vs_torch=1.262x, \n",
      "BM=32, BN=128, BK=32, W=8, S=2: t_triton=0.214 ms, speed_vs_torch=1.015x, \n",
      "BM=32, BN=128, BK=32, W=8, S=3: t_triton=0.203 ms, speed_vs_torch=1.062x, \n",
      "BM=32, BN=128, BK=64, W=2, S=2: t_triton=0.153 ms, speed_vs_torch=1.407x, \n",
      "BM=32, BN=128, BK=64, W=2, S=3: t_triton=0.151 ms, speed_vs_torch=1.421x, \n",
      "BM=32, BN=128, BK=64, W=4, S=2: t_triton=0.165 ms, speed_vs_torch=1.298x, \n",
      "BM=32, BN=128, BK=64, W=4, S=3: t_triton=0.156 ms, speed_vs_torch=1.409x, \n",
      "BM=32, BN=128, BK=64, W=8, S=2: t_triton=0.180 ms, speed_vs_torch=1.225x, \n",
      "BM=32, BN=128, BK=64, W=8, S=3: t_triton=0.187 ms, speed_vs_torch=1.145x, \n",
      "BM=32, BN=128, BK=128, W=2, S=2: t_triton=0.162 ms, speed_vs_torch=1.332x, \n",
      "BM=32, BN=128, BK=128, W=2, S=3: t_triton=0.189 ms, speed_vs_torch=1.127x, \n",
      "BM=32, BN=128, BK=128, W=4, S=2: t_triton=0.176 ms, speed_vs_torch=1.264x, \n",
      "BM=32, BN=128, BK=128, W=4, S=3: t_triton=0.164 ms, speed_vs_torch=1.317x, \n",
      "BM=32, BN=128, BK=128, W=8, S=2: t_triton=0.170 ms, speed_vs_torch=1.281x, \n",
      "BM=32, BN=128, BK=128, W=8, S=3: t_triton=0.180 ms, speed_vs_torch=1.198x, \n",
      "BM=64, BN=32, BK=32, W=2, S=2: t_triton=0.187 ms, speed_vs_torch=1.148x, \n",
      "BM=64, BN=32, BK=32, W=2, S=3: t_triton=0.188 ms, speed_vs_torch=1.159x, \n",
      "BM=64, BN=32, BK=32, W=4, S=2: t_triton=0.206 ms, speed_vs_torch=1.090x, \n",
      "BM=64, BN=32, BK=32, W=4, S=3: t_triton=0.209 ms, speed_vs_torch=1.035x, \n",
      "BM=64, BN=32, BK=32, W=8, S=2: t_triton=0.346 ms, speed_vs_torch=0.638x, \n",
      "BM=64, BN=32, BK=32, W=8, S=3: t_triton=0.340 ms, speed_vs_torch=0.629x, \n",
      "BM=64, BN=32, BK=64, W=2, S=2: t_triton=0.172 ms, speed_vs_torch=1.237x, \n",
      "BM=64, BN=32, BK=64, W=2, S=3: t_triton=0.183 ms, speed_vs_torch=1.202x, \n",
      "BM=64, BN=32, BK=64, W=4, S=2: t_triton=0.178 ms, speed_vs_torch=1.243x, \n",
      "BM=64, BN=32, BK=64, W=4, S=3: t_triton=0.194 ms, speed_vs_torch=1.151x, \n",
      "BM=64, BN=32, BK=64, W=8, S=2: t_triton=0.274 ms, speed_vs_torch=0.806x, \n",
      "BM=64, BN=32, BK=64, W=8, S=3: t_triton=0.279 ms, speed_vs_torch=0.764x, \n",
      "BM=64, BN=32, BK=128, W=2, S=2: t_triton=0.192 ms, speed_vs_torch=1.118x, \n",
      "BM=64, BN=32, BK=128, W=2, S=3: t_triton=0.187 ms, speed_vs_torch=1.167x, \n",
      "BM=64, BN=32, BK=128, W=4, S=2: t_triton=0.206 ms, speed_vs_torch=1.088x, \n",
      "BM=64, BN=32, BK=128, W=4, S=3: t_triton=0.222 ms, speed_vs_torch=0.980x, \n",
      "BM=64, BN=32, BK=128, W=8, S=2: t_triton=0.267 ms, speed_vs_torch=0.812x, \n",
      "BM=64, BN=32, BK=128, W=8, S=3: t_triton=0.280 ms, speed_vs_torch=0.777x, \n",
      "BM=64, BN=64, BK=32, W=2, S=2: t_triton=0.118 ms, speed_vs_torch=1.812x, \n",
      "BM=64, BN=64, BK=32, W=2, S=3: t_triton=0.127 ms, speed_vs_torch=1.686x, \n",
      "BM=64, BN=64, BK=32, W=4, S=2: t_triton=0.177 ms, speed_vs_torch=1.250x, \n",
      "BM=64, BN=64, BK=32, W=4, S=3: t_triton=0.164 ms, speed_vs_torch=1.303x, \n",
      "BM=64, BN=64, BK=32, W=8, S=2: t_triton=0.204 ms, speed_vs_torch=1.063x, \n",
      "BM=64, BN=64, BK=32, W=8, S=3: t_triton=0.195 ms, speed_vs_torch=1.110x, \n",
      "BM=64, BN=64, BK=64, W=2, S=2: t_triton=0.110 ms, speed_vs_torch=1.959x, \n",
      "BM=64, BN=64, BK=64, W=2, S=3: t_triton=0.130 ms, speed_vs_torch=1.653x, \n",
      "BM=64, BN=64, BK=64, W=4, S=2: t_triton=0.153 ms, speed_vs_torch=1.461x, \n",
      "BM=64, BN=64, BK=64, W=4, S=3: t_triton=0.148 ms, speed_vs_torch=1.470x, \n",
      "BM=64, BN=64, BK=64, W=8, S=2: t_triton=0.171 ms, speed_vs_torch=1.281x, \n",
      "BM=64, BN=64, BK=64, W=8, S=3: t_triton=0.166 ms, speed_vs_torch=1.290x, \n",
      "BM=64, BN=64, BK=128, W=2, S=2: t_triton=0.126 ms, speed_vs_torch=1.736x, \n",
      "BM=64, BN=64, BK=128, W=2, S=3: t_triton=0.144 ms, speed_vs_torch=1.508x, \n",
      "BM=64, BN=64, BK=128, W=4, S=2: t_triton=0.161 ms, speed_vs_torch=1.345x, \n",
      "BM=64, BN=64, BK=128, W=4, S=3: t_triton=0.159 ms, speed_vs_torch=1.352x, \n",
      "BM=64, BN=64, BK=128, W=8, S=2: t_triton=0.165 ms, speed_vs_torch=1.317x, \n",
      "BM=64, BN=64, BK=128, W=8, S=3: t_triton=0.165 ms, speed_vs_torch=1.308x, \n",
      "BM=64, BN=128, BK=32, W=2, S=2: t_triton=0.125 ms, speed_vs_torch=1.729x, \n",
      "BM=64, BN=128, BK=32, W=2, S=3: t_triton=0.109 ms, speed_vs_torch=1.946x, \n",
      "BM=64, BN=128, BK=32, W=4, S=2: t_triton=0.124 ms, speed_vs_torch=1.756x, \n",
      "BM=64, BN=128, BK=32, W=4, S=3: t_triton=0.110 ms, speed_vs_torch=1.948x, \n",
      "BM=64, BN=128, BK=32, W=8, S=2: t_triton=0.179 ms, speed_vs_torch=1.213x, \n",
      "BM=64, BN=128, BK=32, W=8, S=3: t_triton=0.179 ms, speed_vs_torch=1.189x, \n",
      "BM=64, BN=128, BK=64, W=2, S=2: t_triton=0.116 ms, speed_vs_torch=1.846x, \n",
      "BM=64, BN=128, BK=64, W=2, S=3: t_triton=0.103 ms, speed_vs_torch=2.069x, \n",
      "BM=64, BN=128, BK=64, W=4, S=2: t_triton=0.108 ms, speed_vs_torch=1.969x, \n",
      "BM=64, BN=128, BK=64, W=4, S=3: t_triton=0.104 ms, speed_vs_torch=2.104x, \n",
      "BM=64, BN=128, BK=64, W=8, S=2: t_triton=0.164 ms, speed_vs_torch=1.311x, \n",
      "BM=64, BN=128, BK=64, W=8, S=3: t_triton=0.151 ms, speed_vs_torch=1.409x, \n",
      "BM=64, BN=128, BK=128, W=2, S=2: t_triton=0.123 ms, speed_vs_torch=1.742x, \n",
      "BM=64, BN=128, BK=128, W=2, S=3: t_triton=0.159 ms, speed_vs_torch=1.346x, \n",
      "BM=64, BN=128, BK=128, W=4, S=2: t_triton=0.115 ms, speed_vs_torch=1.845x, \n",
      "BM=64, BN=128, BK=128, W=4, S=3: t_triton=0.109 ms, speed_vs_torch=1.955x, \n",
      "BM=64, BN=128, BK=128, W=8, S=2: t_triton=0.161 ms, speed_vs_torch=1.387x, \n",
      "BM=64, BN=128, BK=128, W=8, S=3: t_triton=0.151 ms, speed_vs_torch=1.410x, \n",
      "BM=128, BN=32, BK=32, W=2, S=2: t_triton=0.131 ms, speed_vs_torch=1.641x, \n",
      "BM=128, BN=32, BK=32, W=2, S=3: t_triton=0.135 ms, speed_vs_torch=1.601x, \n",
      "BM=128, BN=32, BK=32, W=4, S=2: t_triton=0.187 ms, speed_vs_torch=1.171x, \n",
      "BM=128, BN=32, BK=32, W=4, S=3: t_triton=0.179 ms, speed_vs_torch=1.200x, \n",
      "BM=128, BN=32, BK=32, W=8, S=2: t_triton=0.205 ms, speed_vs_torch=1.070x, \n",
      "BM=128, BN=32, BK=32, W=8, S=3: t_triton=0.207 ms, speed_vs_torch=1.054x, \n",
      "BM=128, BN=32, BK=64, W=2, S=2: t_triton=0.118 ms, speed_vs_torch=1.847x, \n",
      "BM=128, BN=32, BK=64, W=2, S=3: t_triton=0.124 ms, speed_vs_torch=1.737x, \n",
      "BM=128, BN=32, BK=64, W=4, S=2: t_triton=0.164 ms, speed_vs_torch=1.374x, \n",
      "BM=128, BN=32, BK=64, W=4, S=3: t_triton=0.169 ms, speed_vs_torch=1.281x, \n",
      "BM=128, BN=32, BK=64, W=8, S=2: t_triton=0.178 ms, speed_vs_torch=1.233x, \n",
      "BM=128, BN=32, BK=64, W=8, S=3: t_triton=0.178 ms, speed_vs_torch=1.234x, \n",
      "BM=128, BN=32, BK=128, W=2, S=2: t_triton=0.132 ms, speed_vs_torch=1.655x, \n",
      "BM=128, BN=32, BK=128, W=2, S=3: t_triton=0.161 ms, speed_vs_torch=1.330x, \n",
      "BM=128, BN=32, BK=128, W=4, S=2: t_triton=0.188 ms, speed_vs_torch=1.136x, \n",
      "BM=128, BN=32, BK=128, W=4, S=3: t_triton=0.185 ms, speed_vs_torch=1.175x, \n",
      "BM=128, BN=32, BK=128, W=8, S=2: t_triton=0.185 ms, speed_vs_torch=1.166x, \n",
      "BM=128, BN=32, BK=128, W=8, S=3: t_triton=0.197 ms, speed_vs_torch=1.178x, \n",
      "BM=128, BN=64, BK=32, W=2, S=2: t_triton=0.142 ms, speed_vs_torch=1.658x, \n",
      "BM=128, BN=64, BK=32, W=2, S=3: t_triton=0.123 ms, speed_vs_torch=1.863x, \n",
      "BM=128, BN=64, BK=32, W=4, S=2: t_triton=0.137 ms, speed_vs_torch=1.661x, \n",
      "BM=128, BN=64, BK=32, W=4, S=3: t_triton=0.125 ms, speed_vs_torch=1.818x, \n",
      "BM=128, BN=64, BK=32, W=8, S=2: t_triton=0.210 ms, speed_vs_torch=1.089x, \n",
      "BM=128, BN=64, BK=32, W=8, S=3: t_triton=0.163 ms, speed_vs_torch=1.542x, \n",
      "BM=128, BN=64, BK=64, W=2, S=2: t_triton=0.122 ms, speed_vs_torch=1.765x, \n",
      "BM=128, BN=64, BK=64, W=2, S=3: t_triton=0.111 ms, speed_vs_torch=1.920x, \n",
      "BM=128, BN=64, BK=64, W=4, S=2: t_triton=0.109 ms, speed_vs_torch=1.950x, \n",
      "BM=128, BN=64, BK=64, W=4, S=3: t_triton=0.112 ms, speed_vs_torch=1.967x, \n",
      "BM=128, BN=64, BK=64, W=8, S=2: t_triton=0.164 ms, speed_vs_torch=1.327x, \n",
      "BM=128, BN=64, BK=64, W=8, S=3: t_triton=0.150 ms, speed_vs_torch=1.420x, \n",
      "BM=128, BN=64, BK=128, W=2, S=2: t_triton=0.190 ms, speed_vs_torch=1.131x, \n",
      "BM=128, BN=64, BK=128, W=2, S=3: t_triton=0.205 ms, speed_vs_torch=1.037x, \n",
      "BM=128, BN=64, BK=128, W=4, S=2: t_triton=0.119 ms, speed_vs_torch=1.822x, \n",
      "BM=128, BN=64, BK=128, W=4, S=3: t_triton=0.126 ms, speed_vs_torch=1.716x, \n",
      "BM=128, BN=64, BK=128, W=8, S=2: t_triton=0.151 ms, speed_vs_torch=1.410x, \n",
      "BM=128, BN=64, BK=128, W=8, S=3: t_triton=0.157 ms, speed_vs_torch=1.361x, \n",
      "BM=128, BN=128, BK=32, W=2, S=2: t_triton=0.387 ms, speed_vs_torch=0.551x, \n",
      "BM=128, BN=128, BK=32, W=2, S=3: t_triton=0.465 ms, speed_vs_torch=0.458x, \n",
      "BM=128, BN=128, BK=32, W=4, S=2: t_triton=0.121 ms, speed_vs_torch=1.765x, \n",
      "BM=128, BN=128, BK=32, W=4, S=3: t_triton=0.111 ms, speed_vs_torch=1.920x, \n",
      "BM=128, BN=128, BK=32, W=8, S=2: t_triton=0.143 ms, speed_vs_torch=1.493x, \n",
      "BM=128, BN=128, BK=32, W=8, S=3: t_triton=0.127 ms, speed_vs_torch=1.677x, \n",
      "BM=128, BN=128, BK=64, W=2, S=2: t_triton=0.510 ms, speed_vs_torch=0.419x, \n",
      "BM=128, BN=128, BK=64, W=2, S=3: t_triton=0.590 ms, speed_vs_torch=0.361x, \n",
      "BM=128, BN=128, BK=64, W=4, S=2: t_triton=0.109 ms, speed_vs_torch=1.999x, \n",
      "BM=128, BN=128, BK=64, W=4, S=3: t_triton=0.099 ms, speed_vs_torch=2.177x, \n",
      "BM=128, BN=128, BK=64, W=8, S=2: t_triton=0.159 ms, speed_vs_torch=1.340x, \n",
      "BM=128, BN=128, BK=64, W=8, S=3: t_triton=0.124 ms, speed_vs_torch=1.724x, \n",
      "BM=128, BN=128, BK=128, W=2, S=2: t_triton=0.581 ms, speed_vs_torch=0.367x, \n",
      "BM=128, BN=128, BK=128, W=2, S=3: t_triton=0.600 ms, speed_vs_torch=0.362x, \n",
      "BM=128, BN=128, BK=128, W=4, S=2: t_triton=0.130 ms, speed_vs_torch=1.632x, \n",
      "BM=128, BN=128, BK=128, W=4, S=3: t_triton=0.173 ms, speed_vs_torch=1.324x, \n",
      "BM=128, BN=128, BK=128, W=8, S=2: t_triton=0.173 ms, speed_vs_torch=1.316x, \n",
      "BM=128, BN=128, BK=128, W=8, S=3: t_triton=0.120 ms, speed_vs_torch=1.896x, \n"
     ]
    }
   ],
   "source": [
    "df_gemm_tiles = tune_gemm_int8_tiles_for_shape(\n",
    "    M=4096, K=1024, N=1024,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c29522-a1df-4062-adbc-9edc3a7e4709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_info</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch16_ms</th>\n",
       "      <th>speed_vs_torch16</th>\n",
       "      <th>mean_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.099355</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>2.176926</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104093</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>2.103508</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.213402</td>\n",
       "      <td>2.068533</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.217836</td>\n",
       "      <td>1.998669</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.108386</td>\n",
       "      <td>0.213393</td>\n",
       "      <td>1.968820</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shape_info  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "0  4096/1024/1024      128      128       64          4           3   \n",
       "1  4096/1024/1024       64      128       64          4           3   \n",
       "2  4096/1024/1024       64      128       64          2           3   \n",
       "3  4096/1024/1024      128      128       64          4           2   \n",
       "4  4096/1024/1024       64      128       64          4           2   \n",
       "\n",
       "   t_triton_ms  t_torch16_ms  speed_vs_torch16  mean_abs_err  \n",
       "0     0.099355      0.216288          2.176926      0.001488  \n",
       "1     0.104093      0.218960          2.103508      0.001488  \n",
       "2     0.103166      0.213402          2.068533      0.001487  \n",
       "3     0.108991      0.217836          1.998669      0.001489  \n",
       "4     0.108386      0.213393          1.968820      0.001487  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemm_tiles[\"shape_info\"] = (\n",
    "      \"4096/1024/1024\"\n",
    "\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"shape_info\",\n",
    "    \"BLOCK_M\", \"BLOCK_N\", \"BLOCK_K\",\n",
    "    \"num_warps\", \"num_stages\",\n",
    "    \"t_triton_ms\", \"t_torch16_ms\",\n",
    "    \"speed_vs_torch16\",\n",
    "     \"mean_abs_err\"\n",
    "]\n",
    "\n",
    "df_gemm__filtered = df_gemm_tiles[cols].sort_values(\"speed_vs_torch16\", ascending=False).head(5).reset_index(drop=True)\n",
    "df_gemm__filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa596d30-56f6-4fce-ae4e-684416db22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_GEMM_BEST_BLOCK_M = 128\n",
    "INT8_GEMM_BEST_BLOCK_N = 0\n",
    "INT8_GEMM_BEST_BLOCK_K = 64\n",
    "INT8_GEMM_BEST_WARPS   = 4\n",
    "INT8_GEMM_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52cc783a-dfe5-4306-9f8b-b4e9eee2cb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_info</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch16_ms</th>\n",
       "      <th>speed_vs_torch16</th>\n",
       "      <th>mean_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GEMM_INT8</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.099355</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>2.176926</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_INT8</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104093</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>2.103508</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_INT8</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.213402</td>\n",
       "      <td>2.068533</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_INT8</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.217836</td>\n",
       "      <td>1.998669</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEMM_INT8</th>\n",
       "      <td>4096/1024/1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.108386</td>\n",
       "      <td>0.213393</td>\n",
       "      <td>1.968820</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               shape_info  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "GEMM_INT8  4096/1024/1024      128      128       64          4           3   \n",
       "GEMM_INT8  4096/1024/1024       64      128       64          4           3   \n",
       "GEMM_INT8  4096/1024/1024       64      128       64          2           3   \n",
       "GEMM_INT8  4096/1024/1024      128      128       64          4           2   \n",
       "GEMM_INT8  4096/1024/1024       64      128       64          4           2   \n",
       "\n",
       "           t_triton_ms  t_torch16_ms  speed_vs_torch16  mean_abs_err  \n",
       "GEMM_INT8     0.099355      0.216288          2.176926      0.001488  \n",
       "GEMM_INT8     0.104093      0.218960          2.103508      0.001488  \n",
       "GEMM_INT8     0.103166      0.213402          2.068533      0.001487  \n",
       "GEMM_INT8     0.108991      0.217836          1.998669      0.001489  \n",
       "GEMM_INT8     0.108386      0.213393          1.968820      0.001487  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_gemm__filtered.index = [\"GEMM_INT8\"] * len(df_gemm__filtered)\n",
    "df_gemm__filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588e49e-84e6-4dd0-b9a0-73010622b059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de16cb-e4b7-4598-9f28-5bfd9c6045b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942eb61-f0a3-4b67-bd66-e82b70f809bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
