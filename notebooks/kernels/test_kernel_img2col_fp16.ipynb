{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fb587b-ac45-4881-bdf9-74c3dd4e70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384c9f99-93b7-4e60-be40-8978e48c6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from conv_gemm.triton_kernels.fp16.img2col_kernel import img2col_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7fadb5-f7f5-4944-8a32-231bb4220b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2col_fp16(\n",
    "    x,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "):\n",
    "    assert x.is_cuda\n",
    "    assert x.dtype in (torch.float16, torch.float32)\n",
    "\n",
    "    N, Cin, H, W = x.shape\n",
    "\n",
    "    Ho = (H + 2*Ph - Dh*(Kh - 1) - 1)//Sh + 1\n",
    "    Wo = (W + 2*Pw - Dw*(Kw - 1) - 1)//Sw + 1\n",
    "\n",
    "    M = N * Ho * Wo\n",
    "    K = Cin * Kh * Kw\n",
    "\n",
    "    cols = torch.empty((M, K), dtype=x.dtype, device=x.device)\n",
    "    sN, sC, sH, sW = x.stride()\n",
    "\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "\n",
    "    img2col_kernel[grid](\n",
    "        x, cols,\n",
    "        N, Cin, H, W,\n",
    "        Kh, Kw, Sh, Sw, Ph, Pw, Dh, Dw,\n",
    "        Ho, Wo,\n",
    "        sN, sC, sH, sW,\n",
    "        K,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        CAST_FP16=(x.dtype == torch.float16),\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return cols, (Ho, Wo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a2176c-1347-4bfd-98a2-67ff9dc73412",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_img2col_fp16_vs_unfold(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "\n",
    "    x = torch.randn((N, Cin, H, W), device=device, dtype=torch.float16)\n",
    "    x_f16 = x.float().half()\n",
    "\n",
    "    Ho = (H + 2 * Ph - Dh * (Kh - 1) - 1) // Sh + 1\n",
    "    Wo = (W + 2 * Pw - Dw * (Kw - 1) - 1) // Sw + 1\n",
    "\n",
    "    K = Cin * Kh * Kw\n",
    "    M = N * Ho * Wo\n",
    "\n",
    "    # TORCH \n",
    "    def _call_torch():\n",
    "        return F.unfold(\n",
    "            x_f16,\n",
    "            kernel_size=(Kh, Kw),\n",
    "            dilation=(Dh, Dw),\n",
    "            padding=(Ph, Pw),\n",
    "            stride=(Sh, Sw),\n",
    "        )\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        unf = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # torch output → [M, K]\n",
    "    unf_M_K = unf.permute(0, 2, 1).contiguous().view(M, K)\n",
    "\n",
    "    # ----- TRITON FP16 IMG2COL -----\n",
    "    def _call_triton():\n",
    "        cols, (Ho2, Wo2) = img2col_fp16(\n",
    "            x,\n",
    "            Kh, Kw,\n",
    "            Sh, Sw,\n",
    "            Ph, Pw,\n",
    "            Dh, Dw,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        assert Ho2 == Ho and Wo2 == Wo\n",
    "        return cols\n",
    "\n",
    "    for _ in range(5):\n",
    "        _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        cols = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Error Triton INT32 vs Torch FP16\n",
    "    cols_f32 = cols.float()\n",
    "    unf_f32  = unf_M_K.float()\n",
    "\n",
    "    diff = (cols_f32 - unf_f32).abs()\n",
    "    max_abs_err16 = diff.max().item()\n",
    "    mean_abs_err16 = diff.mean().item()\n",
    "    \n",
    "    # bandwidth (FP16 → 2 bytes)\n",
    "    bytes_moved = 2.0 * M * K * 2   # load + store * fp16(2 bytes)\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": 0,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch16_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch16\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch16_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err16,\n",
    "        \"mean_abs_err\": mean_abs_err16,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dd2b6f-b84c-45e0-b254-a162368c30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_img2col_fp16_tiles_for_shape(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3, 4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BK in blocks_K:\n",
    "            for W in warps:\n",
    "                for S in stages:\n",
    "                    try:\n",
    "                        rec = bench_once_img2col_fp16_vs_unfold(\n",
    "                            N, Cin, H, W,\n",
    "                            Kh, Kw,\n",
    "                            Sh, Sw,\n",
    "                            Ph, Pw,\n",
    "                            Dh, Dw,\n",
    "                            BLOCK_M=BM,\n",
    "                            BLOCK_K=BK,\n",
    "                            num_warps=W,\n",
    "                            num_stages=S,\n",
    "                            iters=iters,\n",
    "                            device=device,\n",
    "                        )\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"[SKIP] BM={BM}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    print(\n",
    "                        f\"BM={BM}, BK={BK}, W={W}, S={S}: \"\n",
    "                        f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                        f\"speed_vs_torch={rec['speed_vs_torch16']:.3f}x, \"\n",
    "                    )\n",
    "                    records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this IMG2COL shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc4759a-7e78-4389-8d6d-5e03ca6814c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BK=32, W=2, S=2: t_triton=0.036 ms, speed_vs_torch=5.781x, \n",
      "BM=32, BK=32, W=2, S=3: t_triton=0.036 ms, speed_vs_torch=5.752x, \n",
      "BM=32, BK=32, W=4, S=2: t_triton=0.036 ms, speed_vs_torch=10.757x, \n",
      "BM=32, BK=32, W=4, S=3: t_triton=0.038 ms, speed_vs_torch=10.220x, \n",
      "BM=32, BK=32, W=8, S=2: t_triton=0.047 ms, speed_vs_torch=7.738x, \n",
      "BM=32, BK=32, W=8, S=3: t_triton=0.047 ms, speed_vs_torch=7.705x, \n",
      "BM=32, BK=32, W=16, S=2: t_triton=0.113 ms, speed_vs_torch=3.332x, \n",
      "BM=32, BK=32, W=16, S=3: t_triton=0.115 ms, speed_vs_torch=3.242x, \n",
      "BM=32, BK=64, W=2, S=2: t_triton=0.036 ms, speed_vs_torch=5.511x, \n",
      "BM=32, BK=64, W=2, S=3: t_triton=0.039 ms, speed_vs_torch=5.057x, \n",
      "BM=32, BK=64, W=4, S=2: t_triton=0.036 ms, speed_vs_torch=10.352x, \n",
      "BM=32, BK=64, W=4, S=3: t_triton=0.036 ms, speed_vs_torch=9.988x, \n",
      "BM=32, BK=64, W=8, S=2: t_triton=0.041 ms, speed_vs_torch=8.930x, \n",
      "BM=32, BK=64, W=8, S=3: t_triton=0.041 ms, speed_vs_torch=8.757x, \n",
      "BM=32, BK=64, W=16, S=2: t_triton=0.100 ms, speed_vs_torch=3.847x, \n",
      "BM=32, BK=64, W=16, S=3: t_triton=0.092 ms, speed_vs_torch=4.189x, \n",
      "BM=32, BK=128, W=2, S=2: t_triton=0.036 ms, speed_vs_torch=5.578x, \n",
      "BM=32, BK=128, W=2, S=3: t_triton=0.036 ms, speed_vs_torch=5.537x, \n",
      "BM=32, BK=128, W=4, S=2: t_triton=0.036 ms, speed_vs_torch=10.067x, \n",
      "BM=32, BK=128, W=4, S=3: t_triton=0.036 ms, speed_vs_torch=10.147x, \n",
      "BM=32, BK=128, W=8, S=2: t_triton=0.041 ms, speed_vs_torch=8.931x, \n",
      "BM=32, BK=128, W=8, S=3: t_triton=0.039 ms, speed_vs_torch=9.549x, \n",
      "BM=32, BK=128, W=16, S=2: t_triton=0.080 ms, speed_vs_torch=4.624x, \n",
      "BM=32, BK=128, W=16, S=3: t_triton=0.080 ms, speed_vs_torch=4.564x, \n",
      "BM=64, BK=32, W=2, S=2: t_triton=0.035 ms, speed_vs_torch=5.772x, \n",
      "BM=64, BK=32, W=2, S=3: t_triton=0.036 ms, speed_vs_torch=5.598x, \n",
      "BM=64, BK=32, W=4, S=2: t_triton=0.035 ms, speed_vs_torch=10.375x, \n",
      "BM=64, BK=32, W=4, S=3: t_triton=0.036 ms, speed_vs_torch=10.226x, \n",
      "BM=64, BK=32, W=8, S=2: t_triton=0.042 ms, speed_vs_torch=8.768x, \n",
      "BM=64, BK=32, W=8, S=3: t_triton=0.042 ms, speed_vs_torch=8.887x, \n",
      "BM=64, BK=32, W=16, S=2: t_triton=0.091 ms, speed_vs_torch=4.002x, \n",
      "BM=64, BK=32, W=16, S=3: t_triton=0.094 ms, speed_vs_torch=3.895x, \n",
      "BM=64, BK=64, W=2, S=2: t_triton=0.036 ms, speed_vs_torch=5.724x, \n",
      "BM=64, BK=64, W=2, S=3: t_triton=0.036 ms, speed_vs_torch=5.549x, \n",
      "BM=64, BK=64, W=4, S=2: t_triton=0.035 ms, speed_vs_torch=10.292x, \n",
      "BM=64, BK=64, W=4, S=3: t_triton=0.035 ms, speed_vs_torch=10.207x, \n",
      "BM=64, BK=64, W=8, S=2: t_triton=0.041 ms, speed_vs_torch=9.080x, \n",
      "BM=64, BK=64, W=8, S=3: t_triton=0.041 ms, speed_vs_torch=8.992x, \n",
      "BM=64, BK=64, W=16, S=2: t_triton=0.085 ms, speed_vs_torch=4.372x, \n",
      "BM=64, BK=64, W=16, S=3: t_triton=0.081 ms, speed_vs_torch=4.647x, \n",
      "BM=64, BK=128, W=2, S=2: t_triton=0.050 ms, speed_vs_torch=3.976x, \n",
      "BM=64, BK=128, W=2, S=3: t_triton=0.049 ms, speed_vs_torch=4.034x, \n",
      "BM=64, BK=128, W=4, S=2: t_triton=0.036 ms, speed_vs_torch=10.026x, \n",
      "BM=64, BK=128, W=4, S=3: t_triton=0.036 ms, speed_vs_torch=10.140x, \n",
      "BM=64, BK=128, W=8, S=2: t_triton=0.051 ms, speed_vs_torch=7.221x, \n",
      "BM=64, BK=128, W=8, S=3: t_triton=0.048 ms, speed_vs_torch=7.651x, \n",
      "BM=64, BK=128, W=16, S=2: t_triton=0.078 ms, speed_vs_torch=4.707x, \n",
      "BM=64, BK=128, W=16, S=3: t_triton=0.078 ms, speed_vs_torch=4.772x, \n",
      "BM=128, BK=32, W=2, S=2: t_triton=0.036 ms, speed_vs_torch=5.570x, \n",
      "BM=128, BK=32, W=2, S=3: t_triton=0.035 ms, speed_vs_torch=5.635x, \n",
      "BM=128, BK=32, W=4, S=2: t_triton=0.035 ms, speed_vs_torch=10.208x, \n",
      "BM=128, BK=32, W=4, S=3: t_triton=0.036 ms, speed_vs_torch=10.185x, \n",
      "BM=128, BK=32, W=8, S=2: t_triton=0.039 ms, speed_vs_torch=9.411x, \n",
      "BM=128, BK=32, W=8, S=3: t_triton=0.039 ms, speed_vs_torch=9.326x, \n",
      "BM=128, BK=32, W=16, S=2: t_triton=0.080 ms, speed_vs_torch=4.624x, \n",
      "BM=128, BK=32, W=16, S=3: t_triton=0.080 ms, speed_vs_torch=4.562x, \n",
      "BM=128, BK=64, W=2, S=2: t_triton=0.035 ms, speed_vs_torch=5.643x, \n",
      "BM=128, BK=64, W=2, S=3: t_triton=0.035 ms, speed_vs_torch=5.770x, \n",
      "BM=128, BK=64, W=4, S=2: t_triton=0.034 ms, speed_vs_torch=10.768x, \n",
      "BM=128, BK=64, W=4, S=3: t_triton=0.035 ms, speed_vs_torch=10.272x, \n",
      "BM=128, BK=64, W=8, S=2: t_triton=0.050 ms, speed_vs_torch=7.357x, \n",
      "BM=128, BK=64, W=8, S=3: t_triton=0.050 ms, speed_vs_torch=7.453x, \n",
      "BM=128, BK=64, W=16, S=2: t_triton=0.087 ms, speed_vs_torch=4.197x, \n",
      "BM=128, BK=64, W=16, S=3: t_triton=0.087 ms, speed_vs_torch=4.196x, \n",
      "BM=128, BK=128, W=2, S=2: t_triton=0.050 ms, speed_vs_torch=4.091x, \n",
      "BM=128, BK=128, W=2, S=3: t_triton=0.049 ms, speed_vs_torch=4.078x, \n",
      "BM=128, BK=128, W=4, S=2: t_triton=0.123 ms, speed_vs_torch=2.946x, \n",
      "BM=128, BK=128, W=4, S=3: t_triton=0.122 ms, speed_vs_torch=2.988x, \n",
      "BM=128, BK=128, W=8, S=2: t_triton=0.052 ms, speed_vs_torch=6.958x, \n",
      "BM=128, BK=128, W=8, S=3: t_triton=0.052 ms, speed_vs_torch=7.010x, \n",
      "BM=128, BK=128, W=16, S=2: t_triton=0.095 ms, speed_vs_torch=3.908x, \n",
      "BM=128, BK=128, W=16, S=3: t_triton=0.095 ms, speed_vs_torch=3.856x, \n"
     ]
    }
   ],
   "source": [
    "df_fp16_i2c = tune_img2col_fp16_tiles_for_shape(\n",
    "    N=16, Cin=1, H=256, W=256,\n",
    "    Kh=11, Kw=11,\n",
    "    Sh=1, Sw=1,\n",
    "    Ph=5, Pw=5,\n",
    "    Dh=1, Dw=1,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8, 16),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4a746c-1482-44ac-bc44-3de638cf2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp16_i2c[\"shape_info\"] = (\n",
    "      \"16/1/256/11\"\n",
    "\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"shape_info\",\n",
    "    \"BLOCK_M\", \"BLOCK_N\", \"BLOCK_K\",\n",
    "    \"num_warps\", \"num_stages\",\n",
    "    \"t_triton_ms\", \"t_torch16_ms\",\n",
    "    \"speed_vs_torch16\",\n",
    "     \"mean_abs_err\"\n",
    "]\n",
    "\n",
    "df_i2c_filtered = df_fp16_i2c[cols].sort_values(\"speed_vs_torch16\", ascending=False).head(5).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696ecc27-cf08-4976-8ca9-e97fbaeb74b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_info</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch16_ms</th>\n",
       "      <th>speed_vs_torch16</th>\n",
       "      <th>mean_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IMG2COL_FP16</th>\n",
       "      <td>16/1/256/11</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034008</td>\n",
       "      <td>0.366213</td>\n",
       "      <td>10.768357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG2COL_FP16</th>\n",
       "      <td>16/1/256/11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.036109</td>\n",
       "      <td>0.388443</td>\n",
       "      <td>10.757461</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG2COL_FP16</th>\n",
       "      <td>16/1/256/11</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035246</td>\n",
       "      <td>0.365672</td>\n",
       "      <td>10.374785</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG2COL_FP16</th>\n",
       "      <td>16/1/256/11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035924</td>\n",
       "      <td>0.371879</td>\n",
       "      <td>10.351838</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG2COL_FP16</th>\n",
       "      <td>16/1/256/11</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035085</td>\n",
       "      <td>0.361113</td>\n",
       "      <td>10.292436</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               shape_info  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "IMG2COL_FP16  16/1/256/11      128        0       64          4           2   \n",
       "IMG2COL_FP16  16/1/256/11       32        0       32          4           2   \n",
       "IMG2COL_FP16  16/1/256/11       64        0       32          4           2   \n",
       "IMG2COL_FP16  16/1/256/11       32        0       64          4           2   \n",
       "IMG2COL_FP16  16/1/256/11       64        0       64          4           2   \n",
       "\n",
       "              t_triton_ms  t_torch16_ms  speed_vs_torch16  mean_abs_err  \n",
       "IMG2COL_FP16     0.034008      0.366213         10.768357           0.0  \n",
       "IMG2COL_FP16     0.036109      0.388443         10.757461           0.0  \n",
       "IMG2COL_FP16     0.035246      0.365672         10.374785           0.0  \n",
       "IMG2COL_FP16     0.035924      0.371879         10.351838           0.0  \n",
       "IMG2COL_FP16     0.035085      0.361113         10.292436           0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i2c_filtered.index = [\"IMG2COL_FP16\"] * len(df_i2c_filtered)\n",
    "df_i2c_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d09d25e-dc81-45bc-be97-23ef9846d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP16_IMG2COL_BEST_BLOCK_M = 32\n",
    "FP16_IMG2COL_BEST_BLOCK_N = 0\n",
    "FP16_IMG2COL_BEST_BLOCK_K = 64\n",
    "FP16_IMG2COL_BEST_WARPS   = 2\n",
    "FP16_IMG2COL_BEST_STAGES  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26913e2-9fe1-4376-bd93-0ca0524320d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
