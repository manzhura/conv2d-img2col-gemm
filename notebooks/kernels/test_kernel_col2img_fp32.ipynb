{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be973f9b-e119-4ba7-88fb-80ddd5cd3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d724450-fbce-49f9-b0db-52d4a09f8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from conv_gemm.triton_kernels.fp16.col2img_kernel import col2img_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd5df77-3caf-45a3-b81c-687803f54f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2img_fp32(\n",
    "    cols_f32: torch.Tensor,\n",
    "    N: int, Cin: int,\n",
    "    H: int, W: int,\n",
    "    Kh: int, Kw: int,\n",
    "    Sh: int, Sw: int,\n",
    "    Ph: int, Pw: int,\n",
    "    Dh: int, Dw: int,\n",
    "    BLOCK_M: int,\n",
    "    BLOCK_K: int,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    assert cols_f32.is_cuda\n",
    "    assert cols_f32.dtype == torch.float32\n",
    "    cols_f32 = cols_f32.contiguous()\n",
    "\n",
    "    Ho = (H + 2*Ph - Dh*(Kh-1) - 1)//Sh + 1\n",
    "    Wo = (W + 2*Pw - Dw*(Kw-1) - 1)//Sw + 1\n",
    "    M  = N * Ho * Wo\n",
    "    K  = Cin * Kh * Kw\n",
    "\n",
    "    assert cols_f32.shape == (M, K), f\"cols shape {cols_f32.shape}, expected {(M, K)}\"\n",
    "\n",
    "    x_f32 = torch.zeros((N, Cin, H, W), device=cols_f32.device, dtype=torch.float32)\n",
    "    sN, sC, sH, sW = x_f32.stride()\n",
    "\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "\n",
    "    col2img_kernel[grid](\n",
    "        cols_f32, x_f32,\n",
    "        N, Cin, H, W,\n",
    "        Kh, Kw, Sh, Sw, Ph, Pw, Dh, Dw,\n",
    "        Ho, Wo,\n",
    "        sN, sC, sH, sW,\n",
    "        K,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return x_f32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becfc9fa-38ea-423b-9a54-cbf31fb22697",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_col2img_fp32_vs_torch(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    FP32 Triton col2img vs torch F.fold    \n",
    "    \"\"\"\n",
    "    Ho = (H + 2*Ph - Dh*(Kh-1) - 1)//Sh + 1\n",
    "    Wo = (W + 2*Pw - Dw*(Kw-1) - 1)//Sw + 1\n",
    "    M  = N * Ho * Wo\n",
    "    K  = Cin * Kh * Kw\n",
    "\n",
    "    cols_f32 = torch.randn((M, K), device=device, dtype=torch.float32) #для производной fp32\n",
    "\n",
    "    # Torch F.fold\n",
    "    cols_fold = cols_f32.view(N, Ho*Wo, K).permute(0, 2, 1).contiguous()\n",
    "\n",
    "    def _call_torch():\n",
    "        return F.fold(\n",
    "            cols_fold,\n",
    "            output_size=(H, W),\n",
    "            kernel_size=(Kh, Kw),\n",
    "            dilation=(Dh, Dw),\n",
    "            padding=(Ph, Pw),\n",
    "            stride=(Sh, Sw),\n",
    "        )\n",
    "\n",
    "    for _ in range(5):\n",
    "        _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        x_ref = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # Triton col2img\n",
    "    def _call_triton():\n",
    "        return col2img_fp32(\n",
    "            cols_f32,\n",
    "            N, Cin, H, W,\n",
    "            Kh, Kw,\n",
    "            Sh, Sw,\n",
    "            Ph, Pw,\n",
    "            Dh, Dw,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        x_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # bandwidth\n",
    "    bytes_moved = (cols_f32.numel() + x_i32.numel()) * 4.0\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": 0,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f03996e-c6ee-47d1-a4af-b4bc5015ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_col2img_fp16_tiles_for_shape(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3, 4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BK in blocks_K:\n",
    "            for Wp in warps:\n",
    "                for S in stages:\n",
    "                    try:\n",
    "                        rec = bench_once_col2img_fp32_vs_torch(\n",
    "                            N, Cin, H, W,\n",
    "                            Kh, Kw,\n",
    "                            Sh, Sw,\n",
    "                            Ph, Pw,\n",
    "                            Dh, Dw,\n",
    "                            BLOCK_M=BM,\n",
    "                            BLOCK_K=BK,\n",
    "                            num_warps=Wp,\n",
    "                            num_stages=S,\n",
    "                            iters=iters,\n",
    "                            device=device,\n",
    "                        )\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"[SKIP] BM={BM}, BK={BK}, W={Wp}, S={S}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    print(\n",
    "                        f\"BM={BM}, BK={BK}, W={Wp}, S={S}: \"\n",
    "                        f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                        f\"speed_vs_torch={rec['speed_vs_torch']:.3f}x, \"\n",
    "                    )\n",
    "                    records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this COL2IMG shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7b5710-9471-4f60-8379-a84b50a869ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BK=32, W=2, S=2: t_triton=2.329 ms, speed_vs_torch=1.147x, \n",
      "BM=32, BK=32, W=2, S=3: t_triton=2.353 ms, speed_vs_torch=1.116x, \n",
      "BM=32, BK=32, W=4, S=2: t_triton=1.871 ms, speed_vs_torch=1.414x, \n",
      "BM=32, BK=32, W=4, S=3: t_triton=1.887 ms, speed_vs_torch=1.408x, \n",
      "BM=32, BK=32, W=8, S=2: t_triton=1.957 ms, speed_vs_torch=1.359x, \n",
      "BM=32, BK=32, W=8, S=3: t_triton=1.975 ms, speed_vs_torch=1.337x, \n",
      "BM=32, BK=64, W=2, S=2: t_triton=5.268 ms, speed_vs_torch=0.506x, \n",
      "BM=32, BK=64, W=2, S=3: t_triton=5.307 ms, speed_vs_torch=0.490x, \n",
      "BM=32, BK=64, W=4, S=2: t_triton=2.276 ms, speed_vs_torch=1.144x, \n",
      "BM=32, BK=64, W=4, S=3: t_triton=2.301 ms, speed_vs_torch=1.130x, \n",
      "BM=32, BK=64, W=8, S=2: t_triton=1.913 ms, speed_vs_torch=1.394x, \n",
      "BM=32, BK=64, W=8, S=3: t_triton=1.920 ms, speed_vs_torch=1.369x, \n",
      "BM=32, BK=128, W=2, S=2: t_triton=6.854 ms, speed_vs_torch=0.390x, \n",
      "BM=32, BK=128, W=2, S=3: t_triton=6.941 ms, speed_vs_torch=0.382x, \n",
      "BM=32, BK=128, W=4, S=2: t_triton=5.773 ms, speed_vs_torch=0.461x, \n",
      "BM=32, BK=128, W=4, S=3: t_triton=5.765 ms, speed_vs_torch=0.457x, \n",
      "BM=32, BK=128, W=8, S=2: t_triton=2.950 ms, speed_vs_torch=0.893x, \n",
      "BM=32, BK=128, W=8, S=3: t_triton=2.963 ms, speed_vs_torch=0.892x, \n",
      "BM=64, BK=32, W=2, S=2: t_triton=5.420 ms, speed_vs_torch=0.486x, \n",
      "BM=64, BK=32, W=2, S=3: t_triton=5.442 ms, speed_vs_torch=0.480x, \n",
      "BM=64, BK=32, W=4, S=2: t_triton=2.370 ms, speed_vs_torch=1.129x, \n",
      "BM=64, BK=32, W=4, S=3: t_triton=2.362 ms, speed_vs_torch=1.111x, \n",
      "BM=64, BK=32, W=8, S=2: t_triton=1.971 ms, speed_vs_torch=1.297x, \n",
      "BM=64, BK=32, W=8, S=3: t_triton=1.949 ms, speed_vs_torch=1.377x, \n",
      "BM=64, BK=64, W=2, S=2: t_triton=6.903 ms, speed_vs_torch=0.390x, \n",
      "BM=64, BK=64, W=2, S=3: t_triton=6.895 ms, speed_vs_torch=0.393x, \n",
      "BM=64, BK=64, W=4, S=2: t_triton=5.388 ms, speed_vs_torch=0.492x, \n",
      "BM=64, BK=64, W=4, S=3: t_triton=5.365 ms, speed_vs_torch=0.493x, \n",
      "BM=64, BK=64, W=8, S=2: t_triton=2.372 ms, speed_vs_torch=1.101x, \n",
      "BM=64, BK=64, W=8, S=3: t_triton=2.385 ms, speed_vs_torch=1.102x, \n",
      "BM=64, BK=128, W=2, S=2: t_triton=27.952 ms, speed_vs_torch=0.096x, \n",
      "BM=64, BK=128, W=2, S=3: t_triton=27.923 ms, speed_vs_torch=0.094x, \n",
      "BM=64, BK=128, W=4, S=2: t_triton=6.376 ms, speed_vs_torch=0.414x, \n",
      "BM=64, BK=128, W=4, S=3: t_triton=6.434 ms, speed_vs_torch=0.417x, \n",
      "BM=64, BK=128, W=8, S=2: t_triton=6.080 ms, speed_vs_torch=0.433x, \n",
      "BM=64, BK=128, W=8, S=3: t_triton=6.045 ms, speed_vs_torch=0.435x, \n",
      "BM=128, BK=32, W=2, S=2: t_triton=5.457 ms, speed_vs_torch=0.490x, \n",
      "BM=128, BK=32, W=2, S=3: t_triton=5.356 ms, speed_vs_torch=0.491x, \n",
      "BM=128, BK=32, W=4, S=2: t_triton=5.307 ms, speed_vs_torch=0.500x, \n",
      "BM=128, BK=32, W=4, S=3: t_triton=5.362 ms, speed_vs_torch=0.487x, \n",
      "BM=128, BK=32, W=8, S=2: t_triton=2.507 ms, speed_vs_torch=1.048x, \n",
      "BM=128, BK=32, W=8, S=3: t_triton=2.520 ms, speed_vs_torch=1.058x, \n",
      "BM=128, BK=64, W=2, S=2: t_triton=20.401 ms, speed_vs_torch=0.129x, \n",
      "BM=128, BK=64, W=2, S=3: t_triton=20.392 ms, speed_vs_torch=0.127x, \n",
      "BM=128, BK=64, W=4, S=2: t_triton=7.152 ms, speed_vs_torch=0.370x, \n",
      "BM=128, BK=64, W=4, S=3: t_triton=7.171 ms, speed_vs_torch=0.372x, \n",
      "BM=128, BK=64, W=8, S=2: t_triton=5.893 ms, speed_vs_torch=0.451x, \n",
      "BM=128, BK=64, W=8, S=3: t_triton=5.931 ms, speed_vs_torch=0.445x, \n",
      "BM=128, BK=128, W=2, S=2: t_triton=37.780 ms, speed_vs_torch=0.069x, \n",
      "BM=128, BK=128, W=2, S=3: t_triton=37.548 ms, speed_vs_torch=0.070x, \n",
      "BM=128, BK=128, W=4, S=2: t_triton=61.990 ms, speed_vs_torch=0.042x, \n",
      "BM=128, BK=128, W=4, S=3: t_triton=62.272 ms, speed_vs_torch=0.042x, \n",
      "BM=128, BK=128, W=8, S=2: t_triton=6.790 ms, speed_vs_torch=0.389x, \n",
      "BM=128, BK=128, W=8, S=3: t_triton=6.797 ms, speed_vs_torch=0.391x, \n"
     ]
    }
   ],
   "source": [
    "df_c2i = tune_col2img_fp16_tiles_for_shape(\n",
    "    N=16, Cin=1, H=256, W=256,\n",
    "    Kh=11, Kw=11,\n",
    "    Sh=1, Sw=1,\n",
    "    Ph=5, Pw=5,\n",
    "    Dh=1, Dw=1,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "264be19f-8b5b-406e-9cca-5bc278cd5aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch_ms</th>\n",
       "      <th>speed_vs_torch</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_torch_GBs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.871308</td>\n",
       "      <td>2.645449</td>\n",
       "      <td>1.413690</td>\n",
       "      <td>273.447866</td>\n",
       "      <td>193.428473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.886957</td>\n",
       "      <td>2.656555</td>\n",
       "      <td>1.407851</td>\n",
       "      <td>271.179978</td>\n",
       "      <td>192.619772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.913415</td>\n",
       "      <td>2.666362</td>\n",
       "      <td>1.393510</td>\n",
       "      <td>267.430328</td>\n",
       "      <td>191.911345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.920111</td>\n",
       "      <td>2.627812</td>\n",
       "      <td>1.368573</td>\n",
       "      <td>266.497707</td>\n",
       "      <td>194.726636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.949236</td>\n",
       "      <td>2.684255</td>\n",
       "      <td>1.377081</td>\n",
       "      <td>262.515757</td>\n",
       "      <td>190.632092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.956542</td>\n",
       "      <td>2.658743</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>261.535489</td>\n",
       "      <td>192.461319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.971314</td>\n",
       "      <td>2.557700</td>\n",
       "      <td>1.297459</td>\n",
       "      <td>259.575653</td>\n",
       "      <td>200.064563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.975392</td>\n",
       "      <td>2.640665</td>\n",
       "      <td>1.336780</td>\n",
       "      <td>259.039748</td>\n",
       "      <td>193.778904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.275886</td>\n",
       "      <td>2.602551</td>\n",
       "      <td>1.143533</td>\n",
       "      <td>224.837718</td>\n",
       "      <td>196.616742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1048576</td>\n",
       "      <td>121</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.300876</td>\n",
       "      <td>2.600257</td>\n",
       "      <td>1.130116</td>\n",
       "      <td>222.395788</td>\n",
       "      <td>196.790205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          M    K   N  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "2   1048576  121  16       32        0       32          4           2   \n",
       "3   1048576  121  16       32        0       32          4           3   \n",
       "10  1048576  121  16       32        0       64          8           2   \n",
       "11  1048576  121  16       32        0       64          8           3   \n",
       "23  1048576  121  16       64        0       32          8           3   \n",
       "4   1048576  121  16       32        0       32          8           2   \n",
       "22  1048576  121  16       64        0       32          8           2   \n",
       "5   1048576  121  16       32        0       32          8           3   \n",
       "8   1048576  121  16       32        0       64          4           2   \n",
       "9   1048576  121  16       32        0       64          4           3   \n",
       "\n",
       "    t_triton_ms  t_torch_ms  speed_vs_torch  bw_triton_GBs  bw_torch_GBs  \n",
       "2      1.871308    2.645449        1.413690     273.447866    193.428473  \n",
       "3      1.886957    2.656555        1.407851     271.179978    192.619772  \n",
       "10     1.913415    2.666362        1.393510     267.430328    191.911345  \n",
       "11     1.920111    2.627812        1.368573     266.497707    194.726636  \n",
       "23     1.949236    2.684255        1.377081     262.515757    190.632092  \n",
       "4      1.956542    2.658743        1.358899     261.535489    192.461319  \n",
       "22     1.971314    2.557700        1.297459     259.575653    200.064563  \n",
       "5      1.975392    2.640665        1.336780     259.039748    193.778904  \n",
       "8      2.275886    2.602551        1.143533     224.837718    196.616742  \n",
       "9      2.300876    2.600257        1.130116     222.395788    196.790205  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c2i.sort_values(\"t_triton_ms\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46221b45-83e0-416f-9ff8-e57b8695d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP32_COL2IMG_BEST_BLOCK_M = 32\n",
    "FP32_COL2IMG_BEST_BLOCK_N = 0\n",
    "FP32_COL2IMG_BEST_BLOCK_K = 32\n",
    "FP32_COL2IMG_BEST_WARPS = 4\n",
    "FP32_COL2IMG_BEST_STAGES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915c124-b238-4b34-81f8-1feb8b8984d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
