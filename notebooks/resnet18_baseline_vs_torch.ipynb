{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dffa7c",
   "metadata": {},
   "source": [
    "# ResNet18 Baseline Conv2d Benchmark\n",
    "\n",
    "Сравнение nn.Conv2d и кастомной img2col→GEMM свёртки (Baseline TritonConv2d) на ResNet18 с разными batch size и сценариями спарсификации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319e97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4154eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from conv_gemm.baseline_layers.triton_conv2d import TritonConv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da3ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data_root\": \"/home/manzhura/ITMO/EDLM/conv2d-img2col-gemm/data\",\n",
      "  \"num_classes\": 10,\n",
      "  \"batch_sizes\": [\n",
      "    32,\n",
      "    64,\n",
      "    96,\n",
      "    128,\n",
      "    160,\n",
      "    192,\n",
      "    256\n",
      "  ],\n",
      "  \"num_workers\": 4,\n",
      "  \"train_subset\": 8192,\n",
      "  \"lr\": 0.001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"weight_decay\": 0.0005,\n",
      "  \"warmup_steps\": 5,\n",
      "  \"benchmark_steps\": 40,\n",
      "  \"baseline_conv\": {\n",
      "    \"BLOCK_M\": 64,\n",
      "    \"BLOCK_N\": 64,\n",
      "    \"BLOCK_K\": 64,\n",
      "    \"NUM_WARPS\": 4,\n",
      "    \"NUM_STAGES\": 2\n",
      "  },\n",
      "  \"sparsity_bench\": {\n",
      "    \"modes\": [\n",
      "      \"channel\",\n",
      "      \"block\",\n",
      "      \"input\"\n",
      "    ],\n",
      "    \"keep_ratios\": [\n",
      "      0.75,\n",
      "      0.6,\n",
      "      0.5,\n",
      "      0.25\n",
      "    ],\n",
      "    \"block_size\": 4,\n",
      "    \"batch_size\": 128\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    raise RuntimeError(\"CUDA GPU is required for this benchmark\")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "data_root = Path(\"../data\").resolve()\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"data_root\": str(data_root),\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_sizes\": [32, 64, 96, 128, 160, 192, 256],\n",
    "    \"num_workers\": 4,\n",
    "    \"train_subset\": 8192,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"benchmark_steps\": 40,\n",
    "    \"baseline_conv\": {\n",
    "        \"BLOCK_M\": 64,\n",
    "        \"BLOCK_N\": 64,\n",
    "        \"BLOCK_K\": 64,\n",
    "        \"NUM_WARPS\": 4,\n",
    "        \"NUM_STAGES\": 2,\n",
    "    },\n",
    "    \"sparsity_bench\": {\n",
    "        \"modes\": [\"channel\", \"block\", \"input\"],\n",
    "        \"keep_ratios\": [0.75, 0.6, 0.5, 0.25],\n",
    "        \"block_size\": 4,\n",
    "        \"batch_size\": 128,\n",
    "    },\n",
    "}\n",
    "print(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e44fbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32: 256, 64: 128, 96: 85, 128: 64, 160: 51, 192: 42, 256: 32}\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "full_train = torchvision.datasets.CIFAR10(\n",
    "    root=config[\"data_root\"], train=True, download=True, transform=transform_train\n",
    ")\n",
    "if config[\"train_subset\"] is not None and config[\"train_subset\"] < len(full_train):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    subset_idx = torch.randperm(len(full_train), generator=g)[: config[\"train_subset\"]]\n",
    "    train_dataset = torch.utils.data.Subset(full_train, subset_idx)\n",
    "else:\n",
    "    train_dataset = full_train\n",
    "\n",
    "\n",
    "def make_loader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "train_loaders: Dict[int, DataLoader] = {}\n",
    "for bs in config[\"batch_sizes\"]:\n",
    "    train_loaders[bs] = make_loader(bs)\n",
    "\n",
    "print({bs: len(loader) for bs, loader in train_loaders.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbd874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triton_conv(src: nn.Conv2d, cfg: dict) -> TritonConv2d:\n",
    "    if src.groups != 1:\n",
    "        raise ValueError(\"Baseline TritonConv2d currently supports groups=1 only\")\n",
    "    layer = TritonConv2d(\n",
    "        in_channels=src.in_channels,\n",
    "        out_channels=src.out_channels,\n",
    "        kernel_size=src.kernel_size,\n",
    "        stride=src.stride,\n",
    "        padding=src.padding,\n",
    "        dilation=src.dilation,\n",
    "        bias=(src.bias is not None),\n",
    "        # **cfg,\n",
    "    ).to(src.weight.device)\n",
    "    with torch.no_grad():\n",
    "        layer.weight.copy_(src.weight.detach().to(layer.weight.dtype))\n",
    "        if layer.bias is not None and src.bias is not None:\n",
    "            layer.bias.copy_(src.bias.detach().to(layer.bias.dtype))\n",
    "    return layer\n",
    "\n",
    "\n",
    "def replace_convs_with_baseline(module: nn.Module, cfg: dict):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            setattr(module, name, make_triton_conv(child, cfg))\n",
    "        else:\n",
    "            replace_convs_with_baseline(child, cfg)\n",
    "\n",
    "\n",
    "def build_model_pair(config: dict):\n",
    "    reference = torchvision.models.resnet18(num_classes=config[\"num_classes\"])\n",
    "    baseline = copy.deepcopy(reference)\n",
    "    replace_convs_with_baseline(baseline, config[\"baseline_conv\"])\n",
    "    return reference.half(), baseline.half()\n",
    "\n",
    "\n",
    "def apply_sparsity_to_model(model: nn.Module, mode: str, keep_ratio: float, block_size: int = 4):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, TritonConv2d):\n",
    "            layer.clear_sparsity()\n",
    "            if keep_ratio >= 1.0:\n",
    "                continue\n",
    "            if mode == \"channel\":\n",
    "                layer.set_channel_sparsity(keep_ratio)\n",
    "                layer.set_backward_channel_sparsity(keep_ratio)\n",
    "            elif mode == \"block\":\n",
    "                layer.set_block_sparsity(keep_ratio, block_size=block_size)\n",
    "                layer.set_backward_block_sparsity(keep_ratio, block_size=block_size)\n",
    "            elif mode == \"input\":\n",
    "                layer.set_input_channel_sparsity(keep_ratio)\n",
    "                layer.set_backward_input_channel_sparsity(keep_ratio)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown sparsity mode: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968c84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model: nn.Module, label: str, loader: DataLoader, config: dict):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    warmup = config[\"warmup_steps\"]\n",
    "    total_steps = config[\"benchmark_steps\"]\n",
    "    records = []\n",
    "    data_iter = iter(loader)\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        try:\n",
    "            images, targets = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(loader)\n",
    "            images, targets = next(data_iter)\n",
    "\n",
    "        images = images.half().to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        fwd_start = torch.cuda.Event(enable_timing=True)\n",
    "        fwd_end = torch.cuda.Event(enable_timing=True)\n",
    "        bwd_start = torch.cuda.Event(enable_timing=True)\n",
    "        bwd_end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        fwd_start.record()\n",
    "        outputs = model(images)\n",
    "        fwd_end.record()\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        bwd_start.record()\n",
    "        loss.backward()\n",
    "        bwd_end.record()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        fwd_ms = fwd_start.elapsed_time(fwd_end)\n",
    "        bwd_ms = bwd_start.elapsed_time(bwd_end)\n",
    "        step_ms = fwd_ms + bwd_ms\n",
    "        mem_alloc = torch.cuda.max_memory_allocated(device) / 1024 ** 2\n",
    "        mem_reserved = torch.cuda.max_memory_reserved(device) / 1024 ** 2\n",
    "\n",
    "        if step >= warmup:\n",
    "            records.append({\n",
    "                \"label\": label,\n",
    "                \"step\": step,\n",
    "                \"loss\": float(loss.item()),\n",
    "                \"fwd_ms\": fwd_ms,\n",
    "                \"bwd_ms\": bwd_ms,\n",
    "                \"step_ms\": step_ms,\n",
    "                \"throughput_sps\": images.size(0) / (step_ms / 1000.0),\n",
    "                \"max_mem_alloc_mb\": mem_alloc,\n",
    "                \"max_mem_reserved_mb\": mem_reserved,\n",
    "            })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No data recorded for benchmark\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    summary = {\n",
    "        \"label\": label,\n",
    "        \"avg_forward_ms\": df[\"fwd_ms\"].mean(),\n",
    "        \"avg_backward_ms\": df[\"bwd_ms\"].mean(),\n",
    "        \"avg_step_ms\": df[\"step_ms\"].mean(),\n",
    "        \"samples_per_s\": df[\"throughput_sps\"].mean(),\n",
    "        \"max_mem_alloc_mb\": df[\"max_mem_alloc_mb\"].max(),\n",
    "        \"max_mem_reserved_mb\": df[\"max_mem_reserved_mb\"].max(),\n",
    "    }\n",
    "    return df, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037f8b1",
   "metadata": {},
   "source": [
    "Таблица `summary_df` показывает средние метрики по каждому batch size: `avg_forward_ms`, `avg_backward_ms`, `avg_step_ms`, `samples_per_s`, а также пики памяти (`max_mem_alloc_mb`, `max_mem_reserved_mb`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f25b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch size 32 ===\n",
      "=== Batch size 64 ===\n",
      "=== Batch size 96 ===\n",
      "=== Batch size 128 ===\n",
      "=== Batch size 160 ===\n",
      "=== Batch size 192 ===\n",
      "=== Batch size 256 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>avg_forward_ms</th>\n",
       "      <th>avg_backward_ms</th>\n",
       "      <th>avg_step_ms</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>max_mem_alloc_mb</th>\n",
       "      <th>max_mem_reserved_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variant</th>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>nn.Conv2d (bs=32)</td>\n",
       "      <td>3.352386</td>\n",
       "      <td>3.272921</td>\n",
       "      <td>6.625306</td>\n",
       "      <td>4944.976420</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>Baseline TritonConv2d (bs=32)</td>\n",
       "      <td>14.967819</td>\n",
       "      <td>10.237487</td>\n",
       "      <td>25.205306</td>\n",
       "      <td>1315.258640</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>64</th>\n",
       "      <td>nn.Conv2d (bs=64)</td>\n",
       "      <td>3.449143</td>\n",
       "      <td>3.612469</td>\n",
       "      <td>7.061612</td>\n",
       "      <td>9263.492765</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>64</th>\n",
       "      <td>Baseline TritonConv2d (bs=64)</td>\n",
       "      <td>14.682221</td>\n",
       "      <td>12.818234</td>\n",
       "      <td>27.500455</td>\n",
       "      <td>2421.983281</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>96</th>\n",
       "      <td>nn.Conv2d (bs=96)</td>\n",
       "      <td>4.871213</td>\n",
       "      <td>4.244751</td>\n",
       "      <td>9.115963</td>\n",
       "      <td>12463.983149</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>96</th>\n",
       "      <td>Baseline TritonConv2d (bs=96)</td>\n",
       "      <td>14.985442</td>\n",
       "      <td>13.275414</td>\n",
       "      <td>28.260856</td>\n",
       "      <td>3437.792421</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>128</th>\n",
       "      <td>nn.Conv2d (bs=128)</td>\n",
       "      <td>3.484797</td>\n",
       "      <td>4.517356</td>\n",
       "      <td>8.002153</td>\n",
       "      <td>17313.273701</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>128</th>\n",
       "      <td>Baseline TritonConv2d (bs=128)</td>\n",
       "      <td>18.745069</td>\n",
       "      <td>14.676995</td>\n",
       "      <td>33.422063</td>\n",
       "      <td>4092.673452</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>160</th>\n",
       "      <td>nn.Conv2d (bs=160)</td>\n",
       "      <td>3.765251</td>\n",
       "      <td>3.878726</td>\n",
       "      <td>7.643977</td>\n",
       "      <td>21641.398212</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>160</th>\n",
       "      <td>Baseline TritonConv2d (bs=160)</td>\n",
       "      <td>14.196380</td>\n",
       "      <td>18.071107</td>\n",
       "      <td>32.267487</td>\n",
       "      <td>5117.246650</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>192</th>\n",
       "      <td>nn.Conv2d (bs=192)</td>\n",
       "      <td>3.790720</td>\n",
       "      <td>5.501792</td>\n",
       "      <td>9.292512</td>\n",
       "      <td>23614.536015</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>192</th>\n",
       "      <td>Baseline TritonConv2d (bs=192)</td>\n",
       "      <td>14.511505</td>\n",
       "      <td>19.522888</td>\n",
       "      <td>34.034393</td>\n",
       "      <td>5805.301090</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>256</th>\n",
       "      <td>nn.Conv2d (bs=256)</td>\n",
       "      <td>3.992804</td>\n",
       "      <td>4.867109</td>\n",
       "      <td>8.859913</td>\n",
       "      <td>30712.047041</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>256</th>\n",
       "      <td>Baseline TritonConv2d (bs=256)</td>\n",
       "      <td>14.575157</td>\n",
       "      <td>23.893591</td>\n",
       "      <td>38.468748</td>\n",
       "      <td>6802.338198</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>524.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           label  \\\n",
       "variant               batch_size                                   \n",
       "nn.Conv2d             32                       nn.Conv2d (bs=32)   \n",
       "Baseline TritonConv2d 32           Baseline TritonConv2d (bs=32)   \n",
       "nn.Conv2d             64                       nn.Conv2d (bs=64)   \n",
       "Baseline TritonConv2d 64           Baseline TritonConv2d (bs=64)   \n",
       "nn.Conv2d             96                       nn.Conv2d (bs=96)   \n",
       "Baseline TritonConv2d 96           Baseline TritonConv2d (bs=96)   \n",
       "nn.Conv2d             128                     nn.Conv2d (bs=128)   \n",
       "Baseline TritonConv2d 128         Baseline TritonConv2d (bs=128)   \n",
       "nn.Conv2d             160                     nn.Conv2d (bs=160)   \n",
       "Baseline TritonConv2d 160         Baseline TritonConv2d (bs=160)   \n",
       "nn.Conv2d             192                     nn.Conv2d (bs=192)   \n",
       "Baseline TritonConv2d 192         Baseline TritonConv2d (bs=192)   \n",
       "nn.Conv2d             256                     nn.Conv2d (bs=256)   \n",
       "Baseline TritonConv2d 256         Baseline TritonConv2d (bs=256)   \n",
       "\n",
       "                                  avg_forward_ms  avg_backward_ms  \\\n",
       "variant               batch_size                                    \n",
       "nn.Conv2d             32                3.352386         3.272921   \n",
       "Baseline TritonConv2d 32               14.967819        10.237487   \n",
       "nn.Conv2d             64                3.449143         3.612469   \n",
       "Baseline TritonConv2d 64               14.682221        12.818234   \n",
       "nn.Conv2d             96                4.871213         4.244751   \n",
       "Baseline TritonConv2d 96               14.985442        13.275414   \n",
       "nn.Conv2d             128               3.484797         4.517356   \n",
       "Baseline TritonConv2d 128              18.745069        14.676995   \n",
       "nn.Conv2d             160               3.765251         3.878726   \n",
       "Baseline TritonConv2d 160              14.196380        18.071107   \n",
       "nn.Conv2d             192               3.790720         5.501792   \n",
       "Baseline TritonConv2d 192              14.511505        19.522888   \n",
       "nn.Conv2d             256               3.992804         4.867109   \n",
       "Baseline TritonConv2d 256              14.575157        23.893591   \n",
       "\n",
       "                                  avg_step_ms  samples_per_s  \\\n",
       "variant               batch_size                               \n",
       "nn.Conv2d             32             6.625306    4944.976420   \n",
       "Baseline TritonConv2d 32            25.205306    1315.258640   \n",
       "nn.Conv2d             64             7.061612    9263.492765   \n",
       "Baseline TritonConv2d 64            27.500455    2421.983281   \n",
       "nn.Conv2d             96             9.115963   12463.983149   \n",
       "Baseline TritonConv2d 96            28.260856    3437.792421   \n",
       "nn.Conv2d             128            8.002153   17313.273701   \n",
       "Baseline TritonConv2d 128           33.422063    4092.673452   \n",
       "nn.Conv2d             160            7.643977   21641.398212   \n",
       "Baseline TritonConv2d 160           32.267487    5117.246650   \n",
       "nn.Conv2d             192            9.292512   23614.536015   \n",
       "Baseline TritonConv2d 192           34.034393    5805.301090   \n",
       "nn.Conv2d             256            8.859913   30712.047041   \n",
       "Baseline TritonConv2d 256           38.468748    6802.338198   \n",
       "\n",
       "                                  max_mem_alloc_mb  max_mem_reserved_mb  \n",
       "variant               batch_size                                         \n",
       "nn.Conv2d             32                134.588379                144.0  \n",
       "Baseline TritonConv2d 32                184.434082                212.0  \n",
       "nn.Conv2d             64                133.901367                146.0  \n",
       "Baseline TritonConv2d 64                198.357422                234.0  \n",
       "nn.Conv2d             96                135.214844                170.0  \n",
       "Baseline TritonConv2d 96                222.795898                276.0  \n",
       "nn.Conv2d             128               143.020020                162.0  \n",
       "Baseline TritonConv2d 128               245.483887                308.0  \n",
       "nn.Conv2d             160               148.897949                162.0  \n",
       "Baseline TritonConv2d 160               267.797852                314.0  \n",
       "nn.Conv2d             192               155.960938                196.0  \n",
       "Baseline TritonConv2d 192               290.485840                340.0  \n",
       "nn.Conv2d             256               170.462402                200.0  \n",
       "Baseline TritonConv2d 256               330.862305                524.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_summaries = []\n",
    "batch_details = []\n",
    "\n",
    "for bs, loader in train_loaders.items():\n",
    "    print(f\"=== Batch size {bs} ===\")\n",
    "    torch_model, baseline_model = build_model_pair(config)\n",
    "\n",
    "    torch_df, torch_summary = run_benchmark(torch_model, f\"nn.Conv2d (bs={bs})\", loader, config)\n",
    "    torch_summary.update({\"variant\": \"nn.Conv2d\", \"batch_size\": bs})\n",
    "    batch_summaries.append(torch_summary)\n",
    "    batch_details.append(torch_df.assign(variant=\"nn.Conv2d\", batch_size=bs))\n",
    "\n",
    "    baseline_df, baseline_summary = run_benchmark(baseline_model, f\"Baseline TritonConv2d (bs={bs})\", loader, config)\n",
    "    baseline_summary.update({\"variant\": \"Baseline TritonConv2d\", \"batch_size\": bs})\n",
    "    batch_summaries.append(baseline_summary)\n",
    "    batch_details.append(baseline_df.assign(variant=\"Baseline TritonConv2d\", batch_size=bs))\n",
    "\n",
    "summary_df = pd.DataFrame(batch_summaries).set_index([\"variant\", \"batch_size\"])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b90a09",
   "metadata": {},
   "source": [
    "Вывод `detail_df.groupby(...).describe()` содержит count/mean/std/min/25%/50%/75%/max для метрик `step_ms`, `fwd_ms`, `bwd_ms`, `max_mem_alloc_mb` отдельно по каждому `(variant, batch_size)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e564a702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">step_ms</th>\n",
       "      <th colspan=\"2\" halign=\"left\">fwd_ms</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">bwd_ms</th>\n",
       "      <th colspan=\"8\" halign=\"left\">max_mem_alloc_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variant</th>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Baseline TritonConv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>35.0</td>\n",
       "      <td>25.205306</td>\n",
       "      <td>5.661807</td>\n",
       "      <td>20.064992</td>\n",
       "      <td>22.152496</td>\n",
       "      <td>23.545792</td>\n",
       "      <td>25.194736</td>\n",
       "      <td>45.035456</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.967819</td>\n",
       "      <td>...</td>\n",
       "      <td>10.262912</td>\n",
       "      <td>31.393408</td>\n",
       "      <td>35.0</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>184.434082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>35.0</td>\n",
       "      <td>27.500455</td>\n",
       "      <td>6.705584</td>\n",
       "      <td>22.549472</td>\n",
       "      <td>23.527472</td>\n",
       "      <td>24.793759</td>\n",
       "      <td>27.669617</td>\n",
       "      <td>50.465823</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.682221</td>\n",
       "      <td>...</td>\n",
       "      <td>13.020160</td>\n",
       "      <td>29.878271</td>\n",
       "      <td>35.0</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>198.357422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>35.0</td>\n",
       "      <td>28.260856</td>\n",
       "      <td>3.328288</td>\n",
       "      <td>23.014912</td>\n",
       "      <td>26.144688</td>\n",
       "      <td>27.162752</td>\n",
       "      <td>29.711168</td>\n",
       "      <td>38.442881</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.985442</td>\n",
       "      <td>...</td>\n",
       "      <td>13.830144</td>\n",
       "      <td>18.161663</td>\n",
       "      <td>35.0</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>222.795898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>35.0</td>\n",
       "      <td>33.422063</td>\n",
       "      <td>9.799308</td>\n",
       "      <td>23.451936</td>\n",
       "      <td>26.837024</td>\n",
       "      <td>28.814336</td>\n",
       "      <td>38.268896</td>\n",
       "      <td>58.330786</td>\n",
       "      <td>35.0</td>\n",
       "      <td>18.745069</td>\n",
       "      <td>...</td>\n",
       "      <td>14.815232</td>\n",
       "      <td>36.040703</td>\n",
       "      <td>35.0</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>245.483887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>35.0</td>\n",
       "      <td>32.267487</td>\n",
       "      <td>6.840601</td>\n",
       "      <td>26.865376</td>\n",
       "      <td>27.999920</td>\n",
       "      <td>29.643712</td>\n",
       "      <td>33.469808</td>\n",
       "      <td>60.418880</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.196380</td>\n",
       "      <td>...</td>\n",
       "      <td>17.794559</td>\n",
       "      <td>45.735935</td>\n",
       "      <td>35.0</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>267.797852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>35.0</td>\n",
       "      <td>34.034393</td>\n",
       "      <td>6.863784</td>\n",
       "      <td>27.454624</td>\n",
       "      <td>30.557808</td>\n",
       "      <td>31.530016</td>\n",
       "      <td>34.828513</td>\n",
       "      <td>58.785151</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.511505</td>\n",
       "      <td>...</td>\n",
       "      <td>18.081792</td>\n",
       "      <td>46.098431</td>\n",
       "      <td>35.0</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>290.485840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>35.0</td>\n",
       "      <td>38.468748</td>\n",
       "      <td>6.607062</td>\n",
       "      <td>31.874240</td>\n",
       "      <td>34.618976</td>\n",
       "      <td>37.166624</td>\n",
       "      <td>39.910048</td>\n",
       "      <td>62.222399</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.575157</td>\n",
       "      <td>...</td>\n",
       "      <td>24.305040</td>\n",
       "      <td>41.905025</td>\n",
       "      <td>35.0</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>330.862305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">nn.Conv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>35.0</td>\n",
       "      <td>6.625306</td>\n",
       "      <td>1.094632</td>\n",
       "      <td>5.305344</td>\n",
       "      <td>5.846768</td>\n",
       "      <td>6.147808</td>\n",
       "      <td>7.212800</td>\n",
       "      <td>9.580544</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.352386</td>\n",
       "      <td>...</td>\n",
       "      <td>3.766032</td>\n",
       "      <td>6.169600</td>\n",
       "      <td>35.0</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>134.588379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>35.0</td>\n",
       "      <td>7.061612</td>\n",
       "      <td>1.164192</td>\n",
       "      <td>5.438464</td>\n",
       "      <td>6.523296</td>\n",
       "      <td>6.748544</td>\n",
       "      <td>7.613648</td>\n",
       "      <td>11.556416</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.449143</td>\n",
       "      <td>...</td>\n",
       "      <td>4.119040</td>\n",
       "      <td>6.786048</td>\n",
       "      <td>35.0</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>133.901367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>35.0</td>\n",
       "      <td>9.115963</td>\n",
       "      <td>6.323146</td>\n",
       "      <td>5.613088</td>\n",
       "      <td>6.458848</td>\n",
       "      <td>7.282304</td>\n",
       "      <td>9.238448</td>\n",
       "      <td>38.306112</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.871213</td>\n",
       "      <td>...</td>\n",
       "      <td>4.218368</td>\n",
       "      <td>17.643520</td>\n",
       "      <td>35.0</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>135.214844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>35.0</td>\n",
       "      <td>8.002153</td>\n",
       "      <td>2.947658</td>\n",
       "      <td>5.638720</td>\n",
       "      <td>6.299632</td>\n",
       "      <td>7.489984</td>\n",
       "      <td>8.058400</td>\n",
       "      <td>20.697855</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.484797</td>\n",
       "      <td>...</td>\n",
       "      <td>4.770816</td>\n",
       "      <td>17.155071</td>\n",
       "      <td>35.0</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>143.020020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>35.0</td>\n",
       "      <td>7.643977</td>\n",
       "      <td>1.636321</td>\n",
       "      <td>5.682272</td>\n",
       "      <td>6.730464</td>\n",
       "      <td>7.226944</td>\n",
       "      <td>8.003600</td>\n",
       "      <td>14.447168</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.765251</td>\n",
       "      <td>...</td>\n",
       "      <td>4.263904</td>\n",
       "      <td>6.309888</td>\n",
       "      <td>35.0</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>148.897949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>35.0</td>\n",
       "      <td>9.292512</td>\n",
       "      <td>4.875700</td>\n",
       "      <td>5.267040</td>\n",
       "      <td>6.956368</td>\n",
       "      <td>7.695552</td>\n",
       "      <td>8.858112</td>\n",
       "      <td>28.303360</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.790720</td>\n",
       "      <td>...</td>\n",
       "      <td>5.085136</td>\n",
       "      <td>23.287807</td>\n",
       "      <td>35.0</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>155.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>35.0</td>\n",
       "      <td>8.859913</td>\n",
       "      <td>3.380395</td>\n",
       "      <td>6.579584</td>\n",
       "      <td>7.509648</td>\n",
       "      <td>7.993696</td>\n",
       "      <td>8.617504</td>\n",
       "      <td>26.725727</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.992804</td>\n",
       "      <td>...</td>\n",
       "      <td>4.668848</td>\n",
       "      <td>23.002111</td>\n",
       "      <td>35.0</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>170.462402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 step_ms                                  \\\n",
       "                                   count       mean       std        min   \n",
       "variant               batch_size                                           \n",
       "Baseline TritonConv2d 32            35.0  25.205306  5.661807  20.064992   \n",
       "                      64            35.0  27.500455  6.705584  22.549472   \n",
       "                      96            35.0  28.260856  3.328288  23.014912   \n",
       "                      128           35.0  33.422063  9.799308  23.451936   \n",
       "                      160           35.0  32.267487  6.840601  26.865376   \n",
       "                      192           35.0  34.034393  6.863784  27.454624   \n",
       "                      256           35.0  38.468748  6.607062  31.874240   \n",
       "nn.Conv2d             32            35.0   6.625306  1.094632   5.305344   \n",
       "                      64            35.0   7.061612  1.164192   5.438464   \n",
       "                      96            35.0   9.115963  6.323146   5.613088   \n",
       "                      128           35.0   8.002153  2.947658   5.638720   \n",
       "                      160           35.0   7.643977  1.636321   5.682272   \n",
       "                      192           35.0   9.292512  4.875700   5.267040   \n",
       "                      256           35.0   8.859913  3.380395   6.579584   \n",
       "\n",
       "                                                                              \\\n",
       "                                        25%        50%        75%        max   \n",
       "variant               batch_size                                               \n",
       "Baseline TritonConv2d 32          22.152496  23.545792  25.194736  45.035456   \n",
       "                      64          23.527472  24.793759  27.669617  50.465823   \n",
       "                      96          26.144688  27.162752  29.711168  38.442881   \n",
       "                      128         26.837024  28.814336  38.268896  58.330786   \n",
       "                      160         27.999920  29.643712  33.469808  60.418880   \n",
       "                      192         30.557808  31.530016  34.828513  58.785151   \n",
       "                      256         34.618976  37.166624  39.910048  62.222399   \n",
       "nn.Conv2d             32           5.846768   6.147808   7.212800   9.580544   \n",
       "                      64           6.523296   6.748544   7.613648  11.556416   \n",
       "                      96           6.458848   7.282304   9.238448  38.306112   \n",
       "                      128          6.299632   7.489984   8.058400  20.697855   \n",
       "                      160          6.730464   7.226944   8.003600  14.447168   \n",
       "                      192          6.956368   7.695552   8.858112  28.303360   \n",
       "                      256          7.509648   7.993696   8.617504  26.725727   \n",
       "\n",
       "                                 fwd_ms             ...     bwd_ms             \\\n",
       "                                  count       mean  ...        75%        max   \n",
       "variant               batch_size                    ...                         \n",
       "Baseline TritonConv2d 32           35.0  14.967819  ...  10.262912  31.393408   \n",
       "                      64           35.0  14.682221  ...  13.020160  29.878271   \n",
       "                      96           35.0  14.985442  ...  13.830144  18.161663   \n",
       "                      128          35.0  18.745069  ...  14.815232  36.040703   \n",
       "                      160          35.0  14.196380  ...  17.794559  45.735935   \n",
       "                      192          35.0  14.511505  ...  18.081792  46.098431   \n",
       "                      256          35.0  14.575157  ...  24.305040  41.905025   \n",
       "nn.Conv2d             32           35.0   3.352386  ...   3.766032   6.169600   \n",
       "                      64           35.0   3.449143  ...   4.119040   6.786048   \n",
       "                      96           35.0   4.871213  ...   4.218368  17.643520   \n",
       "                      128          35.0   3.484797  ...   4.770816  17.155071   \n",
       "                      160          35.0   3.765251  ...   4.263904   6.309888   \n",
       "                      192          35.0   3.790720  ...   5.085136  23.287807   \n",
       "                      256          35.0   3.992804  ...   4.668848  23.002111   \n",
       "\n",
       "                                 max_mem_alloc_mb                   \\\n",
       "                                            count        mean  std   \n",
       "variant               batch_size                                     \n",
       "Baseline TritonConv2d 32                     35.0  184.434082  0.0   \n",
       "                      64                     35.0  198.357422  0.0   \n",
       "                      96                     35.0  222.795898  0.0   \n",
       "                      128                    35.0  245.483887  0.0   \n",
       "                      160                    35.0  267.797852  0.0   \n",
       "                      192                    35.0  290.485840  0.0   \n",
       "                      256                    35.0  330.862305  0.0   \n",
       "nn.Conv2d             32                     35.0  134.588379  0.0   \n",
       "                      64                     35.0  133.901367  0.0   \n",
       "                      96                     35.0  135.214844  0.0   \n",
       "                      128                    35.0  143.020020  0.0   \n",
       "                      160                    35.0  148.897949  0.0   \n",
       "                      192                    35.0  155.960938  0.0   \n",
       "                      256                    35.0  170.462402  0.0   \n",
       "\n",
       "                                                                      \\\n",
       "                                         min         25%         50%   \n",
       "variant               batch_size                                       \n",
       "Baseline TritonConv2d 32          184.434082  184.434082  184.434082   \n",
       "                      64          198.357422  198.357422  198.357422   \n",
       "                      96          222.795898  222.795898  222.795898   \n",
       "                      128         245.483887  245.483887  245.483887   \n",
       "                      160         267.797852  267.797852  267.797852   \n",
       "                      192         290.485840  290.485840  290.485840   \n",
       "                      256         330.862305  330.862305  330.862305   \n",
       "nn.Conv2d             32          134.588379  134.588379  134.588379   \n",
       "                      64          133.901367  133.901367  133.901367   \n",
       "                      96          135.214844  135.214844  135.214844   \n",
       "                      128         143.020020  143.020020  143.020020   \n",
       "                      160         148.897949  148.897949  148.897949   \n",
       "                      192         155.960938  155.960938  155.960938   \n",
       "                      256         170.462402  170.462402  170.462402   \n",
       "\n",
       "                                                          \n",
       "                                         75%         max  \n",
       "variant               batch_size                          \n",
       "Baseline TritonConv2d 32          184.434082  184.434082  \n",
       "                      64          198.357422  198.357422  \n",
       "                      96          222.795898  222.795898  \n",
       "                      128         245.483887  245.483887  \n",
       "                      160         267.797852  267.797852  \n",
       "                      192         290.485840  290.485840  \n",
       "                      256         330.862305  330.862305  \n",
       "nn.Conv2d             32          134.588379  134.588379  \n",
       "                      64          133.901367  133.901367  \n",
       "                      96          135.214844  135.214844  \n",
       "                      128         143.020020  143.020020  \n",
       "                      160         148.897949  148.897949  \n",
       "                      192         155.960938  155.960938  \n",
       "                      256         170.462402  170.462402  \n",
       "\n",
       "[14 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_df = pd.concat(batch_details, ignore_index=True)\n",
    "metrics = [\"step_ms\", \"fwd_ms\", \"bwd_ms\", \"max_mem_alloc_mb\"]\n",
    "detail_df.groupby([\"variant\", \"batch_size\"])[metrics].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd2b6d",
   "metadata": {},
   "source": [
    "`baseline_vs_torch_df` сравнивает nn.Conv2d и Baseline TritonConv2d: пары столбцов с абсолютными значениями (forward/backward/step время, throughput, память) и коэффициенты ускорения (`speedup_*`, `throughput_ratio`, `mem_*_ratio`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d81011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>torch_forward_ms</th>\n",
       "      <th>baseline_forward_ms</th>\n",
       "      <th>torch_backward_ms</th>\n",
       "      <th>baseline_backward_ms</th>\n",
       "      <th>torch_step_ms</th>\n",
       "      <th>baseline_step_ms</th>\n",
       "      <th>torch_samples_per_s</th>\n",
       "      <th>baseline_samples_per_s</th>\n",
       "      <th>speedup_forward</th>\n",
       "      <th>speedup_backward</th>\n",
       "      <th>speedup_step</th>\n",
       "      <th>throughput_ratio</th>\n",
       "      <th>torch_mem_alloc_mb</th>\n",
       "      <th>baseline_mem_alloc_mb</th>\n",
       "      <th>torch_mem_reserved_mb</th>\n",
       "      <th>baseline_mem_reserved_mb</th>\n",
       "      <th>mem_alloc_ratio</th>\n",
       "      <th>mem_reserved_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.352386</td>\n",
       "      <td>14.967819</td>\n",
       "      <td>3.272921</td>\n",
       "      <td>10.237487</td>\n",
       "      <td>6.625306</td>\n",
       "      <td>25.205306</td>\n",
       "      <td>4944.976420</td>\n",
       "      <td>1315.258640</td>\n",
       "      <td>0.223973</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.262854</td>\n",
       "      <td>0.265979</td>\n",
       "      <td>134.588379</td>\n",
       "      <td>184.434082</td>\n",
       "      <td>144.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1.370357</td>\n",
       "      <td>1.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3.449143</td>\n",
       "      <td>14.682221</td>\n",
       "      <td>3.612469</td>\n",
       "      <td>12.818234</td>\n",
       "      <td>7.061612</td>\n",
       "      <td>27.500455</td>\n",
       "      <td>9263.492765</td>\n",
       "      <td>2421.983281</td>\n",
       "      <td>0.234920</td>\n",
       "      <td>0.281823</td>\n",
       "      <td>0.256782</td>\n",
       "      <td>0.261455</td>\n",
       "      <td>133.901367</td>\n",
       "      <td>198.357422</td>\n",
       "      <td>146.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.481370</td>\n",
       "      <td>1.602740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4.871213</td>\n",
       "      <td>14.985442</td>\n",
       "      <td>4.244751</td>\n",
       "      <td>13.275414</td>\n",
       "      <td>9.115963</td>\n",
       "      <td>28.260856</td>\n",
       "      <td>12463.983149</td>\n",
       "      <td>3437.792421</td>\n",
       "      <td>0.325063</td>\n",
       "      <td>0.319745</td>\n",
       "      <td>0.322565</td>\n",
       "      <td>0.275818</td>\n",
       "      <td>135.214844</td>\n",
       "      <td>222.795898</td>\n",
       "      <td>170.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1.647718</td>\n",
       "      <td>1.623529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3.484797</td>\n",
       "      <td>18.745069</td>\n",
       "      <td>4.517356</td>\n",
       "      <td>14.676995</td>\n",
       "      <td>8.002153</td>\n",
       "      <td>33.422063</td>\n",
       "      <td>17313.273701</td>\n",
       "      <td>4092.673452</td>\n",
       "      <td>0.185905</td>\n",
       "      <td>0.307785</td>\n",
       "      <td>0.239427</td>\n",
       "      <td>0.236389</td>\n",
       "      <td>143.020020</td>\n",
       "      <td>245.483887</td>\n",
       "      <td>162.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>1.716430</td>\n",
       "      <td>1.901235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3.765251</td>\n",
       "      <td>14.196380</td>\n",
       "      <td>3.878726</td>\n",
       "      <td>18.071107</td>\n",
       "      <td>7.643977</td>\n",
       "      <td>32.267487</td>\n",
       "      <td>21641.398212</td>\n",
       "      <td>5117.246650</td>\n",
       "      <td>0.265226</td>\n",
       "      <td>0.214637</td>\n",
       "      <td>0.236894</td>\n",
       "      <td>0.236456</td>\n",
       "      <td>148.897949</td>\n",
       "      <td>267.797852</td>\n",
       "      <td>162.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1.798533</td>\n",
       "      <td>1.938272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>3.790720</td>\n",
       "      <td>14.511505</td>\n",
       "      <td>5.501792</td>\n",
       "      <td>19.522888</td>\n",
       "      <td>9.292512</td>\n",
       "      <td>34.034393</td>\n",
       "      <td>23614.536015</td>\n",
       "      <td>5805.301090</td>\n",
       "      <td>0.261222</td>\n",
       "      <td>0.281812</td>\n",
       "      <td>0.273033</td>\n",
       "      <td>0.245836</td>\n",
       "      <td>155.960938</td>\n",
       "      <td>290.485840</td>\n",
       "      <td>196.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>1.862555</td>\n",
       "      <td>1.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>3.992804</td>\n",
       "      <td>14.575157</td>\n",
       "      <td>4.867109</td>\n",
       "      <td>23.893591</td>\n",
       "      <td>8.859913</td>\n",
       "      <td>38.468748</td>\n",
       "      <td>30712.047041</td>\n",
       "      <td>6802.338198</td>\n",
       "      <td>0.273946</td>\n",
       "      <td>0.203699</td>\n",
       "      <td>0.230315</td>\n",
       "      <td>0.221488</td>\n",
       "      <td>170.462402</td>\n",
       "      <td>330.862305</td>\n",
       "      <td>200.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>1.940969</td>\n",
       "      <td>2.620000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            torch_forward_ms  baseline_forward_ms  torch_backward_ms  \\\n",
       "batch_size                                                             \n",
       "32                  3.352386            14.967819           3.272921   \n",
       "64                  3.449143            14.682221           3.612469   \n",
       "96                  4.871213            14.985442           4.244751   \n",
       "128                 3.484797            18.745069           4.517356   \n",
       "160                 3.765251            14.196380           3.878726   \n",
       "192                 3.790720            14.511505           5.501792   \n",
       "256                 3.992804            14.575157           4.867109   \n",
       "\n",
       "            baseline_backward_ms  torch_step_ms  baseline_step_ms  \\\n",
       "batch_size                                                          \n",
       "32                     10.237487       6.625306         25.205306   \n",
       "64                     12.818234       7.061612         27.500455   \n",
       "96                     13.275414       9.115963         28.260856   \n",
       "128                    14.676995       8.002153         33.422063   \n",
       "160                    18.071107       7.643977         32.267487   \n",
       "192                    19.522888       9.292512         34.034393   \n",
       "256                    23.893591       8.859913         38.468748   \n",
       "\n",
       "            torch_samples_per_s  baseline_samples_per_s  speedup_forward  \\\n",
       "batch_size                                                                 \n",
       "32                  4944.976420             1315.258640         0.223973   \n",
       "64                  9263.492765             2421.983281         0.234920   \n",
       "96                 12463.983149             3437.792421         0.325063   \n",
       "128                17313.273701             4092.673452         0.185905   \n",
       "160                21641.398212             5117.246650         0.265226   \n",
       "192                23614.536015             5805.301090         0.261222   \n",
       "256                30712.047041             6802.338198         0.273946   \n",
       "\n",
       "            speedup_backward  speedup_step  throughput_ratio  \\\n",
       "batch_size                                                     \n",
       "32                  0.319700      0.262854          0.265979   \n",
       "64                  0.281823      0.256782          0.261455   \n",
       "96                  0.319745      0.322565          0.275818   \n",
       "128                 0.307785      0.239427          0.236389   \n",
       "160                 0.214637      0.236894          0.236456   \n",
       "192                 0.281812      0.273033          0.245836   \n",
       "256                 0.203699      0.230315          0.221488   \n",
       "\n",
       "            torch_mem_alloc_mb  baseline_mem_alloc_mb  torch_mem_reserved_mb  \\\n",
       "batch_size                                                                     \n",
       "32                  134.588379             184.434082                  144.0   \n",
       "64                  133.901367             198.357422                  146.0   \n",
       "96                  135.214844             222.795898                  170.0   \n",
       "128                 143.020020             245.483887                  162.0   \n",
       "160                 148.897949             267.797852                  162.0   \n",
       "192                 155.960938             290.485840                  196.0   \n",
       "256                 170.462402             330.862305                  200.0   \n",
       "\n",
       "            baseline_mem_reserved_mb  mem_alloc_ratio  mem_reserved_ratio  \n",
       "batch_size                                                                 \n",
       "32                             212.0         1.370357            1.472222  \n",
       "64                             234.0         1.481370            1.602740  \n",
       "96                             276.0         1.647718            1.623529  \n",
       "128                            308.0         1.716430            1.901235  \n",
       "160                            314.0         1.798533            1.938272  \n",
       "192                            340.0         1.862555            1.734694  \n",
       "256                            524.0         1.940969            2.620000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_compare_rows = []\n",
    "for bs in config[\"batch_sizes\"]:\n",
    "    torch_row = summary_df.loc[(\"nn.Conv2d\", bs)]\n",
    "    baseline_row = summary_df.loc[(\"Baseline TritonConv2d\", bs)]\n",
    "    comparison = {\n",
    "        \"batch_size\": bs,\n",
    "        \"torch_forward_ms\": torch_row[\"avg_forward_ms\"],\n",
    "        \"baseline_forward_ms\": baseline_row[\"avg_forward_ms\"],\n",
    "        \"torch_backward_ms\": torch_row[\"avg_backward_ms\"],\n",
    "        \"baseline_backward_ms\": baseline_row[\"avg_backward_ms\"],\n",
    "        \"torch_step_ms\": torch_row[\"avg_step_ms\"],\n",
    "        \"baseline_step_ms\": baseline_row[\"avg_step_ms\"],\n",
    "        \"torch_samples_per_s\": torch_row[\"samples_per_s\"],\n",
    "        \"baseline_samples_per_s\": baseline_row[\"samples_per_s\"],\n",
    "        \"speedup_forward\": torch_row[\"avg_forward_ms\"] / baseline_row[\"avg_forward_ms\"],\n",
    "        \"speedup_backward\": torch_row[\"avg_backward_ms\"] / baseline_row[\"avg_backward_ms\"],\n",
    "        \"speedup_step\": torch_row[\"avg_step_ms\"] / baseline_row[\"avg_step_ms\"],\n",
    "        \"throughput_ratio\": baseline_row[\"samples_per_s\"] / torch_row[\"samples_per_s\"],\n",
    "        \"torch_mem_alloc_mb\": torch_row[\"max_mem_alloc_mb\"],\n",
    "        \"baseline_mem_alloc_mb\": baseline_row[\"max_mem_alloc_mb\"],\n",
    "        \"torch_mem_reserved_mb\": torch_row[\"max_mem_reserved_mb\"],\n",
    "        \"baseline_mem_reserved_mb\": baseline_row[\"max_mem_reserved_mb\"],\n",
    "        \"mem_alloc_ratio\": baseline_row[\"max_mem_alloc_mb\"] / torch_row[\"max_mem_alloc_mb\"],\n",
    "        \"mem_reserved_ratio\": baseline_row[\"max_mem_reserved_mb\"] / torch_row[\"max_mem_reserved_mb\"],\n",
    "    }\n",
    "    baseline_compare_rows.append(comparison)\n",
    "\n",
    "baseline_vs_torch_df = pd.DataFrame(baseline_compare_rows).set_index(\"batch_size\")\n",
    "baseline_vs_torch_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91f8b1",
   "metadata": {},
   "source": [
    "Таблица `summary_df` показывает средние метрики по каждому batch size: `avg_forward_ms`, `avg_backward_ms`, `avg_step_ms`, `samples_per_s`, а также пики памяти (`max_mem_alloc_mb`, `max_mem_reserved_mb`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fee49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>avg_forward_ms</th>\n",
       "      <th>avg_backward_ms</th>\n",
       "      <th>avg_step_ms</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>max_mem_alloc_mb</th>\n",
       "      <th>max_mem_reserved_mb</th>\n",
       "      <th>variant</th>\n",
       "      <th>mode</th>\n",
       "      <th>keep_ratio</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Input sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>22.774992</td>\n",
       "      <td>15.296472</td>\n",
       "      <td>38.071464</td>\n",
       "      <td>3527.153975</td>\n",
       "      <td>201.454102</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Input sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>22.180967</td>\n",
       "      <td>15.335120</td>\n",
       "      <td>37.516087</td>\n",
       "      <td>3511.374428</td>\n",
       "      <td>233.928223</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Block sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>22.696155</td>\n",
       "      <td>15.941438</td>\n",
       "      <td>38.637594</td>\n",
       "      <td>3432.325143</td>\n",
       "      <td>240.817871</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Channel sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>24.155531</td>\n",
       "      <td>15.187616</td>\n",
       "      <td>39.343147</td>\n",
       "      <td>3373.784959</td>\n",
       "      <td>242.205078</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Input sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>23.975558</td>\n",
       "      <td>15.610528</td>\n",
       "      <td>39.586086</td>\n",
       "      <td>3353.336249</td>\n",
       "      <td>219.550293</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Channel sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>23.896154</td>\n",
       "      <td>16.314183</td>\n",
       "      <td>40.210337</td>\n",
       "      <td>3348.705365</td>\n",
       "      <td>242.558105</td>\n",
       "      <td>528.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Block sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>24.241746</td>\n",
       "      <td>15.560325</td>\n",
       "      <td>39.802071</td>\n",
       "      <td>3339.391017</td>\n",
       "      <td>242.205078</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Channel sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>23.523890</td>\n",
       "      <td>17.282415</td>\n",
       "      <td>40.806305</td>\n",
       "      <td>3251.883801</td>\n",
       "      <td>240.817871</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Block sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>24.818793</td>\n",
       "      <td>16.722576</td>\n",
       "      <td>41.541369</td>\n",
       "      <td>3222.426748</td>\n",
       "      <td>242.558105</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Channel sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>24.400073</td>\n",
       "      <td>17.726504</td>\n",
       "      <td>42.126578</td>\n",
       "      <td>3170.031484</td>\n",
       "      <td>243.114746</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Block sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>26.088966</td>\n",
       "      <td>17.167575</td>\n",
       "      <td>43.256541</td>\n",
       "      <td>3087.547104</td>\n",
       "      <td>243.127930</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Input sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>25.000742</td>\n",
       "      <td>21.745026</td>\n",
       "      <td>46.745768</td>\n",
       "      <td>2878.083112</td>\n",
       "      <td>222.798340</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   label  avg_forward_ms  avg_backward_ms  \\\n",
       "0     Input sparsity (keep=0.25, bs=128)       22.774992        15.296472   \n",
       "1     Input sparsity (keep=0.75, bs=128)       22.180967        15.335120   \n",
       "2     Block sparsity (keep=0.50, bs=128)       22.696155        15.941438   \n",
       "3   Channel sparsity (keep=0.25, bs=128)       24.155531        15.187616   \n",
       "4     Input sparsity (keep=0.50, bs=128)       23.975558        15.610528   \n",
       "5   Channel sparsity (keep=0.75, bs=128)       23.896154        16.314183   \n",
       "6     Block sparsity (keep=0.25, bs=128)       24.241746        15.560325   \n",
       "7   Channel sparsity (keep=0.50, bs=128)       23.523890        17.282415   \n",
       "8     Block sparsity (keep=0.75, bs=128)       24.818793        16.722576   \n",
       "9   Channel sparsity (keep=0.60, bs=128)       24.400073        17.726504   \n",
       "10    Block sparsity (keep=0.60, bs=128)       26.088966        17.167575   \n",
       "11    Input sparsity (keep=0.60, bs=128)       25.000742        21.745026   \n",
       "\n",
       "    avg_step_ms  samples_per_s  max_mem_alloc_mb  max_mem_reserved_mb  \\\n",
       "0     38.071464    3527.153975        201.454102                530.0   \n",
       "1     37.516087    3511.374428        233.928223                530.0   \n",
       "2     38.637594    3432.325143        240.817871                530.0   \n",
       "3     39.343147    3373.784959        242.205078                530.0   \n",
       "4     39.586086    3353.336249        219.550293                530.0   \n",
       "5     40.210337    3348.705365        242.558105                528.0   \n",
       "6     39.802071    3339.391017        242.205078                530.0   \n",
       "7     40.806305    3251.883801        240.817871                530.0   \n",
       "8     41.541369    3222.426748        242.558105                530.0   \n",
       "9     42.126578    3170.031484        243.114746                530.0   \n",
       "10    43.256541    3087.547104        243.127930                530.0   \n",
       "11    46.745768    2878.083112        222.798340                530.0   \n",
       "\n",
       "              variant     mode  keep_ratio  batch_size  \n",
       "0     Sparsity::input    input        0.25         128  \n",
       "1     Sparsity::input    input        0.75         128  \n",
       "2     Sparsity::block    block        0.50         128  \n",
       "3   Sparsity::channel  channel        0.25         128  \n",
       "4     Sparsity::input    input        0.50         128  \n",
       "5   Sparsity::channel  channel        0.75         128  \n",
       "6     Sparsity::block    block        0.25         128  \n",
       "7   Sparsity::channel  channel        0.50         128  \n",
       "8     Sparsity::block    block        0.75         128  \n",
       "9   Sparsity::channel  channel        0.60         128  \n",
       "10    Sparsity::block    block        0.60         128  \n",
       "11    Sparsity::input    input        0.60         128  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_cfg = config[\"sparsity_bench\"]\n",
    "sparsity_bs = sparsity_cfg[\"batch_size\"]\n",
    "if sparsity_bs not in train_loaders:\n",
    "    train_loaders[sparsity_bs] = make_loader(sparsity_bs)\n",
    "sparsity_loader = train_loaders[sparsity_bs]\n",
    "\n",
    "sparsity_summaries = []\n",
    "sparsity_details = []\n",
    "\n",
    "for mode in sparsity_cfg[\"modes\"]:\n",
    "    for ratio in sparsity_cfg[\"keep_ratios\"]:\n",
    "        _, baseline_model = build_model_pair(config)\n",
    "        apply_sparsity_to_model(\n",
    "            baseline_model,\n",
    "            mode,\n",
    "            keep_ratio=ratio,\n",
    "            block_size=sparsity_cfg.get(\"block_size\", 4),\n",
    "        )\n",
    "        label = f\"{mode.capitalize()} sparsity (keep={ratio:.2f}, bs={sparsity_bs})\"\n",
    "        bench_df, bench_summary = run_benchmark(baseline_model, label, sparsity_loader, config)\n",
    "        bench_summary.update({\n",
    "            \"variant\": f\"Sparsity::{mode}\",\n",
    "            \"mode\": mode,\n",
    "            \"keep_ratio\": ratio,\n",
    "            \"batch_size\": sparsity_bs,\n",
    "        })\n",
    "        sparsity_summaries.append(bench_summary)\n",
    "        sparsity_details.append(\n",
    "            bench_df.assign(variant=f\"Sparsity::{mode}\", mode=mode, keep_ratio=ratio, batch_size=sparsity_bs)\n",
    "        )\n",
    "\n",
    "sparsity_summary_df = pd.DataFrame(sparsity_summaries).sort_values(\"samples_per_s\", ascending=False).reset_index(drop=True)\n",
    "sparsity_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d37f9",
   "metadata": {},
   "source": [
    "`sparsity_compare_df` добавляет к тем же сценариям относительные значения относительно эталонного nn.Conv2d (`speedup_*_vs_torch`, `throughput_ratio_vs_torch`, `mem_*_ratio_vs_torch`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e7be95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>avg_forward_ms</th>\n",
       "      <th>avg_backward_ms</th>\n",
       "      <th>avg_step_ms</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>max_mem_alloc_mb</th>\n",
       "      <th>max_mem_reserved_mb</th>\n",
       "      <th>variant</th>\n",
       "      <th>mode</th>\n",
       "      <th>keep_ratio</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>speedup_forward_vs_torch</th>\n",
       "      <th>speedup_backward_vs_torch</th>\n",
       "      <th>speedup_step_vs_torch</th>\n",
       "      <th>throughput_ratio_vs_torch</th>\n",
       "      <th>mem_alloc_ratio_vs_torch</th>\n",
       "      <th>mem_reserved_ratio_vs_torch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Input sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>22.774992</td>\n",
       "      <td>15.296472</td>\n",
       "      <td>38.071464</td>\n",
       "      <td>3527.153975</td>\n",
       "      <td>201.454102</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.153010</td>\n",
       "      <td>0.295320</td>\n",
       "      <td>0.210188</td>\n",
       "      <td>0.203725</td>\n",
       "      <td>1.408573</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Input sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>22.180967</td>\n",
       "      <td>15.335120</td>\n",
       "      <td>37.516087</td>\n",
       "      <td>3511.374428</td>\n",
       "      <td>233.928223</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0.157108</td>\n",
       "      <td>0.294576</td>\n",
       "      <td>0.213299</td>\n",
       "      <td>0.202814</td>\n",
       "      <td>1.635633</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Block sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>22.696155</td>\n",
       "      <td>15.941438</td>\n",
       "      <td>38.637594</td>\n",
       "      <td>3432.325143</td>\n",
       "      <td>240.817871</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.153541</td>\n",
       "      <td>0.283372</td>\n",
       "      <td>0.207108</td>\n",
       "      <td>0.198248</td>\n",
       "      <td>1.683805</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Channel sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>24.155531</td>\n",
       "      <td>15.187616</td>\n",
       "      <td>39.343147</td>\n",
       "      <td>3373.784959</td>\n",
       "      <td>242.205078</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.144265</td>\n",
       "      <td>0.297437</td>\n",
       "      <td>0.203394</td>\n",
       "      <td>0.194867</td>\n",
       "      <td>1.693505</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Input sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>23.975558</td>\n",
       "      <td>15.610528</td>\n",
       "      <td>39.586086</td>\n",
       "      <td>3353.336249</td>\n",
       "      <td>219.550293</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>0.289379</td>\n",
       "      <td>0.202146</td>\n",
       "      <td>0.193686</td>\n",
       "      <td>1.535102</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Channel sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>23.896154</td>\n",
       "      <td>16.314183</td>\n",
       "      <td>40.210337</td>\n",
       "      <td>3348.705365</td>\n",
       "      <td>242.558105</td>\n",
       "      <td>528.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0.145831</td>\n",
       "      <td>0.276897</td>\n",
       "      <td>0.199007</td>\n",
       "      <td>0.193418</td>\n",
       "      <td>1.695973</td>\n",
       "      <td>3.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Block sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>24.241746</td>\n",
       "      <td>15.560325</td>\n",
       "      <td>39.802071</td>\n",
       "      <td>3339.391017</td>\n",
       "      <td>242.205078</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.143752</td>\n",
       "      <td>0.290312</td>\n",
       "      <td>0.201049</td>\n",
       "      <td>0.192880</td>\n",
       "      <td>1.693505</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Channel sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>23.523890</td>\n",
       "      <td>17.282415</td>\n",
       "      <td>40.806305</td>\n",
       "      <td>3251.883801</td>\n",
       "      <td>240.817871</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.148139</td>\n",
       "      <td>0.261385</td>\n",
       "      <td>0.196101</td>\n",
       "      <td>0.187826</td>\n",
       "      <td>1.683805</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Block sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>24.818793</td>\n",
       "      <td>16.722576</td>\n",
       "      <td>41.541369</td>\n",
       "      <td>3222.426748</td>\n",
       "      <td>242.558105</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0.140410</td>\n",
       "      <td>0.270135</td>\n",
       "      <td>0.192631</td>\n",
       "      <td>0.186125</td>\n",
       "      <td>1.695973</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Channel sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>24.400073</td>\n",
       "      <td>17.726504</td>\n",
       "      <td>42.126578</td>\n",
       "      <td>3170.031484</td>\n",
       "      <td>243.114746</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.142819</td>\n",
       "      <td>0.254836</td>\n",
       "      <td>0.189955</td>\n",
       "      <td>0.183098</td>\n",
       "      <td>1.699865</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Block sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>26.088966</td>\n",
       "      <td>17.167575</td>\n",
       "      <td>43.256541</td>\n",
       "      <td>3087.547104</td>\n",
       "      <td>243.127930</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.133574</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>0.178334</td>\n",
       "      <td>1.699957</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Input sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>25.000742</td>\n",
       "      <td>21.745026</td>\n",
       "      <td>46.745768</td>\n",
       "      <td>2878.083112</td>\n",
       "      <td>222.798340</td>\n",
       "      <td>530.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.139388</td>\n",
       "      <td>0.207742</td>\n",
       "      <td>0.171185</td>\n",
       "      <td>0.166236</td>\n",
       "      <td>1.557812</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   label  avg_forward_ms  avg_backward_ms  \\\n",
       "0     Input sparsity (keep=0.25, bs=128)       22.774992        15.296472   \n",
       "1     Input sparsity (keep=0.75, bs=128)       22.180967        15.335120   \n",
       "2     Block sparsity (keep=0.50, bs=128)       22.696155        15.941438   \n",
       "3   Channel sparsity (keep=0.25, bs=128)       24.155531        15.187616   \n",
       "4     Input sparsity (keep=0.50, bs=128)       23.975558        15.610528   \n",
       "5   Channel sparsity (keep=0.75, bs=128)       23.896154        16.314183   \n",
       "6     Block sparsity (keep=0.25, bs=128)       24.241746        15.560325   \n",
       "7   Channel sparsity (keep=0.50, bs=128)       23.523890        17.282415   \n",
       "8     Block sparsity (keep=0.75, bs=128)       24.818793        16.722576   \n",
       "9   Channel sparsity (keep=0.60, bs=128)       24.400073        17.726504   \n",
       "10    Block sparsity (keep=0.60, bs=128)       26.088966        17.167575   \n",
       "11    Input sparsity (keep=0.60, bs=128)       25.000742        21.745026   \n",
       "\n",
       "    avg_step_ms  samples_per_s  max_mem_alloc_mb  max_mem_reserved_mb  \\\n",
       "0     38.071464    3527.153975        201.454102                530.0   \n",
       "1     37.516087    3511.374428        233.928223                530.0   \n",
       "2     38.637594    3432.325143        240.817871                530.0   \n",
       "3     39.343147    3373.784959        242.205078                530.0   \n",
       "4     39.586086    3353.336249        219.550293                530.0   \n",
       "5     40.210337    3348.705365        242.558105                528.0   \n",
       "6     39.802071    3339.391017        242.205078                530.0   \n",
       "7     40.806305    3251.883801        240.817871                530.0   \n",
       "8     41.541369    3222.426748        242.558105                530.0   \n",
       "9     42.126578    3170.031484        243.114746                530.0   \n",
       "10    43.256541    3087.547104        243.127930                530.0   \n",
       "11    46.745768    2878.083112        222.798340                530.0   \n",
       "\n",
       "              variant     mode  keep_ratio  batch_size  \\\n",
       "0     Sparsity::input    input        0.25         128   \n",
       "1     Sparsity::input    input        0.75         128   \n",
       "2     Sparsity::block    block        0.50         128   \n",
       "3   Sparsity::channel  channel        0.25         128   \n",
       "4     Sparsity::input    input        0.50         128   \n",
       "5   Sparsity::channel  channel        0.75         128   \n",
       "6     Sparsity::block    block        0.25         128   \n",
       "7   Sparsity::channel  channel        0.50         128   \n",
       "8     Sparsity::block    block        0.75         128   \n",
       "9   Sparsity::channel  channel        0.60         128   \n",
       "10    Sparsity::block    block        0.60         128   \n",
       "11    Sparsity::input    input        0.60         128   \n",
       "\n",
       "    speedup_forward_vs_torch  speedup_backward_vs_torch  \\\n",
       "0                   0.153010                   0.295320   \n",
       "1                   0.157108                   0.294576   \n",
       "2                   0.153541                   0.283372   \n",
       "3                   0.144265                   0.297437   \n",
       "4                   0.145348                   0.289379   \n",
       "5                   0.145831                   0.276897   \n",
       "6                   0.143752                   0.290312   \n",
       "7                   0.148139                   0.261385   \n",
       "8                   0.140410                   0.270135   \n",
       "9                   0.142819                   0.254836   \n",
       "10                  0.133574                   0.263133   \n",
       "11                  0.139388                   0.207742   \n",
       "\n",
       "    speedup_step_vs_torch  throughput_ratio_vs_torch  \\\n",
       "0                0.210188                   0.203725   \n",
       "1                0.213299                   0.202814   \n",
       "2                0.207108                   0.198248   \n",
       "3                0.203394                   0.194867   \n",
       "4                0.202146                   0.193686   \n",
       "5                0.199007                   0.193418   \n",
       "6                0.201049                   0.192880   \n",
       "7                0.196101                   0.187826   \n",
       "8                0.192631                   0.186125   \n",
       "9                0.189955                   0.183098   \n",
       "10               0.184993                   0.178334   \n",
       "11               0.171185                   0.166236   \n",
       "\n",
       "    mem_alloc_ratio_vs_torch  mem_reserved_ratio_vs_torch  \n",
       "0                   1.408573                     3.271605  \n",
       "1                   1.635633                     3.271605  \n",
       "2                   1.683805                     3.271605  \n",
       "3                   1.693505                     3.271605  \n",
       "4                   1.535102                     3.271605  \n",
       "5                   1.695973                     3.259259  \n",
       "6                   1.693505                     3.271605  \n",
       "7                   1.683805                     3.271605  \n",
       "8                   1.695973                     3.271605  \n",
       "9                   1.699865                     3.271605  \n",
       "10                  1.699957                     3.271605  \n",
       "11                  1.557812                     3.271605  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_reference = summary_df.loc[(\"nn.Conv2d\", sparsity_bs)]\n",
    "\n",
    "sparsity_compare_df = sparsity_summary_df.copy()\n",
    "sparsity_compare_df[\"speedup_forward_vs_torch\"] = sparsity_reference[\"avg_forward_ms\"] / sparsity_compare_df[\"avg_forward_ms\"]\n",
    "sparsity_compare_df[\"speedup_backward_vs_torch\"] = sparsity_reference[\"avg_backward_ms\"] / sparsity_compare_df[\"avg_backward_ms\"]\n",
    "sparsity_compare_df[\"speedup_step_vs_torch\"] = sparsity_reference[\"avg_step_ms\"] / sparsity_compare_df[\"avg_step_ms\"]\n",
    "sparsity_compare_df[\"throughput_ratio_vs_torch\"] = sparsity_compare_df[\"samples_per_s\"] / sparsity_reference[\"samples_per_s\"]\n",
    "sparsity_compare_df[\"mem_alloc_ratio_vs_torch\"] = sparsity_compare_df[\"max_mem_alloc_mb\"] / sparsity_reference[\"max_mem_alloc_mb\"]\n",
    "sparsity_compare_df[\"mem_reserved_ratio_vs_torch\"] = sparsity_compare_df[\"max_mem_reserved_mb\"] / sparsity_reference[\"max_mem_reserved_mb\"]\n",
    "sparsity_compare_df = sparsity_compare_df.sort_values(\"samples_per_s\", ascending=False).reset_index(drop=True)\n",
    "sparsity_compare_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c83ece",
   "metadata": {},
   "source": [
    "`ranking_df` — упорядоченный рейтинг сценариев спарсификации: показывает `mode`, `keep_ratio`, абсолютный throughput и его отношение к торчу, а также ускорения forward/backward/step и изменение памяти.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba124c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>mode</th>\n",
       "      <th>keep_ratio</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>throughput_ratio_vs_torch</th>\n",
       "      <th>speedup_forward_vs_torch</th>\n",
       "      <th>speedup_backward_vs_torch</th>\n",
       "      <th>speedup_step_vs_torch</th>\n",
       "      <th>mem_alloc_ratio_vs_torch</th>\n",
       "      <th>mem_reserved_ratio_vs_torch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3527.153975</td>\n",
       "      <td>0.203725</td>\n",
       "      <td>0.153010</td>\n",
       "      <td>0.295320</td>\n",
       "      <td>0.210188</td>\n",
       "      <td>1.408573</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3511.374428</td>\n",
       "      <td>0.202814</td>\n",
       "      <td>0.157108</td>\n",
       "      <td>0.294576</td>\n",
       "      <td>0.213299</td>\n",
       "      <td>1.635633</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3432.325143</td>\n",
       "      <td>0.198248</td>\n",
       "      <td>0.153541</td>\n",
       "      <td>0.283372</td>\n",
       "      <td>0.207108</td>\n",
       "      <td>1.683805</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3373.784959</td>\n",
       "      <td>0.194867</td>\n",
       "      <td>0.144265</td>\n",
       "      <td>0.297437</td>\n",
       "      <td>0.203394</td>\n",
       "      <td>1.693505</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3353.336249</td>\n",
       "      <td>0.193686</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>0.289379</td>\n",
       "      <td>0.202146</td>\n",
       "      <td>1.535102</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3348.705365</td>\n",
       "      <td>0.193418</td>\n",
       "      <td>0.145831</td>\n",
       "      <td>0.276897</td>\n",
       "      <td>0.199007</td>\n",
       "      <td>1.695973</td>\n",
       "      <td>3.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3339.391017</td>\n",
       "      <td>0.192880</td>\n",
       "      <td>0.143752</td>\n",
       "      <td>0.290312</td>\n",
       "      <td>0.201049</td>\n",
       "      <td>1.693505</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3251.883801</td>\n",
       "      <td>0.187826</td>\n",
       "      <td>0.148139</td>\n",
       "      <td>0.261385</td>\n",
       "      <td>0.196101</td>\n",
       "      <td>1.683805</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3222.426748</td>\n",
       "      <td>0.186125</td>\n",
       "      <td>0.140410</td>\n",
       "      <td>0.270135</td>\n",
       "      <td>0.192631</td>\n",
       "      <td>1.695973</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.60</td>\n",
       "      <td>3170.031484</td>\n",
       "      <td>0.183098</td>\n",
       "      <td>0.142819</td>\n",
       "      <td>0.254836</td>\n",
       "      <td>0.189955</td>\n",
       "      <td>1.699865</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.60</td>\n",
       "      <td>3087.547104</td>\n",
       "      <td>0.178334</td>\n",
       "      <td>0.133574</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>1.699957</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2878.083112</td>\n",
       "      <td>0.166236</td>\n",
       "      <td>0.139388</td>\n",
       "      <td>0.207742</td>\n",
       "      <td>0.171185</td>\n",
       "      <td>1.557812</td>\n",
       "      <td>3.271605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              variant     mode  keep_ratio  samples_per_s  \\\n",
       "0     Sparsity::input    input        0.25    3527.153975   \n",
       "1     Sparsity::input    input        0.75    3511.374428   \n",
       "2     Sparsity::block    block        0.50    3432.325143   \n",
       "3   Sparsity::channel  channel        0.25    3373.784959   \n",
       "4     Sparsity::input    input        0.50    3353.336249   \n",
       "5   Sparsity::channel  channel        0.75    3348.705365   \n",
       "6     Sparsity::block    block        0.25    3339.391017   \n",
       "7   Sparsity::channel  channel        0.50    3251.883801   \n",
       "8     Sparsity::block    block        0.75    3222.426748   \n",
       "9   Sparsity::channel  channel        0.60    3170.031484   \n",
       "10    Sparsity::block    block        0.60    3087.547104   \n",
       "11    Sparsity::input    input        0.60    2878.083112   \n",
       "\n",
       "    throughput_ratio_vs_torch  speedup_forward_vs_torch  \\\n",
       "0                    0.203725                  0.153010   \n",
       "1                    0.202814                  0.157108   \n",
       "2                    0.198248                  0.153541   \n",
       "3                    0.194867                  0.144265   \n",
       "4                    0.193686                  0.145348   \n",
       "5                    0.193418                  0.145831   \n",
       "6                    0.192880                  0.143752   \n",
       "7                    0.187826                  0.148139   \n",
       "8                    0.186125                  0.140410   \n",
       "9                    0.183098                  0.142819   \n",
       "10                   0.178334                  0.133574   \n",
       "11                   0.166236                  0.139388   \n",
       "\n",
       "    speedup_backward_vs_torch  speedup_step_vs_torch  \\\n",
       "0                    0.295320               0.210188   \n",
       "1                    0.294576               0.213299   \n",
       "2                    0.283372               0.207108   \n",
       "3                    0.297437               0.203394   \n",
       "4                    0.289379               0.202146   \n",
       "5                    0.276897               0.199007   \n",
       "6                    0.290312               0.201049   \n",
       "7                    0.261385               0.196101   \n",
       "8                    0.270135               0.192631   \n",
       "9                    0.254836               0.189955   \n",
       "10                   0.263133               0.184993   \n",
       "11                   0.207742               0.171185   \n",
       "\n",
       "    mem_alloc_ratio_vs_torch  mem_reserved_ratio_vs_torch  \n",
       "0                   1.408573                     3.271605  \n",
       "1                   1.635633                     3.271605  \n",
       "2                   1.683805                     3.271605  \n",
       "3                   1.693505                     3.271605  \n",
       "4                   1.535102                     3.271605  \n",
       "5                   1.695973                     3.259259  \n",
       "6                   1.693505                     3.271605  \n",
       "7                   1.683805                     3.271605  \n",
       "8                   1.695973                     3.271605  \n",
       "9                   1.699865                     3.271605  \n",
       "10                  1.699957                     3.271605  \n",
       "11                  1.557812                     3.271605  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_df = sparsity_compare_df[[\n",
    "    \"variant\",\n",
    "    \"mode\",\n",
    "    \"keep_ratio\",\n",
    "    \"samples_per_s\",\n",
    "    \"throughput_ratio_vs_torch\",\n",
    "    \"speedup_forward_vs_torch\",\n",
    "    \"speedup_backward_vs_torch\",\n",
    "    \"speedup_step_vs_torch\",\n",
    "    \"mem_alloc_ratio_vs_torch\",\n",
    "    \"mem_reserved_ratio_vs_torch\",\n",
    "]].copy()\n",
    "ranking_df = ranking_df.sort_values(\"throughput_ratio_vs_torch\", ascending=False).reset_index(drop=True)\n",
    "ranking_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f63e3a-c709-4b0b-99f3-d43879429496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
