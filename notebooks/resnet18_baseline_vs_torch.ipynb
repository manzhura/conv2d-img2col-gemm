{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dffa7c",
   "metadata": {},
   "source": [
    "# ResNet18 Baseline Conv2d Benchmark\n",
    "\n",
    "Сравнение nn.Conv2d и кастомной img2col→GEMM свёртки (Baseline TritonConv2d) на ResNet18 с разными batch size и сценариями спарсификации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319e97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4154eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from conv_gemm.baseline_layers.triton_conv2d import TritonConv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da3ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data_root\": \"/mnt/d/VSCode-Projects/conv2d-img2col-gemm/data\",\n",
      "  \"num_classes\": 10,\n",
      "  \"batch_sizes\": [\n",
      "    32,\n",
      "    64,\n",
      "    96,\n",
      "    128,\n",
      "    160,\n",
      "    192,\n",
      "    256\n",
      "  ],\n",
      "  \"num_workers\": 4,\n",
      "  \"train_subset\": 8192,\n",
      "  \"lr\": 0.001,\n",
      "  \"momentum\": 0.9,\n",
      "  \"weight_decay\": 0.0005,\n",
      "  \"warmup_steps\": 5,\n",
      "  \"benchmark_steps\": 40,\n",
      "  \"baseline_conv\": {\n",
      "    \"BLOCK_M\": 64,\n",
      "    \"BLOCK_N\": 64,\n",
      "    \"BLOCK_K\": 64,\n",
      "    \"NUM_WARPS\": 4,\n",
      "    \"NUM_STAGES\": 2\n",
      "  },\n",
      "  \"sparsity_bench\": {\n",
      "    \"modes\": [\n",
      "      \"channel\",\n",
      "      \"block\",\n",
      "      \"input\"\n",
      "    ],\n",
      "    \"keep_ratios\": [\n",
      "      0.75,\n",
      "      0.6,\n",
      "      0.5,\n",
      "      0.25\n",
      "    ],\n",
      "    \"block_size\": 4,\n",
      "    \"batch_size\": 128\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    raise RuntimeError(\"CUDA GPU is required for this benchmark\")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "data_root = Path(\"../data\").resolve()\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"data_root\": str(data_root),\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_sizes\": [32, 64, 96, 128, 160, 192, 256],\n",
    "    \"num_workers\": 4,\n",
    "    \"train_subset\": 8192,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"benchmark_steps\": 40,\n",
    "    \"baseline_conv\": {\n",
    "        \"BLOCK_M\": 64,\n",
    "        \"BLOCK_N\": 64,\n",
    "        \"BLOCK_K\": 64,\n",
    "        \"NUM_WARPS\": 4,\n",
    "        \"NUM_STAGES\": 2,\n",
    "    },\n",
    "    \"sparsity_bench\": {\n",
    "        \"modes\": [\"channel\", \"block\", \"input\"],\n",
    "        \"keep_ratios\": [0.75, 0.6, 0.5, 0.25],\n",
    "        \"block_size\": 4,\n",
    "        \"batch_size\": 128,\n",
    "    },\n",
    "}\n",
    "print(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e44fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32: 256, 64: 128, 96: 85, 128: 64, 160: 51, 192: 42, 256: 32}\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "full_train = torchvision.datasets.CIFAR10(\n",
    "    root=config[\"data_root\"], train=True, download=True, transform=transform_train\n",
    ")\n",
    "if config[\"train_subset\"] is not None and config[\"train_subset\"] < len(full_train):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    subset_idx = torch.randperm(len(full_train), generator=g)[: config[\"train_subset\"]]\n",
    "    train_dataset = torch.utils.data.Subset(full_train, subset_idx)\n",
    "else:\n",
    "    train_dataset = full_train\n",
    "\n",
    "\n",
    "def make_loader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "train_loaders: Dict[int, DataLoader] = {}\n",
    "for bs in config[\"batch_sizes\"]:\n",
    "    train_loaders[bs] = make_loader(bs)\n",
    "\n",
    "print({bs: len(loader) for bs, loader in train_loaders.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbd874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triton_conv(src: nn.Conv2d, cfg: dict) -> TritonConv2d:\n",
    "    if src.groups != 1:\n",
    "        raise ValueError(\"Baseline TritonConv2d currently supports groups=1 only\")\n",
    "    layer = TritonConv2d(\n",
    "        in_channels=src.in_channels,\n",
    "        out_channels=src.out_channels,\n",
    "        kernel_size=src.kernel_size,\n",
    "        stride=src.stride,\n",
    "        padding=src.padding,\n",
    "        dilation=src.dilation,\n",
    "        bias=(src.bias is not None),\n",
    "        **cfg,\n",
    "    ).to(src.weight.device)\n",
    "    with torch.no_grad():\n",
    "        layer.weight.copy_(src.weight.detach().to(layer.weight.dtype))\n",
    "        if layer.bias is not None and src.bias is not None:\n",
    "            layer.bias.copy_(src.bias.detach().to(layer.bias.dtype))\n",
    "    return layer\n",
    "\n",
    "\n",
    "def replace_convs_with_baseline(module: nn.Module, cfg: dict):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            setattr(module, name, make_triton_conv(child, cfg))\n",
    "        else:\n",
    "            replace_convs_with_baseline(child, cfg)\n",
    "\n",
    "\n",
    "def build_model_pair(config: dict):\n",
    "    reference = torchvision.models.resnet18(num_classes=config[\"num_classes\"])\n",
    "    baseline = copy.deepcopy(reference)\n",
    "    replace_convs_with_baseline(baseline, config[\"baseline_conv\"])\n",
    "    return reference, baseline\n",
    "\n",
    "\n",
    "def apply_sparsity_to_model(model: nn.Module, mode: str, keep_ratio: float, block_size: int = 4):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, TritonConv2d):\n",
    "            layer.clear_sparsity()\n",
    "            if keep_ratio >= 1.0:\n",
    "                continue\n",
    "            if mode == \"channel\":\n",
    "                layer.set_channel_sparsity(keep_ratio)\n",
    "                layer.set_backward_channel_sparsity(keep_ratio)\n",
    "            elif mode == \"block\":\n",
    "                layer.set_block_sparsity(keep_ratio, block_size=block_size)\n",
    "                layer.set_backward_block_sparsity(keep_ratio, block_size=block_size)\n",
    "            elif mode == \"input\":\n",
    "                layer.set_input_channel_sparsity(keep_ratio)\n",
    "                layer.set_backward_input_channel_sparsity(keep_ratio)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown sparsity mode: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968c84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model: nn.Module, label: str, loader: DataLoader, config: dict):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    warmup = config[\"warmup_steps\"]\n",
    "    total_steps = config[\"benchmark_steps\"]\n",
    "    records = []\n",
    "    data_iter = iter(loader)\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        try:\n",
    "            images, targets = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(loader)\n",
    "            images, targets = next(data_iter)\n",
    "\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        fwd_start = torch.cuda.Event(enable_timing=True)\n",
    "        fwd_end = torch.cuda.Event(enable_timing=True)\n",
    "        bwd_start = torch.cuda.Event(enable_timing=True)\n",
    "        bwd_end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        fwd_start.record()\n",
    "        outputs = model(images)\n",
    "        fwd_end.record()\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        bwd_start.record()\n",
    "        loss.backward()\n",
    "        bwd_end.record()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        fwd_ms = fwd_start.elapsed_time(fwd_end)\n",
    "        bwd_ms = bwd_start.elapsed_time(bwd_end)\n",
    "        step_ms = fwd_ms + bwd_ms\n",
    "        mem_alloc = torch.cuda.max_memory_allocated(device) / 1024 ** 2\n",
    "        mem_reserved = torch.cuda.max_memory_reserved(device) / 1024 ** 2\n",
    "\n",
    "        if step >= warmup:\n",
    "            records.append({\n",
    "                \"label\": label,\n",
    "                \"step\": step,\n",
    "                \"loss\": float(loss.item()),\n",
    "                \"fwd_ms\": fwd_ms,\n",
    "                \"bwd_ms\": bwd_ms,\n",
    "                \"step_ms\": step_ms,\n",
    "                \"throughput_sps\": images.size(0) / (step_ms / 1000.0),\n",
    "                \"max_mem_alloc_mb\": mem_alloc,\n",
    "                \"max_mem_reserved_mb\": mem_reserved,\n",
    "            })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No data recorded for benchmark\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    summary = {\n",
    "        \"label\": label,\n",
    "        \"avg_forward_ms\": df[\"fwd_ms\"].mean(),\n",
    "        \"avg_backward_ms\": df[\"bwd_ms\"].mean(),\n",
    "        \"avg_step_ms\": df[\"step_ms\"].mean(),\n",
    "        \"samples_per_s\": df[\"throughput_sps\"].mean(),\n",
    "        \"max_mem_alloc_mb\": df[\"max_mem_alloc_mb\"].max(),\n",
    "        \"max_mem_reserved_mb\": df[\"max_mem_reserved_mb\"].max(),\n",
    "    }\n",
    "    return df, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037f8b1",
   "metadata": {},
   "source": [
    "Таблица `summary_df` показывает средние метрики по каждому batch size: `avg_forward_ms`, `avg_backward_ms`, `avg_step_ms`, `samples_per_s`, а также пики памяти (`max_mem_alloc_mb`, `max_mem_reserved_mb`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f25b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch size 32 ===\n",
      "=== Batch size 64 ===\n",
      "=== Batch size 96 ===\n",
      "=== Batch size 128 ===\n",
      "=== Batch size 160 ===\n",
      "=== Batch size 192 ===\n",
      "=== Batch size 256 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>avg_forward_ms</th>\n",
       "      <th>avg_backward_ms</th>\n",
       "      <th>avg_step_ms</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>max_mem_alloc_mb</th>\n",
       "      <th>max_mem_reserved_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variant</th>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>nn.Conv2d (bs=32)</td>\n",
       "      <td>6.945951</td>\n",
       "      <td>7.504282</td>\n",
       "      <td>14.450233</td>\n",
       "      <td>2321.329362</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>Baseline TritonConv2d (bs=32)</td>\n",
       "      <td>45.697910</td>\n",
       "      <td>18.743004</td>\n",
       "      <td>64.440913</td>\n",
       "      <td>498.520284</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>64</th>\n",
       "      <td>nn.Conv2d (bs=64)</td>\n",
       "      <td>7.130069</td>\n",
       "      <td>6.846434</td>\n",
       "      <td>13.976503</td>\n",
       "      <td>4738.278499</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>64</th>\n",
       "      <td>Baseline TritonConv2d (bs=64)</td>\n",
       "      <td>46.524829</td>\n",
       "      <td>19.813932</td>\n",
       "      <td>66.338762</td>\n",
       "      <td>973.842799</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>96</th>\n",
       "      <td>nn.Conv2d (bs=96)</td>\n",
       "      <td>5.744413</td>\n",
       "      <td>8.117101</td>\n",
       "      <td>13.861514</td>\n",
       "      <td>7074.106999</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>96</th>\n",
       "      <td>Baseline TritonConv2d (bs=96)</td>\n",
       "      <td>44.192646</td>\n",
       "      <td>22.477297</td>\n",
       "      <td>66.669944</td>\n",
       "      <td>1448.606724</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>128</th>\n",
       "      <td>nn.Conv2d (bs=128)</td>\n",
       "      <td>8.705702</td>\n",
       "      <td>8.687528</td>\n",
       "      <td>17.393231</td>\n",
       "      <td>7575.380323</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>128</th>\n",
       "      <td>Baseline TritonConv2d (bs=128)</td>\n",
       "      <td>46.460595</td>\n",
       "      <td>27.343490</td>\n",
       "      <td>73.804085</td>\n",
       "      <td>1750.624751</td>\n",
       "      <td>281.832520</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>160</th>\n",
       "      <td>nn.Conv2d (bs=160)</td>\n",
       "      <td>8.192318</td>\n",
       "      <td>13.367062</td>\n",
       "      <td>21.559380</td>\n",
       "      <td>7779.820179</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>160</th>\n",
       "      <td>Baseline TritonConv2d (bs=160)</td>\n",
       "      <td>45.952003</td>\n",
       "      <td>32.969670</td>\n",
       "      <td>78.921672</td>\n",
       "      <td>2044.275960</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>192</th>\n",
       "      <td>nn.Conv2d (bs=192)</td>\n",
       "      <td>6.524090</td>\n",
       "      <td>11.664851</td>\n",
       "      <td>18.188941</td>\n",
       "      <td>10674.234165</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>192</th>\n",
       "      <td>Baseline TritonConv2d (bs=192)</td>\n",
       "      <td>42.599159</td>\n",
       "      <td>37.132375</td>\n",
       "      <td>79.731534</td>\n",
       "      <td>2420.601214</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn.Conv2d</th>\n",
       "      <th>256</th>\n",
       "      <td>nn.Conv2d (bs=256)</td>\n",
       "      <td>7.873459</td>\n",
       "      <td>14.507681</td>\n",
       "      <td>22.381140</td>\n",
       "      <td>11634.171224</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline TritonConv2d</th>\n",
       "      <th>256</th>\n",
       "      <td>Baseline TritonConv2d (bs=256)</td>\n",
       "      <td>44.108297</td>\n",
       "      <td>53.017687</td>\n",
       "      <td>97.125984</td>\n",
       "      <td>2647.492920</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>598.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           label  \\\n",
       "variant               batch_size                                   \n",
       "nn.Conv2d             32                       nn.Conv2d (bs=32)   \n",
       "Baseline TritonConv2d 32           Baseline TritonConv2d (bs=32)   \n",
       "nn.Conv2d             64                       nn.Conv2d (bs=64)   \n",
       "Baseline TritonConv2d 64           Baseline TritonConv2d (bs=64)   \n",
       "nn.Conv2d             96                       nn.Conv2d (bs=96)   \n",
       "Baseline TritonConv2d 96           Baseline TritonConv2d (bs=96)   \n",
       "nn.Conv2d             128                     nn.Conv2d (bs=128)   \n",
       "Baseline TritonConv2d 128         Baseline TritonConv2d (bs=128)   \n",
       "nn.Conv2d             160                     nn.Conv2d (bs=160)   \n",
       "Baseline TritonConv2d 160         Baseline TritonConv2d (bs=160)   \n",
       "nn.Conv2d             192                     nn.Conv2d (bs=192)   \n",
       "Baseline TritonConv2d 192         Baseline TritonConv2d (bs=192)   \n",
       "nn.Conv2d             256                     nn.Conv2d (bs=256)   \n",
       "Baseline TritonConv2d 256         Baseline TritonConv2d (bs=256)   \n",
       "\n",
       "                                  avg_forward_ms  avg_backward_ms  \\\n",
       "variant               batch_size                                    \n",
       "nn.Conv2d             32                6.945951         7.504282   \n",
       "Baseline TritonConv2d 32               45.697910        18.743004   \n",
       "nn.Conv2d             64                7.130069         6.846434   \n",
       "Baseline TritonConv2d 64               46.524829        19.813932   \n",
       "nn.Conv2d             96                5.744413         8.117101   \n",
       "Baseline TritonConv2d 96               44.192646        22.477297   \n",
       "nn.Conv2d             128               8.705702         8.687528   \n",
       "Baseline TritonConv2d 128              46.460595        27.343490   \n",
       "nn.Conv2d             160               8.192318        13.367062   \n",
       "Baseline TritonConv2d 160              45.952003        32.969670   \n",
       "nn.Conv2d             192               6.524090        11.664851   \n",
       "Baseline TritonConv2d 192              42.599159        37.132375   \n",
       "nn.Conv2d             256               7.873459        14.507681   \n",
       "Baseline TritonConv2d 256              44.108297        53.017687   \n",
       "\n",
       "                                  avg_step_ms  samples_per_s  \\\n",
       "variant               batch_size                               \n",
       "nn.Conv2d             32            14.450233    2321.329362   \n",
       "Baseline TritonConv2d 32            64.440913     498.520284   \n",
       "nn.Conv2d             64            13.976503    4738.278499   \n",
       "Baseline TritonConv2d 64            66.338762     973.842799   \n",
       "nn.Conv2d             96            13.861514    7074.106999   \n",
       "Baseline TritonConv2d 96            66.669944    1448.606724   \n",
       "nn.Conv2d             128           17.393231    7575.380323   \n",
       "Baseline TritonConv2d 128           73.804085    1750.624751   \n",
       "nn.Conv2d             160           21.559380    7779.820179   \n",
       "Baseline TritonConv2d 160           78.921672    2044.275960   \n",
       "nn.Conv2d             192           18.188941   10674.234165   \n",
       "Baseline TritonConv2d 192           79.731534    2420.601214   \n",
       "nn.Conv2d             256           22.381140   11634.171224   \n",
       "Baseline TritonConv2d 256           97.125984    2647.492920   \n",
       "\n",
       "                                  max_mem_alloc_mb  max_mem_reserved_mb  \n",
       "variant               batch_size                                         \n",
       "nn.Conv2d             32                192.057129                220.0  \n",
       "Baseline TritonConv2d 32                207.492676                226.0  \n",
       "nn.Conv2d             64                190.058105                228.0  \n",
       "Baseline TritonConv2d 64                228.024902                258.0  \n",
       "nn.Conv2d             96                190.685059                222.0  \n",
       "Baseline TritonConv2d 96                252.019043                312.0  \n",
       "nn.Conv2d             128               204.617676                228.0  \n",
       "Baseline TritonConv2d 128               281.832520                340.0  \n",
       "nn.Conv2d             160               219.119629                252.0  \n",
       "Baseline TritonConv2d 160               306.271973                366.0  \n",
       "nn.Conv2d             192               230.495605                274.0  \n",
       "Baseline TritonConv2d 192               334.960449                402.0  \n",
       "nn.Conv2d             256               258.373535                306.0  \n",
       "Baseline TritonConv2d 256               383.338379                598.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_summaries = []\n",
    "batch_details = []\n",
    "\n",
    "for bs, loader in train_loaders.items():\n",
    "    print(f\"=== Batch size {bs} ===\")\n",
    "    torch_model, baseline_model = build_model_pair(config)\n",
    "\n",
    "    torch_df, torch_summary = run_benchmark(torch_model, f\"nn.Conv2d (bs={bs})\", loader, config)\n",
    "    torch_summary.update({\"variant\": \"nn.Conv2d\", \"batch_size\": bs})\n",
    "    batch_summaries.append(torch_summary)\n",
    "    batch_details.append(torch_df.assign(variant=\"nn.Conv2d\", batch_size=bs))\n",
    "\n",
    "    baseline_df, baseline_summary = run_benchmark(baseline_model, f\"Baseline TritonConv2d (bs={bs})\", loader, config)\n",
    "    baseline_summary.update({\"variant\": \"Baseline TritonConv2d\", \"batch_size\": bs})\n",
    "    batch_summaries.append(baseline_summary)\n",
    "    batch_details.append(baseline_df.assign(variant=\"Baseline TritonConv2d\", batch_size=bs))\n",
    "\n",
    "summary_df = pd.DataFrame(batch_summaries).set_index([\"variant\", \"batch_size\"])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b90a09",
   "metadata": {},
   "source": [
    "Вывод `detail_df.groupby(...).describe()` содержит count/mean/std/min/25%/50%/75%/max для метрик `step_ms`, `fwd_ms`, `bwd_ms`, `max_mem_alloc_mb` отдельно по каждому `(variant, batch_size)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e564a702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">step_ms</th>\n",
       "      <th colspan=\"2\" halign=\"left\">fwd_ms</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">bwd_ms</th>\n",
       "      <th colspan=\"8\" halign=\"left\">max_mem_alloc_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variant</th>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Baseline TritonConv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>35.0</td>\n",
       "      <td>64.440913</td>\n",
       "      <td>4.251413</td>\n",
       "      <td>56.388992</td>\n",
       "      <td>62.039184</td>\n",
       "      <td>63.634241</td>\n",
       "      <td>65.847728</td>\n",
       "      <td>80.410431</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.697910</td>\n",
       "      <td>...</td>\n",
       "      <td>18.828288</td>\n",
       "      <td>28.275711</td>\n",
       "      <td>35.0</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>207.492676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>35.0</td>\n",
       "      <td>66.338762</td>\n",
       "      <td>6.978582</td>\n",
       "      <td>58.422527</td>\n",
       "      <td>62.235024</td>\n",
       "      <td>64.566689</td>\n",
       "      <td>68.469855</td>\n",
       "      <td>86.814560</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.524829</td>\n",
       "      <td>...</td>\n",
       "      <td>20.283393</td>\n",
       "      <td>22.143999</td>\n",
       "      <td>35.0</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>228.024902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>35.0</td>\n",
       "      <td>66.669944</td>\n",
       "      <td>5.457417</td>\n",
       "      <td>60.666687</td>\n",
       "      <td>62.735361</td>\n",
       "      <td>64.800766</td>\n",
       "      <td>69.165567</td>\n",
       "      <td>81.938688</td>\n",
       "      <td>35.0</td>\n",
       "      <td>44.192646</td>\n",
       "      <td>...</td>\n",
       "      <td>22.541823</td>\n",
       "      <td>27.817984</td>\n",
       "      <td>35.0</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>252.019043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>35.0</td>\n",
       "      <td>73.804085</td>\n",
       "      <td>7.826183</td>\n",
       "      <td>65.466335</td>\n",
       "      <td>68.828208</td>\n",
       "      <td>71.731010</td>\n",
       "      <td>74.427024</td>\n",
       "      <td>102.152164</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.460595</td>\n",
       "      <td>...</td>\n",
       "      <td>26.953216</td>\n",
       "      <td>35.956738</td>\n",
       "      <td>35.0</td>\n",
       "      <td>281.125377</td>\n",
       "      <td>0.748738</td>\n",
       "      <td>280.082520</td>\n",
       "      <td>280.457520</td>\n",
       "      <td>280.832520</td>\n",
       "      <td>281.832520</td>\n",
       "      <td>281.832520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>35.0</td>\n",
       "      <td>78.921672</td>\n",
       "      <td>7.717207</td>\n",
       "      <td>68.343361</td>\n",
       "      <td>73.366129</td>\n",
       "      <td>77.659521</td>\n",
       "      <td>81.677439</td>\n",
       "      <td>101.251940</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.952003</td>\n",
       "      <td>...</td>\n",
       "      <td>33.454079</td>\n",
       "      <td>43.146240</td>\n",
       "      <td>35.0</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>306.271973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>35.0</td>\n",
       "      <td>79.731534</td>\n",
       "      <td>6.119224</td>\n",
       "      <td>73.734718</td>\n",
       "      <td>75.315536</td>\n",
       "      <td>77.947838</td>\n",
       "      <td>80.613407</td>\n",
       "      <td>96.305569</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.599159</td>\n",
       "      <td>...</td>\n",
       "      <td>37.422081</td>\n",
       "      <td>41.207809</td>\n",
       "      <td>35.0</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>334.960449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>35.0</td>\n",
       "      <td>97.125984</td>\n",
       "      <td>6.911990</td>\n",
       "      <td>89.954685</td>\n",
       "      <td>91.713455</td>\n",
       "      <td>95.792736</td>\n",
       "      <td>100.641424</td>\n",
       "      <td>123.397758</td>\n",
       "      <td>35.0</td>\n",
       "      <td>44.108297</td>\n",
       "      <td>...</td>\n",
       "      <td>52.304897</td>\n",
       "      <td>65.390594</td>\n",
       "      <td>35.0</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>383.338379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">nn.Conv2d</th>\n",
       "      <th>32</th>\n",
       "      <td>35.0</td>\n",
       "      <td>14.450233</td>\n",
       "      <td>3.935573</td>\n",
       "      <td>11.075040</td>\n",
       "      <td>12.327680</td>\n",
       "      <td>13.103872</td>\n",
       "      <td>14.478848</td>\n",
       "      <td>30.395841</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.945951</td>\n",
       "      <td>...</td>\n",
       "      <td>7.135760</td>\n",
       "      <td>18.880512</td>\n",
       "      <td>35.0</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>192.057129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>35.0</td>\n",
       "      <td>13.976503</td>\n",
       "      <td>2.805161</td>\n",
       "      <td>10.746112</td>\n",
       "      <td>11.784752</td>\n",
       "      <td>12.721408</td>\n",
       "      <td>15.659072</td>\n",
       "      <td>21.054720</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.130069</td>\n",
       "      <td>...</td>\n",
       "      <td>6.983680</td>\n",
       "      <td>15.997952</td>\n",
       "      <td>35.0</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>190.058105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>35.0</td>\n",
       "      <td>13.861514</td>\n",
       "      <td>2.371935</td>\n",
       "      <td>12.315904</td>\n",
       "      <td>12.564736</td>\n",
       "      <td>12.817376</td>\n",
       "      <td>13.792912</td>\n",
       "      <td>23.297791</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.744413</td>\n",
       "      <td>...</td>\n",
       "      <td>7.607296</td>\n",
       "      <td>18.003967</td>\n",
       "      <td>35.0</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>190.685059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>35.0</td>\n",
       "      <td>17.393231</td>\n",
       "      <td>3.226433</td>\n",
       "      <td>13.574464</td>\n",
       "      <td>15.503536</td>\n",
       "      <td>16.532577</td>\n",
       "      <td>18.523696</td>\n",
       "      <td>26.371520</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.705702</td>\n",
       "      <td>...</td>\n",
       "      <td>8.825856</td>\n",
       "      <td>11.010048</td>\n",
       "      <td>35.0</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>204.617676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>35.0</td>\n",
       "      <td>21.559380</td>\n",
       "      <td>4.983187</td>\n",
       "      <td>15.366944</td>\n",
       "      <td>17.359216</td>\n",
       "      <td>20.084896</td>\n",
       "      <td>26.242592</td>\n",
       "      <td>31.677920</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.192318</td>\n",
       "      <td>...</td>\n",
       "      <td>15.683584</td>\n",
       "      <td>24.272896</td>\n",
       "      <td>35.0</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>219.119629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>35.0</td>\n",
       "      <td>18.188941</td>\n",
       "      <td>2.140933</td>\n",
       "      <td>16.097088</td>\n",
       "      <td>16.856624</td>\n",
       "      <td>17.436352</td>\n",
       "      <td>18.659232</td>\n",
       "      <td>26.378241</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.524090</td>\n",
       "      <td>...</td>\n",
       "      <td>11.556864</td>\n",
       "      <td>21.215233</td>\n",
       "      <td>35.0</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>230.495605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>35.0</td>\n",
       "      <td>22.381140</td>\n",
       "      <td>3.178098</td>\n",
       "      <td>19.657696</td>\n",
       "      <td>20.026768</td>\n",
       "      <td>21.066752</td>\n",
       "      <td>23.349056</td>\n",
       "      <td>30.037889</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.873459</td>\n",
       "      <td>...</td>\n",
       "      <td>14.148096</td>\n",
       "      <td>22.427649</td>\n",
       "      <td>35.0</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>258.373535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 step_ms                                  \\\n",
       "                                   count       mean       std        min   \n",
       "variant               batch_size                                           \n",
       "Baseline TritonConv2d 32            35.0  64.440913  4.251413  56.388992   \n",
       "                      64            35.0  66.338762  6.978582  58.422527   \n",
       "                      96            35.0  66.669944  5.457417  60.666687   \n",
       "                      128           35.0  73.804085  7.826183  65.466335   \n",
       "                      160           35.0  78.921672  7.717207  68.343361   \n",
       "                      192           35.0  79.731534  6.119224  73.734718   \n",
       "                      256           35.0  97.125984  6.911990  89.954685   \n",
       "nn.Conv2d             32            35.0  14.450233  3.935573  11.075040   \n",
       "                      64            35.0  13.976503  2.805161  10.746112   \n",
       "                      96            35.0  13.861514  2.371935  12.315904   \n",
       "                      128           35.0  17.393231  3.226433  13.574464   \n",
       "                      160           35.0  21.559380  4.983187  15.366944   \n",
       "                      192           35.0  18.188941  2.140933  16.097088   \n",
       "                      256           35.0  22.381140  3.178098  19.657696   \n",
       "\n",
       "                                                                    \\\n",
       "                                        25%        50%         75%   \n",
       "variant               batch_size                                     \n",
       "Baseline TritonConv2d 32          62.039184  63.634241   65.847728   \n",
       "                      64          62.235024  64.566689   68.469855   \n",
       "                      96          62.735361  64.800766   69.165567   \n",
       "                      128         68.828208  71.731010   74.427024   \n",
       "                      160         73.366129  77.659521   81.677439   \n",
       "                      192         75.315536  77.947838   80.613407   \n",
       "                      256         91.713455  95.792736  100.641424   \n",
       "nn.Conv2d             32          12.327680  13.103872   14.478848   \n",
       "                      64          11.784752  12.721408   15.659072   \n",
       "                      96          12.564736  12.817376   13.792912   \n",
       "                      128         15.503536  16.532577   18.523696   \n",
       "                      160         17.359216  20.084896   26.242592   \n",
       "                      192         16.856624  17.436352   18.659232   \n",
       "                      256         20.026768  21.066752   23.349056   \n",
       "\n",
       "                                             fwd_ms             ...  \\\n",
       "                                         max  count       mean  ...   \n",
       "variant               batch_size                                ...   \n",
       "Baseline TritonConv2d 32           80.410431   35.0  45.697910  ...   \n",
       "                      64           86.814560   35.0  46.524829  ...   \n",
       "                      96           81.938688   35.0  44.192646  ...   \n",
       "                      128         102.152164   35.0  46.460595  ...   \n",
       "                      160         101.251940   35.0  45.952003  ...   \n",
       "                      192          96.305569   35.0  42.599159  ...   \n",
       "                      256         123.397758   35.0  44.108297  ...   \n",
       "nn.Conv2d             32           30.395841   35.0   6.945951  ...   \n",
       "                      64           21.054720   35.0   7.130069  ...   \n",
       "                      96           23.297791   35.0   5.744413  ...   \n",
       "                      128          26.371520   35.0   8.705702  ...   \n",
       "                      160          31.677920   35.0   8.192318  ...   \n",
       "                      192          26.378241   35.0   6.524090  ...   \n",
       "                      256          30.037889   35.0   7.873459  ...   \n",
       "\n",
       "                                     bwd_ms            max_mem_alloc_mb  \\\n",
       "                                        75%        max            count   \n",
       "variant               batch_size                                          \n",
       "Baseline TritonConv2d 32          18.828288  28.275711             35.0   \n",
       "                      64          20.283393  22.143999             35.0   \n",
       "                      96          22.541823  27.817984             35.0   \n",
       "                      128         26.953216  35.956738             35.0   \n",
       "                      160         33.454079  43.146240             35.0   \n",
       "                      192         37.422081  41.207809             35.0   \n",
       "                      256         52.304897  65.390594             35.0   \n",
       "nn.Conv2d             32           7.135760  18.880512             35.0   \n",
       "                      64           6.983680  15.997952             35.0   \n",
       "                      96           7.607296  18.003967             35.0   \n",
       "                      128          8.825856  11.010048             35.0   \n",
       "                      160         15.683584  24.272896             35.0   \n",
       "                      192         11.556864  21.215233             35.0   \n",
       "                      256         14.148096  22.427649             35.0   \n",
       "\n",
       "                                                                    \\\n",
       "                                        mean       std         min   \n",
       "variant               batch_size                                     \n",
       "Baseline TritonConv2d 32          207.492676  0.000000  207.492676   \n",
       "                      64          228.024902  0.000000  228.024902   \n",
       "                      96          252.019043  0.000000  252.019043   \n",
       "                      128         281.125377  0.748738  280.082520   \n",
       "                      160         306.271973  0.000000  306.271973   \n",
       "                      192         334.960449  0.000000  334.960449   \n",
       "                      256         383.338379  0.000000  383.338379   \n",
       "nn.Conv2d             32          192.057129  0.000000  192.057129   \n",
       "                      64          190.058105  0.000000  190.058105   \n",
       "                      96          190.685059  0.000000  190.685059   \n",
       "                      128         204.617676  0.000000  204.617676   \n",
       "                      160         219.119629  0.000000  219.119629   \n",
       "                      192         230.495605  0.000000  230.495605   \n",
       "                      256         258.373535  0.000000  258.373535   \n",
       "\n",
       "                                                                      \\\n",
       "                                         25%         50%         75%   \n",
       "variant               batch_size                                       \n",
       "Baseline TritonConv2d 32          207.492676  207.492676  207.492676   \n",
       "                      64          228.024902  228.024902  228.024902   \n",
       "                      96          252.019043  252.019043  252.019043   \n",
       "                      128         280.457520  280.832520  281.832520   \n",
       "                      160         306.271973  306.271973  306.271973   \n",
       "                      192         334.960449  334.960449  334.960449   \n",
       "                      256         383.338379  383.338379  383.338379   \n",
       "nn.Conv2d             32          192.057129  192.057129  192.057129   \n",
       "                      64          190.058105  190.058105  190.058105   \n",
       "                      96          190.685059  190.685059  190.685059   \n",
       "                      128         204.617676  204.617676  204.617676   \n",
       "                      160         219.119629  219.119629  219.119629   \n",
       "                      192         230.495605  230.495605  230.495605   \n",
       "                      256         258.373535  258.373535  258.373535   \n",
       "\n",
       "                                              \n",
       "                                         max  \n",
       "variant               batch_size              \n",
       "Baseline TritonConv2d 32          207.492676  \n",
       "                      64          228.024902  \n",
       "                      96          252.019043  \n",
       "                      128         281.832520  \n",
       "                      160         306.271973  \n",
       "                      192         334.960449  \n",
       "                      256         383.338379  \n",
       "nn.Conv2d             32          192.057129  \n",
       "                      64          190.058105  \n",
       "                      96          190.685059  \n",
       "                      128         204.617676  \n",
       "                      160         219.119629  \n",
       "                      192         230.495605  \n",
       "                      256         258.373535  \n",
       "\n",
       "[14 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_df = pd.concat(batch_details, ignore_index=True)\n",
    "metrics = [\"step_ms\", \"fwd_ms\", \"bwd_ms\", \"max_mem_alloc_mb\"]\n",
    "detail_df.groupby([\"variant\", \"batch_size\"])[metrics].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd2b6d",
   "metadata": {},
   "source": [
    "`baseline_vs_torch_df` сравнивает nn.Conv2d и Baseline TritonConv2d: пары столбцов с абсолютными значениями (forward/backward/step время, throughput, память) и коэффициенты ускорения (`speedup_*`, `throughput_ratio`, `mem_*_ratio`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d81011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>torch_forward_ms</th>\n",
       "      <th>baseline_forward_ms</th>\n",
       "      <th>torch_backward_ms</th>\n",
       "      <th>baseline_backward_ms</th>\n",
       "      <th>torch_step_ms</th>\n",
       "      <th>baseline_step_ms</th>\n",
       "      <th>torch_samples_per_s</th>\n",
       "      <th>baseline_samples_per_s</th>\n",
       "      <th>speedup_forward</th>\n",
       "      <th>speedup_backward</th>\n",
       "      <th>speedup_step</th>\n",
       "      <th>throughput_ratio</th>\n",
       "      <th>torch_mem_alloc_mb</th>\n",
       "      <th>baseline_mem_alloc_mb</th>\n",
       "      <th>torch_mem_reserved_mb</th>\n",
       "      <th>baseline_mem_reserved_mb</th>\n",
       "      <th>mem_alloc_ratio</th>\n",
       "      <th>mem_reserved_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6.945951</td>\n",
       "      <td>45.697910</td>\n",
       "      <td>7.504282</td>\n",
       "      <td>18.743004</td>\n",
       "      <td>14.450233</td>\n",
       "      <td>64.440913</td>\n",
       "      <td>2321.329362</td>\n",
       "      <td>498.520284</td>\n",
       "      <td>0.151997</td>\n",
       "      <td>0.400378</td>\n",
       "      <td>0.224240</td>\n",
       "      <td>0.214756</td>\n",
       "      <td>192.057129</td>\n",
       "      <td>207.492676</td>\n",
       "      <td>220.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1.080370</td>\n",
       "      <td>1.027273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>7.130069</td>\n",
       "      <td>46.524829</td>\n",
       "      <td>6.846434</td>\n",
       "      <td>19.813932</td>\n",
       "      <td>13.976503</td>\n",
       "      <td>66.338762</td>\n",
       "      <td>4738.278499</td>\n",
       "      <td>973.842799</td>\n",
       "      <td>0.153253</td>\n",
       "      <td>0.345536</td>\n",
       "      <td>0.210684</td>\n",
       "      <td>0.205527</td>\n",
       "      <td>190.058105</td>\n",
       "      <td>228.024902</td>\n",
       "      <td>228.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>1.199764</td>\n",
       "      <td>1.131579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.744413</td>\n",
       "      <td>44.192646</td>\n",
       "      <td>8.117101</td>\n",
       "      <td>22.477297</td>\n",
       "      <td>13.861514</td>\n",
       "      <td>66.669944</td>\n",
       "      <td>7074.106999</td>\n",
       "      <td>1448.606724</td>\n",
       "      <td>0.129986</td>\n",
       "      <td>0.361124</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.204776</td>\n",
       "      <td>190.685059</td>\n",
       "      <td>252.019043</td>\n",
       "      <td>222.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>1.321651</td>\n",
       "      <td>1.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>8.705702</td>\n",
       "      <td>46.460595</td>\n",
       "      <td>8.687528</td>\n",
       "      <td>27.343490</td>\n",
       "      <td>17.393231</td>\n",
       "      <td>73.804085</td>\n",
       "      <td>7575.380323</td>\n",
       "      <td>1750.624751</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>0.317718</td>\n",
       "      <td>0.235668</td>\n",
       "      <td>0.231094</td>\n",
       "      <td>204.617676</td>\n",
       "      <td>281.832520</td>\n",
       "      <td>228.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>1.377362</td>\n",
       "      <td>1.491228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>8.192318</td>\n",
       "      <td>45.952003</td>\n",
       "      <td>13.367062</td>\n",
       "      <td>32.969670</td>\n",
       "      <td>21.559380</td>\n",
       "      <td>78.921672</td>\n",
       "      <td>7779.820179</td>\n",
       "      <td>2044.275960</td>\n",
       "      <td>0.178280</td>\n",
       "      <td>0.405435</td>\n",
       "      <td>0.273174</td>\n",
       "      <td>0.262766</td>\n",
       "      <td>219.119629</td>\n",
       "      <td>306.271973</td>\n",
       "      <td>252.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>1.397739</td>\n",
       "      <td>1.452381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>6.524090</td>\n",
       "      <td>42.599159</td>\n",
       "      <td>11.664851</td>\n",
       "      <td>37.132375</td>\n",
       "      <td>18.188941</td>\n",
       "      <td>79.731534</td>\n",
       "      <td>10674.234165</td>\n",
       "      <td>2420.601214</td>\n",
       "      <td>0.153151</td>\n",
       "      <td>0.314142</td>\n",
       "      <td>0.228127</td>\n",
       "      <td>0.226770</td>\n",
       "      <td>230.495605</td>\n",
       "      <td>334.960449</td>\n",
       "      <td>274.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>1.453218</td>\n",
       "      <td>1.467153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>7.873459</td>\n",
       "      <td>44.108297</td>\n",
       "      <td>14.507681</td>\n",
       "      <td>53.017687</td>\n",
       "      <td>22.381140</td>\n",
       "      <td>97.125984</td>\n",
       "      <td>11634.171224</td>\n",
       "      <td>2647.492920</td>\n",
       "      <td>0.178503</td>\n",
       "      <td>0.273639</td>\n",
       "      <td>0.230434</td>\n",
       "      <td>0.227562</td>\n",
       "      <td>258.373535</td>\n",
       "      <td>383.338379</td>\n",
       "      <td>306.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>1.483660</td>\n",
       "      <td>1.954248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            torch_forward_ms  baseline_forward_ms  torch_backward_ms  \\\n",
       "batch_size                                                             \n",
       "32                  6.945951            45.697910           7.504282   \n",
       "64                  7.130069            46.524829           6.846434   \n",
       "96                  5.744413            44.192646           8.117101   \n",
       "128                 8.705702            46.460595           8.687528   \n",
       "160                 8.192318            45.952003          13.367062   \n",
       "192                 6.524090            42.599159          11.664851   \n",
       "256                 7.873459            44.108297          14.507681   \n",
       "\n",
       "            baseline_backward_ms  torch_step_ms  baseline_step_ms  \\\n",
       "batch_size                                                          \n",
       "32                     18.743004      14.450233         64.440913   \n",
       "64                     19.813932      13.976503         66.338762   \n",
       "96                     22.477297      13.861514         66.669944   \n",
       "128                    27.343490      17.393231         73.804085   \n",
       "160                    32.969670      21.559380         78.921672   \n",
       "192                    37.132375      18.188941         79.731534   \n",
       "256                    53.017687      22.381140         97.125984   \n",
       "\n",
       "            torch_samples_per_s  baseline_samples_per_s  speedup_forward  \\\n",
       "batch_size                                                                 \n",
       "32                  2321.329362              498.520284         0.151997   \n",
       "64                  4738.278499              973.842799         0.153253   \n",
       "96                  7074.106999             1448.606724         0.129986   \n",
       "128                 7575.380323             1750.624751         0.187378   \n",
       "160                 7779.820179             2044.275960         0.178280   \n",
       "192                10674.234165             2420.601214         0.153151   \n",
       "256                11634.171224             2647.492920         0.178503   \n",
       "\n",
       "            speedup_backward  speedup_step  throughput_ratio  \\\n",
       "batch_size                                                     \n",
       "32                  0.400378      0.224240          0.214756   \n",
       "64                  0.345536      0.210684          0.205527   \n",
       "96                  0.361124      0.207912          0.204776   \n",
       "128                 0.317718      0.235668          0.231094   \n",
       "160                 0.405435      0.273174          0.262766   \n",
       "192                 0.314142      0.228127          0.226770   \n",
       "256                 0.273639      0.230434          0.227562   \n",
       "\n",
       "            torch_mem_alloc_mb  baseline_mem_alloc_mb  torch_mem_reserved_mb  \\\n",
       "batch_size                                                                     \n",
       "32                  192.057129             207.492676                  220.0   \n",
       "64                  190.058105             228.024902                  228.0   \n",
       "96                  190.685059             252.019043                  222.0   \n",
       "128                 204.617676             281.832520                  228.0   \n",
       "160                 219.119629             306.271973                  252.0   \n",
       "192                 230.495605             334.960449                  274.0   \n",
       "256                 258.373535             383.338379                  306.0   \n",
       "\n",
       "            baseline_mem_reserved_mb  mem_alloc_ratio  mem_reserved_ratio  \n",
       "batch_size                                                                 \n",
       "32                             226.0         1.080370            1.027273  \n",
       "64                             258.0         1.199764            1.131579  \n",
       "96                             312.0         1.321651            1.405405  \n",
       "128                            340.0         1.377362            1.491228  \n",
       "160                            366.0         1.397739            1.452381  \n",
       "192                            402.0         1.453218            1.467153  \n",
       "256                            598.0         1.483660            1.954248  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_compare_rows = []\n",
    "for bs in config[\"batch_sizes\"]:\n",
    "    torch_row = summary_df.loc[(\"nn.Conv2d\", bs)]\n",
    "    baseline_row = summary_df.loc[(\"Baseline TritonConv2d\", bs)]\n",
    "    comparison = {\n",
    "        \"batch_size\": bs,\n",
    "        \"torch_forward_ms\": torch_row[\"avg_forward_ms\"],\n",
    "        \"baseline_forward_ms\": baseline_row[\"avg_forward_ms\"],\n",
    "        \"torch_backward_ms\": torch_row[\"avg_backward_ms\"],\n",
    "        \"baseline_backward_ms\": baseline_row[\"avg_backward_ms\"],\n",
    "        \"torch_step_ms\": torch_row[\"avg_step_ms\"],\n",
    "        \"baseline_step_ms\": baseline_row[\"avg_step_ms\"],\n",
    "        \"torch_samples_per_s\": torch_row[\"samples_per_s\"],\n",
    "        \"baseline_samples_per_s\": baseline_row[\"samples_per_s\"],\n",
    "        \"speedup_forward\": torch_row[\"avg_forward_ms\"] / baseline_row[\"avg_forward_ms\"],\n",
    "        \"speedup_backward\": torch_row[\"avg_backward_ms\"] / baseline_row[\"avg_backward_ms\"],\n",
    "        \"speedup_step\": torch_row[\"avg_step_ms\"] / baseline_row[\"avg_step_ms\"],\n",
    "        \"throughput_ratio\": baseline_row[\"samples_per_s\"] / torch_row[\"samples_per_s\"],\n",
    "        \"torch_mem_alloc_mb\": torch_row[\"max_mem_alloc_mb\"],\n",
    "        \"baseline_mem_alloc_mb\": baseline_row[\"max_mem_alloc_mb\"],\n",
    "        \"torch_mem_reserved_mb\": torch_row[\"max_mem_reserved_mb\"],\n",
    "        \"baseline_mem_reserved_mb\": baseline_row[\"max_mem_reserved_mb\"],\n",
    "        \"mem_alloc_ratio\": baseline_row[\"max_mem_alloc_mb\"] / torch_row[\"max_mem_alloc_mb\"],\n",
    "        \"mem_reserved_ratio\": baseline_row[\"max_mem_reserved_mb\"] / torch_row[\"max_mem_reserved_mb\"],\n",
    "    }\n",
    "    baseline_compare_rows.append(comparison)\n",
    "\n",
    "baseline_vs_torch_df = pd.DataFrame(baseline_compare_rows).set_index(\"batch_size\")\n",
    "baseline_vs_torch_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91f8b1",
   "metadata": {},
   "source": [
    "Таблица `summary_df` показывает средние метрики по каждому batch size: `avg_forward_ms`, `avg_backward_ms`, `avg_step_ms`, `samples_per_s`, а также пики памяти (`max_mem_alloc_mb`, `max_mem_reserved_mb`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fee49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>avg_forward_ms</th>\n",
       "      <th>avg_backward_ms</th>\n",
       "      <th>avg_step_ms</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>max_mem_alloc_mb</th>\n",
       "      <th>max_mem_reserved_mb</th>\n",
       "      <th>variant</th>\n",
       "      <th>mode</th>\n",
       "      <th>keep_ratio</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Channel sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>58.553237</td>\n",
       "      <td>27.390333</td>\n",
       "      <td>85.943570</td>\n",
       "      <td>1495.724046</td>\n",
       "      <td>281.906738</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Block sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>58.612525</td>\n",
       "      <td>27.804467</td>\n",
       "      <td>86.416992</td>\n",
       "      <td>1494.505578</td>\n",
       "      <td>277.916504</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Channel sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>58.730010</td>\n",
       "      <td>28.239315</td>\n",
       "      <td>86.969325</td>\n",
       "      <td>1479.867645</td>\n",
       "      <td>280.566895</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Block sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>60.174384</td>\n",
       "      <td>27.837381</td>\n",
       "      <td>88.011765</td>\n",
       "      <td>1464.490586</td>\n",
       "      <td>281.906738</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Channel sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>62.127724</td>\n",
       "      <td>27.842911</td>\n",
       "      <td>89.970635</td>\n",
       "      <td>1441.579470</td>\n",
       "      <td>277.916504</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Channel sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>62.129142</td>\n",
       "      <td>27.794782</td>\n",
       "      <td>89.923925</td>\n",
       "      <td>1435.233769</td>\n",
       "      <td>276.178711</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Input sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>60.846281</td>\n",
       "      <td>29.797142</td>\n",
       "      <td>90.643423</td>\n",
       "      <td>1420.362348</td>\n",
       "      <td>257.366699</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Block sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>61.405683</td>\n",
       "      <td>29.465658</td>\n",
       "      <td>90.871340</td>\n",
       "      <td>1419.049735</td>\n",
       "      <td>280.562500</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Input sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>70.232736</td>\n",
       "      <td>28.134634</td>\n",
       "      <td>98.367370</td>\n",
       "      <td>1416.643851</td>\n",
       "      <td>260.319336</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Input sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>61.056020</td>\n",
       "      <td>31.047008</td>\n",
       "      <td>92.103029</td>\n",
       "      <td>1404.039844</td>\n",
       "      <td>267.901855</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Input sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>65.574614</td>\n",
       "      <td>28.594586</td>\n",
       "      <td>94.169200</td>\n",
       "      <td>1367.928489</td>\n",
       "      <td>247.798828</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Block sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>64.929408</td>\n",
       "      <td>30.507008</td>\n",
       "      <td>95.436416</td>\n",
       "      <td>1353.455830</td>\n",
       "      <td>276.178711</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   label  avg_forward_ms  avg_backward_ms  \\\n",
       "0   Channel sparsity (keep=0.75, bs=128)       58.553237        27.390333   \n",
       "1     Block sparsity (keep=0.50, bs=128)       58.612525        27.804467   \n",
       "2   Channel sparsity (keep=0.60, bs=128)       58.730010        28.239315   \n",
       "3     Block sparsity (keep=0.75, bs=128)       60.174384        27.837381   \n",
       "4   Channel sparsity (keep=0.50, bs=128)       62.127724        27.842911   \n",
       "5   Channel sparsity (keep=0.25, bs=128)       62.129142        27.794782   \n",
       "6     Input sparsity (keep=0.50, bs=128)       60.846281        29.797142   \n",
       "7     Block sparsity (keep=0.60, bs=128)       61.405683        29.465658   \n",
       "8     Input sparsity (keep=0.60, bs=128)       70.232736        28.134634   \n",
       "9     Input sparsity (keep=0.75, bs=128)       61.056020        31.047008   \n",
       "10    Input sparsity (keep=0.25, bs=128)       65.574614        28.594586   \n",
       "11    Block sparsity (keep=0.25, bs=128)       64.929408        30.507008   \n",
       "\n",
       "    avg_step_ms  samples_per_s  max_mem_alloc_mb  max_mem_reserved_mb  \\\n",
       "0     85.943570    1495.724046        281.906738                602.0   \n",
       "1     86.416992    1494.505578        277.916504                602.0   \n",
       "2     86.969325    1479.867645        280.566895                602.0   \n",
       "3     88.011765    1464.490586        281.906738                602.0   \n",
       "4     89.970635    1441.579470        277.916504                602.0   \n",
       "5     89.923925    1435.233769        276.178711                602.0   \n",
       "6     90.643423    1420.362348        257.366699                602.0   \n",
       "7     90.871340    1419.049735        280.562500                602.0   \n",
       "8     98.367370    1416.643851        260.319336                602.0   \n",
       "9     92.103029    1404.039844        267.901855                602.0   \n",
       "10    94.169200    1367.928489        247.798828                602.0   \n",
       "11    95.436416    1353.455830        276.178711                602.0   \n",
       "\n",
       "              variant     mode  keep_ratio  batch_size  \n",
       "0   Sparsity::channel  channel        0.75         128  \n",
       "1     Sparsity::block    block        0.50         128  \n",
       "2   Sparsity::channel  channel        0.60         128  \n",
       "3     Sparsity::block    block        0.75         128  \n",
       "4   Sparsity::channel  channel        0.50         128  \n",
       "5   Sparsity::channel  channel        0.25         128  \n",
       "6     Sparsity::input    input        0.50         128  \n",
       "7     Sparsity::block    block        0.60         128  \n",
       "8     Sparsity::input    input        0.60         128  \n",
       "9     Sparsity::input    input        0.75         128  \n",
       "10    Sparsity::input    input        0.25         128  \n",
       "11    Sparsity::block    block        0.25         128  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_cfg = config[\"sparsity_bench\"]\n",
    "sparsity_bs = sparsity_cfg[\"batch_size\"]\n",
    "if sparsity_bs not in train_loaders:\n",
    "    train_loaders[sparsity_bs] = make_loader(sparsity_bs)\n",
    "sparsity_loader = train_loaders[sparsity_bs]\n",
    "\n",
    "sparsity_summaries = []\n",
    "sparsity_details = []\n",
    "\n",
    "for mode in sparsity_cfg[\"modes\"]:\n",
    "    for ratio in sparsity_cfg[\"keep_ratios\"]:\n",
    "        _, baseline_model = build_model_pair(config)\n",
    "        apply_sparsity_to_model(\n",
    "            baseline_model,\n",
    "            mode,\n",
    "            keep_ratio=ratio,\n",
    "            block_size=sparsity_cfg.get(\"block_size\", 4),\n",
    "        )\n",
    "        label = f\"{mode.capitalize()} sparsity (keep={ratio:.2f}, bs={sparsity_bs})\"\n",
    "        bench_df, bench_summary = run_benchmark(baseline_model, label, sparsity_loader, config)\n",
    "        bench_summary.update({\n",
    "            \"variant\": f\"Sparsity::{mode}\",\n",
    "            \"mode\": mode,\n",
    "            \"keep_ratio\": ratio,\n",
    "            \"batch_size\": sparsity_bs,\n",
    "        })\n",
    "        sparsity_summaries.append(bench_summary)\n",
    "        sparsity_details.append(\n",
    "            bench_df.assign(variant=f\"Sparsity::{mode}\", mode=mode, keep_ratio=ratio, batch_size=sparsity_bs)\n",
    "        )\n",
    "\n",
    "sparsity_summary_df = pd.DataFrame(sparsity_summaries).sort_values(\"samples_per_s\", ascending=False).reset_index(drop=True)\n",
    "sparsity_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d37f9",
   "metadata": {},
   "source": [
    "`sparsity_compare_df` добавляет к тем же сценариям относительные значения относительно эталонного nn.Conv2d (`speedup_*_vs_torch`, `throughput_ratio_vs_torch`, `mem_*_ratio_vs_torch`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7be95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>avg_forward_ms</th>\n",
       "      <th>avg_backward_ms</th>\n",
       "      <th>avg_step_ms</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>max_mem_alloc_mb</th>\n",
       "      <th>max_mem_reserved_mb</th>\n",
       "      <th>variant</th>\n",
       "      <th>mode</th>\n",
       "      <th>keep_ratio</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>speedup_forward_vs_torch</th>\n",
       "      <th>speedup_backward_vs_torch</th>\n",
       "      <th>speedup_step_vs_torch</th>\n",
       "      <th>throughput_ratio_vs_torch</th>\n",
       "      <th>mem_alloc_ratio_vs_torch</th>\n",
       "      <th>mem_reserved_ratio_vs_torch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Channel sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>58.553237</td>\n",
       "      <td>27.390333</td>\n",
       "      <td>85.943570</td>\n",
       "      <td>1495.724046</td>\n",
       "      <td>281.906738</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0.148680</td>\n",
       "      <td>0.317175</td>\n",
       "      <td>0.202380</td>\n",
       "      <td>0.197445</td>\n",
       "      <td>1.377724</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Block sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>58.612525</td>\n",
       "      <td>27.804467</td>\n",
       "      <td>86.416992</td>\n",
       "      <td>1494.505578</td>\n",
       "      <td>277.916504</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.148530</td>\n",
       "      <td>0.312451</td>\n",
       "      <td>0.201271</td>\n",
       "      <td>0.197285</td>\n",
       "      <td>1.358223</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Channel sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>58.730010</td>\n",
       "      <td>28.239315</td>\n",
       "      <td>86.969325</td>\n",
       "      <td>1479.867645</td>\n",
       "      <td>280.566895</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.148233</td>\n",
       "      <td>0.307639</td>\n",
       "      <td>0.199993</td>\n",
       "      <td>0.195352</td>\n",
       "      <td>1.371176</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Block sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>60.174384</td>\n",
       "      <td>27.837381</td>\n",
       "      <td>88.011765</td>\n",
       "      <td>1464.490586</td>\n",
       "      <td>281.906738</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0.144675</td>\n",
       "      <td>0.312081</td>\n",
       "      <td>0.197624</td>\n",
       "      <td>0.193322</td>\n",
       "      <td>1.377724</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Channel sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>62.127724</td>\n",
       "      <td>27.842911</td>\n",
       "      <td>89.970635</td>\n",
       "      <td>1441.579470</td>\n",
       "      <td>277.916504</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.140126</td>\n",
       "      <td>0.312019</td>\n",
       "      <td>0.193321</td>\n",
       "      <td>0.190298</td>\n",
       "      <td>1.358223</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Channel sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>62.129142</td>\n",
       "      <td>27.794782</td>\n",
       "      <td>89.923925</td>\n",
       "      <td>1435.233769</td>\n",
       "      <td>276.178711</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.140123</td>\n",
       "      <td>0.312560</td>\n",
       "      <td>0.193422</td>\n",
       "      <td>0.189460</td>\n",
       "      <td>1.349730</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Input sparsity (keep=0.50, bs=128)</td>\n",
       "      <td>60.846281</td>\n",
       "      <td>29.797142</td>\n",
       "      <td>90.643423</td>\n",
       "      <td>1420.362348</td>\n",
       "      <td>257.366699</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.143077</td>\n",
       "      <td>0.291556</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.187497</td>\n",
       "      <td>1.257793</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Block sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>61.405683</td>\n",
       "      <td>29.465658</td>\n",
       "      <td>90.871340</td>\n",
       "      <td>1419.049735</td>\n",
       "      <td>280.562500</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.141774</td>\n",
       "      <td>0.294836</td>\n",
       "      <td>0.191405</td>\n",
       "      <td>0.187324</td>\n",
       "      <td>1.371155</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Input sparsity (keep=0.60, bs=128)</td>\n",
       "      <td>70.232736</td>\n",
       "      <td>28.134634</td>\n",
       "      <td>98.367370</td>\n",
       "      <td>1416.643851</td>\n",
       "      <td>260.319336</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.123955</td>\n",
       "      <td>0.308784</td>\n",
       "      <td>0.176819</td>\n",
       "      <td>0.187006</td>\n",
       "      <td>1.272223</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Input sparsity (keep=0.75, bs=128)</td>\n",
       "      <td>61.056020</td>\n",
       "      <td>31.047008</td>\n",
       "      <td>92.103029</td>\n",
       "      <td>1404.039844</td>\n",
       "      <td>267.901855</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0.142585</td>\n",
       "      <td>0.279819</td>\n",
       "      <td>0.188845</td>\n",
       "      <td>0.185342</td>\n",
       "      <td>1.309280</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Input sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>65.574614</td>\n",
       "      <td>28.594586</td>\n",
       "      <td>94.169200</td>\n",
       "      <td>1367.928489</td>\n",
       "      <td>247.798828</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.132760</td>\n",
       "      <td>0.303817</td>\n",
       "      <td>0.184702</td>\n",
       "      <td>0.180576</td>\n",
       "      <td>1.211033</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Block sparsity (keep=0.25, bs=128)</td>\n",
       "      <td>64.929408</td>\n",
       "      <td>30.507008</td>\n",
       "      <td>95.436416</td>\n",
       "      <td>1353.455830</td>\n",
       "      <td>276.178711</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.134079</td>\n",
       "      <td>0.284772</td>\n",
       "      <td>0.182249</td>\n",
       "      <td>0.178665</td>\n",
       "      <td>1.349730</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   label  avg_forward_ms  avg_backward_ms  \\\n",
       "0   Channel sparsity (keep=0.75, bs=128)       58.553237        27.390333   \n",
       "1     Block sparsity (keep=0.50, bs=128)       58.612525        27.804467   \n",
       "2   Channel sparsity (keep=0.60, bs=128)       58.730010        28.239315   \n",
       "3     Block sparsity (keep=0.75, bs=128)       60.174384        27.837381   \n",
       "4   Channel sparsity (keep=0.50, bs=128)       62.127724        27.842911   \n",
       "5   Channel sparsity (keep=0.25, bs=128)       62.129142        27.794782   \n",
       "6     Input sparsity (keep=0.50, bs=128)       60.846281        29.797142   \n",
       "7     Block sparsity (keep=0.60, bs=128)       61.405683        29.465658   \n",
       "8     Input sparsity (keep=0.60, bs=128)       70.232736        28.134634   \n",
       "9     Input sparsity (keep=0.75, bs=128)       61.056020        31.047008   \n",
       "10    Input sparsity (keep=0.25, bs=128)       65.574614        28.594586   \n",
       "11    Block sparsity (keep=0.25, bs=128)       64.929408        30.507008   \n",
       "\n",
       "    avg_step_ms  samples_per_s  max_mem_alloc_mb  max_mem_reserved_mb  \\\n",
       "0     85.943570    1495.724046        281.906738                602.0   \n",
       "1     86.416992    1494.505578        277.916504                602.0   \n",
       "2     86.969325    1479.867645        280.566895                602.0   \n",
       "3     88.011765    1464.490586        281.906738                602.0   \n",
       "4     89.970635    1441.579470        277.916504                602.0   \n",
       "5     89.923925    1435.233769        276.178711                602.0   \n",
       "6     90.643423    1420.362348        257.366699                602.0   \n",
       "7     90.871340    1419.049735        280.562500                602.0   \n",
       "8     98.367370    1416.643851        260.319336                602.0   \n",
       "9     92.103029    1404.039844        267.901855                602.0   \n",
       "10    94.169200    1367.928489        247.798828                602.0   \n",
       "11    95.436416    1353.455830        276.178711                602.0   \n",
       "\n",
       "              variant     mode  keep_ratio  batch_size  \\\n",
       "0   Sparsity::channel  channel        0.75         128   \n",
       "1     Sparsity::block    block        0.50         128   \n",
       "2   Sparsity::channel  channel        0.60         128   \n",
       "3     Sparsity::block    block        0.75         128   \n",
       "4   Sparsity::channel  channel        0.50         128   \n",
       "5   Sparsity::channel  channel        0.25         128   \n",
       "6     Sparsity::input    input        0.50         128   \n",
       "7     Sparsity::block    block        0.60         128   \n",
       "8     Sparsity::input    input        0.60         128   \n",
       "9     Sparsity::input    input        0.75         128   \n",
       "10    Sparsity::input    input        0.25         128   \n",
       "11    Sparsity::block    block        0.25         128   \n",
       "\n",
       "    speedup_forward_vs_torch  speedup_backward_vs_torch  \\\n",
       "0                   0.148680                   0.317175   \n",
       "1                   0.148530                   0.312451   \n",
       "2                   0.148233                   0.307639   \n",
       "3                   0.144675                   0.312081   \n",
       "4                   0.140126                   0.312019   \n",
       "5                   0.140123                   0.312560   \n",
       "6                   0.143077                   0.291556   \n",
       "7                   0.141774                   0.294836   \n",
       "8                   0.123955                   0.308784   \n",
       "9                   0.142585                   0.279819   \n",
       "10                  0.132760                   0.303817   \n",
       "11                  0.134079                   0.284772   \n",
       "\n",
       "    speedup_step_vs_torch  throughput_ratio_vs_torch  \\\n",
       "0                0.202380                   0.197445   \n",
       "1                0.201271                   0.197285   \n",
       "2                0.199993                   0.195352   \n",
       "3                0.197624                   0.193322   \n",
       "4                0.193321                   0.190298   \n",
       "5                0.193422                   0.189460   \n",
       "6                0.191886                   0.187497   \n",
       "7                0.191405                   0.187324   \n",
       "8                0.176819                   0.187006   \n",
       "9                0.188845                   0.185342   \n",
       "10               0.184702                   0.180576   \n",
       "11               0.182249                   0.178665   \n",
       "\n",
       "    mem_alloc_ratio_vs_torch  mem_reserved_ratio_vs_torch  \n",
       "0                   1.377724                     2.640351  \n",
       "1                   1.358223                     2.640351  \n",
       "2                   1.371176                     2.640351  \n",
       "3                   1.377724                     2.640351  \n",
       "4                   1.358223                     2.640351  \n",
       "5                   1.349730                     2.640351  \n",
       "6                   1.257793                     2.640351  \n",
       "7                   1.371155                     2.640351  \n",
       "8                   1.272223                     2.640351  \n",
       "9                   1.309280                     2.640351  \n",
       "10                  1.211033                     2.640351  \n",
       "11                  1.349730                     2.640351  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_reference = summary_df.loc[(\"nn.Conv2d\", sparsity_bs)]\n",
    "\n",
    "sparsity_compare_df = sparsity_summary_df.copy()\n",
    "sparsity_compare_df[\"speedup_forward_vs_torch\"] = sparsity_reference[\"avg_forward_ms\"] / sparsity_compare_df[\"avg_forward_ms\"]\n",
    "sparsity_compare_df[\"speedup_backward_vs_torch\"] = sparsity_reference[\"avg_backward_ms\"] / sparsity_compare_df[\"avg_backward_ms\"]\n",
    "sparsity_compare_df[\"speedup_step_vs_torch\"] = sparsity_reference[\"avg_step_ms\"] / sparsity_compare_df[\"avg_step_ms\"]\n",
    "sparsity_compare_df[\"throughput_ratio_vs_torch\"] = sparsity_compare_df[\"samples_per_s\"] / sparsity_reference[\"samples_per_s\"]\n",
    "sparsity_compare_df[\"mem_alloc_ratio_vs_torch\"] = sparsity_compare_df[\"max_mem_alloc_mb\"] / sparsity_reference[\"max_mem_alloc_mb\"]\n",
    "sparsity_compare_df[\"mem_reserved_ratio_vs_torch\"] = sparsity_compare_df[\"max_mem_reserved_mb\"] / sparsity_reference[\"max_mem_reserved_mb\"]\n",
    "sparsity_compare_df = sparsity_compare_df.sort_values(\"samples_per_s\", ascending=False).reset_index(drop=True)\n",
    "sparsity_compare_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c83ece",
   "metadata": {},
   "source": [
    "`ranking_df` — упорядоченный рейтинг сценариев спарсификации: показывает `mode`, `keep_ratio`, абсолютный throughput и его отношение к торчу, а также ускорения forward/backward/step и изменение памяти.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba124c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>mode</th>\n",
       "      <th>keep_ratio</th>\n",
       "      <th>samples_per_s</th>\n",
       "      <th>throughput_ratio_vs_torch</th>\n",
       "      <th>speedup_forward_vs_torch</th>\n",
       "      <th>speedup_backward_vs_torch</th>\n",
       "      <th>speedup_step_vs_torch</th>\n",
       "      <th>mem_alloc_ratio_vs_torch</th>\n",
       "      <th>mem_reserved_ratio_vs_torch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1495.724046</td>\n",
       "      <td>0.197445</td>\n",
       "      <td>0.148680</td>\n",
       "      <td>0.317175</td>\n",
       "      <td>0.202380</td>\n",
       "      <td>1.377724</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1494.505578</td>\n",
       "      <td>0.197285</td>\n",
       "      <td>0.148530</td>\n",
       "      <td>0.312451</td>\n",
       "      <td>0.201271</td>\n",
       "      <td>1.358223</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1479.867645</td>\n",
       "      <td>0.195352</td>\n",
       "      <td>0.148233</td>\n",
       "      <td>0.307639</td>\n",
       "      <td>0.199993</td>\n",
       "      <td>1.371176</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1464.490586</td>\n",
       "      <td>0.193322</td>\n",
       "      <td>0.144675</td>\n",
       "      <td>0.312081</td>\n",
       "      <td>0.197624</td>\n",
       "      <td>1.377724</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1441.579470</td>\n",
       "      <td>0.190298</td>\n",
       "      <td>0.140126</td>\n",
       "      <td>0.312019</td>\n",
       "      <td>0.193321</td>\n",
       "      <td>1.358223</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sparsity::channel</td>\n",
       "      <td>channel</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1435.233769</td>\n",
       "      <td>0.189460</td>\n",
       "      <td>0.140123</td>\n",
       "      <td>0.312560</td>\n",
       "      <td>0.193422</td>\n",
       "      <td>1.349730</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1420.362348</td>\n",
       "      <td>0.187497</td>\n",
       "      <td>0.143077</td>\n",
       "      <td>0.291556</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>1.257793</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1419.049735</td>\n",
       "      <td>0.187324</td>\n",
       "      <td>0.141774</td>\n",
       "      <td>0.294836</td>\n",
       "      <td>0.191405</td>\n",
       "      <td>1.371155</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1416.643851</td>\n",
       "      <td>0.187006</td>\n",
       "      <td>0.123955</td>\n",
       "      <td>0.308784</td>\n",
       "      <td>0.176819</td>\n",
       "      <td>1.272223</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1404.039844</td>\n",
       "      <td>0.185342</td>\n",
       "      <td>0.142585</td>\n",
       "      <td>0.279819</td>\n",
       "      <td>0.188845</td>\n",
       "      <td>1.309280</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sparsity::input</td>\n",
       "      <td>input</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1367.928489</td>\n",
       "      <td>0.180576</td>\n",
       "      <td>0.132760</td>\n",
       "      <td>0.303817</td>\n",
       "      <td>0.184702</td>\n",
       "      <td>1.211033</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sparsity::block</td>\n",
       "      <td>block</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1353.455830</td>\n",
       "      <td>0.178665</td>\n",
       "      <td>0.134079</td>\n",
       "      <td>0.284772</td>\n",
       "      <td>0.182249</td>\n",
       "      <td>1.349730</td>\n",
       "      <td>2.640351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              variant     mode  keep_ratio  samples_per_s  \\\n",
       "0   Sparsity::channel  channel        0.75    1495.724046   \n",
       "1     Sparsity::block    block        0.50    1494.505578   \n",
       "2   Sparsity::channel  channel        0.60    1479.867645   \n",
       "3     Sparsity::block    block        0.75    1464.490586   \n",
       "4   Sparsity::channel  channel        0.50    1441.579470   \n",
       "5   Sparsity::channel  channel        0.25    1435.233769   \n",
       "6     Sparsity::input    input        0.50    1420.362348   \n",
       "7     Sparsity::block    block        0.60    1419.049735   \n",
       "8     Sparsity::input    input        0.60    1416.643851   \n",
       "9     Sparsity::input    input        0.75    1404.039844   \n",
       "10    Sparsity::input    input        0.25    1367.928489   \n",
       "11    Sparsity::block    block        0.25    1353.455830   \n",
       "\n",
       "    throughput_ratio_vs_torch  speedup_forward_vs_torch  \\\n",
       "0                    0.197445                  0.148680   \n",
       "1                    0.197285                  0.148530   \n",
       "2                    0.195352                  0.148233   \n",
       "3                    0.193322                  0.144675   \n",
       "4                    0.190298                  0.140126   \n",
       "5                    0.189460                  0.140123   \n",
       "6                    0.187497                  0.143077   \n",
       "7                    0.187324                  0.141774   \n",
       "8                    0.187006                  0.123955   \n",
       "9                    0.185342                  0.142585   \n",
       "10                   0.180576                  0.132760   \n",
       "11                   0.178665                  0.134079   \n",
       "\n",
       "    speedup_backward_vs_torch  speedup_step_vs_torch  \\\n",
       "0                    0.317175               0.202380   \n",
       "1                    0.312451               0.201271   \n",
       "2                    0.307639               0.199993   \n",
       "3                    0.312081               0.197624   \n",
       "4                    0.312019               0.193321   \n",
       "5                    0.312560               0.193422   \n",
       "6                    0.291556               0.191886   \n",
       "7                    0.294836               0.191405   \n",
       "8                    0.308784               0.176819   \n",
       "9                    0.279819               0.188845   \n",
       "10                   0.303817               0.184702   \n",
       "11                   0.284772               0.182249   \n",
       "\n",
       "    mem_alloc_ratio_vs_torch  mem_reserved_ratio_vs_torch  \n",
       "0                   1.377724                     2.640351  \n",
       "1                   1.358223                     2.640351  \n",
       "2                   1.371176                     2.640351  \n",
       "3                   1.377724                     2.640351  \n",
       "4                   1.358223                     2.640351  \n",
       "5                   1.349730                     2.640351  \n",
       "6                   1.257793                     2.640351  \n",
       "7                   1.371155                     2.640351  \n",
       "8                   1.272223                     2.640351  \n",
       "9                   1.309280                     2.640351  \n",
       "10                  1.211033                     2.640351  \n",
       "11                  1.349730                     2.640351  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_df = sparsity_compare_df[[\n",
    "    \"variant\",\n",
    "    \"mode\",\n",
    "    \"keep_ratio\",\n",
    "    \"samples_per_s\",\n",
    "    \"throughput_ratio_vs_torch\",\n",
    "    \"speedup_forward_vs_torch\",\n",
    "    \"speedup_backward_vs_torch\",\n",
    "    \"speedup_step_vs_torch\",\n",
    "    \"mem_alloc_ratio_vs_torch\",\n",
    "    \"mem_reserved_ratio_vs_torch\",\n",
    "]].copy()\n",
    "ranking_df = ranking_df.sort_values(\"throughput_ratio_vs_torch\", ascending=False).reset_index(drop=True)\n",
    "ranking_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
