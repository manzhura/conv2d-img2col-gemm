{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a172295f-0e2d-4b2a-8e7f-4cb1a9f04edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_gemm.layers.triton_conv2d import TritonConv2d \n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cadf2671-56f1-45e8-be24-7b20c12bb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_configs():\n",
    "    device = \"cuda\"\n",
    "\n",
    "    # === РАЗМЕРЫ ПОД ТВОЮ СЕТКУ ===\n",
    "    B = 8\n",
    "    Cin = 64\n",
    "    Cout = 128\n",
    "    H = 256\n",
    "    W = 256\n",
    "    ks = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    dilation = 1\n",
    "\n",
    "    x = torch.randn(B, Cin, H, W, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Набор конфигов для перебора\n",
    "    configs = [\n",
    "        dict(BLOCK_M=64, BLOCK_N=64, BLOCK_K=32, NUM_WARPS=4, NUM_STAGES=2),\n",
    "        dict(BLOCK_M=64, BLOCK_N=32, BLOCK_K=32, NUM_WARPS=4, NUM_STAGES=2),\n",
    "        dict(BLOCK_M=32, BLOCK_N=64, BLOCK_K=32, NUM_WARPS=4, NUM_STAGES=2),\n",
    "        dict(BLOCK_M=32, BLOCK_N=32, BLOCK_K=32, NUM_WARPS=4, NUM_STAGES=2),\n",
    "\n",
    "        dict(BLOCK_M=64, BLOCK_N=64, BLOCK_K=16, NUM_WARPS=4, NUM_STAGES=2),\n",
    "        dict(BLOCK_M=64, BLOCK_N=32, BLOCK_K=16, NUM_WARPS=4, NUM_STAGES=2),\n",
    "        dict(BLOCK_M=32, BLOCK_N=64, BLOCK_K=16, NUM_WARPS=4, NUM_STAGES=2),\n",
    "\n",
    "        dict(BLOCK_M=64, BLOCK_N=64, BLOCK_K=32, NUM_WARPS=4, NUM_STAGES=1),\n",
    "        dict(BLOCK_M=64, BLOCK_N=64, BLOCK_K=16, NUM_WARPS=4, NUM_STAGES=1),\n",
    "\n",
    "        dict(BLOCK_M=64, BLOCK_N=64, BLOCK_K=32, NUM_WARPS=2, NUM_STAGES=2),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # глобальная синхра перед всеми замерами\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    for cfg in configs:\n",
    "        print(\"\\n=== Testing config:\", cfg, \"===\")\n",
    "        try:\n",
    "            conv = TritonConv2d(\n",
    "                in_channels=Cin,\n",
    "                out_channels=Cout,\n",
    "                kernel_size=ks,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "                BLOCK_M=cfg[\"BLOCK_M\"],\n",
    "                BLOCK_N=cfg[\"BLOCK_N\"],\n",
    "                BLOCK_K=cfg[\"BLOCK_K\"],\n",
    "                NUM_WARPS=cfg[\"NUM_WARPS\"],\n",
    "                NUM_STAGES=cfg[\"NUM_STAGES\"],\n",
    "                precision_mode=\"fp16_runtime\",  # можно \"fp16\" для начала\n",
    "            ).to(device)\n",
    "\n",
    "            conv.train()\n",
    "            opt = torch.optim.SGD(conv.parameters(), lr=1e-3)\n",
    "\n",
    "            # прогрев 2 итерации (без тайминга)\n",
    "            for _ in range(2):\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                out = conv(x)\n",
    "                loss = out.float().pow(2).mean()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # измеряем несколько раз, усредняем\n",
    "            iters = 10\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            total_ms = 0.0\n",
    "            last_loss = None\n",
    "\n",
    "            for _ in range(iters):\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                start_event.record()\n",
    "\n",
    "                out = conv(x)\n",
    "                loss = out.float().pow(2).mean()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()  # гарантируем, что итерация закончилась\n",
    "                iter_ms = start_event.elapsed_time(end_event)  # ms\n",
    "                total_ms += iter_ms\n",
    "                last_loss = loss.item()\n",
    "\n",
    "            avg_ms = total_ms / iters\n",
    "            print(f\"OK, avg_time = {avg_ms:.3f} ms, loss={last_loss:.4f}\")\n",
    "            results.append((avg_ms / 1000.0, cfg, \"OK\"))  # храним в секундах\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            if \"OutOfResources\" in msg or \"out of resource: shared memory\" in msg:\n",
    "                print(\"!! OutOfResources / shared memory for config:\", cfg)\n",
    "                results.append((None, cfg, \"OOR\"))\n",
    "            else:\n",
    "                print(\"!! RuntimeError other:\", msg)\n",
    "                results.append((None, cfg, \"ERR\"))\n",
    "\n",
    "    print(\"\\n\\n========= SUMMARY =========\")\n",
    "    for dt, cfg, status in sorted(results, key=lambda x: (x[0] if x[0] is not None else 1e9)):\n",
    "        t_str = f\"{dt*1000:.3f} ms\" if dt is not None else \"   -   \"\n",
    "        print(status, \"|\", t_str, \"|\", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069bdfd7-bb0f-4488-9aa6-6565a1837fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 307.00 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.43 GiB memory in use. Of the allocated memory 1.19 GiB is allocated by PyTorch, and 65.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 295.31 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.43 GiB memory in use. Of the allocated memory 1.06 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 199.31 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 264.44 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 272.94 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 224.06 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 304.56 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 1} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 126.06 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 1} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 304.38 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== Testing config: {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 2, 'NUM_STAGES': 2} ===\n",
      "!! RuntimeError other: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 74.62 MiB is free. Process 41506 has 4.70 GiB memory in use. Including non-PyTorch memory, this process has 1.68 GiB memory in use. Of the allocated memory 1.31 GiB is allocated by PyTorch, and 193.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "========= SUMMARY =========\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 2}\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 4, 'NUM_STAGES': 1}\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16, 'NUM_WARPS': 4, 'NUM_STAGES': 1}\n",
      "ERR |    -    | {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'NUM_WARPS': 2, 'NUM_STAGES': 2}\n"
     ]
    }
   ],
   "source": [
    "test_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262e585-c80d-4a67-b5a2-b9107859a972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
