{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e469698a-c56b-4f18-a64e-cf5a4e0654f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\".\"))  # вставляем в начало\n",
    "from conv_gemm.layers.triton_conv2d import TritonConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2360130-d422-433f-943b-8e37d1c7d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def bench(fn, warmup=50, iters=500):\n",
    "    \"\"\"Простой CUDA benchmark: среднее время итерации в секундах.\"\"\"\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter() - t0) / iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307671e7-5d1a-44f0-9b75-5e1b1d630a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_forward_backward_with_precision(cfg, precision_mode: str):\n",
    "    device = \"cuda\"\n",
    "    B, Cin, Cout, H, W, ks = cfg[\"B\"], cfg[\"Cin\"], cfg[\"Cout\"], cfg[\"H\"], cfg[\"W\"], cfg[\"ks\"]\n",
    "    stride, padding, dilation = cfg[\"stride\"], cfg[\"padding\"], cfg[\"dilation\"]\n",
    "    dtype = cfg[\"dtype\"]                      # у тебя уже fp16\n",
    "    use_bias = cfg.get(\"use_bias\", True)\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # вход — half (чтобы имело смысл гонять img2col в 2 байта/элемент)\n",
    "    x = torch.randn(B, Cin, H, W, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "    # Triton-модель\n",
    "    m_tri = TritonConv2d(\n",
    "        in_channels=Cin, out_channels=Cout, kernel_size=ks,\n",
    "        stride=stride, padding=padding, dilation=dilation, bias=use_bias,\n",
    "        BLOCK_M=cfg.get(\"BLOCK_M\",64), BLOCK_N=cfg.get(\"BLOCK_N\",64), BLOCK_K=cfg.get(\"BLOCK_K\",32),\n",
    "        NUM_WARPS=cfg.get(\"NUM_WARPS\",4), NUM_STAGES=cfg.get(\"NUM_STAGES\",2),\n",
    "        precision_mode=precision_mode\n",
    "    ).to(device)\n",
    "\n",
    "    # Эталон: Conv2d\n",
    "    m_ref = torch.nn.Conv2d(Cin, Cout, ks, stride=stride, padding=padding,\n",
    "                            dilation=dilation, bias=use_bias).to(device)\n",
    "\n",
    "    # Синхронизируем начальные веса/биас\n",
    "    with torch.no_grad():\n",
    "        m_ref.weight.copy_(m_tri.weight.float())   # у ref всё в fp16\n",
    "        if use_bias: m_ref.bias.copy_( (m_tri.bias.float() if m_tri.bias is not None else torch.zeros_like(m_ref.bias)) )\n",
    "\n",
    "    # Применяем precision режим к Triton-слою (переводит хранение/тени)\n",
    "    m_tri.set_precision(precision_mode)\n",
    "\n",
    "    # Для корректного сравнения: прогоняем ref в том же «эффективном» dtype входа\n",
    "    # y_ref всегда считаем в fp16 (как torch делает по умолчанию),\n",
    "    # а для сравнения кастаем в float, как и раньше.\n",
    "    with torch.no_grad():\n",
    "        y_tri = m_tri(x)          # Triton: в зависимости от режима считает в fp16/fp16 с acc fp16\n",
    "        y_ref = m_ref(x.float())  # Torch: считает в fp16 (стабильная эталонная база)\n",
    "        mae = (y_ref.float() - y_tri.float()).abs().mean().item()\n",
    "        mx  = (y_ref.float() - y_tri.float()).abs().max().item()\n",
    "        print(f\"[{precision_mode}][FWD] MAE={mae:.6e} | max|diff|={mx:.3e}\")\n",
    "\n",
    "    # ---- Backward сравнение (общий upstream-градиент) ----\n",
    "    gy = torch.randn_like(y_ref, dtype=torch.float32)\n",
    "\n",
    "    # ref grads\n",
    "    for p in m_ref.parameters():\n",
    "        if p.grad is not None: p.grad.zero_()\n",
    "    if x.grad is not None: x.grad.zero_()\n",
    "    y = m_ref(x.float())\n",
    "    (y.float() * gy).sum().backward()\n",
    "    dx_ref = x.grad.detach().float().clone()\n",
    "    dw_ref = m_ref.weight.grad.detach().float().clone()\n",
    "    db_ref = m_ref.bias.grad.detach().float().clone() if use_bias else None\n",
    "\n",
    "    # triton grads\n",
    "    if x.grad is not None: x.grad.zero_()\n",
    "    for p in m_tri.parameters():\n",
    "        if p.grad is not None: p.grad.zero_()\n",
    "    y = m_tri(x)                                  # внутри bwd у нас всё в fp16 — стабильно\n",
    "    (y.float() * gy).sum().backward()\n",
    "    dx_tri = x.grad.detach().float().clone()\n",
    "    dw_tri = m_tri.weight.grad.detach().float().clone()\n",
    "    db_tri = m_tri.bias.grad.detach().float().clone() if use_bias else None\n",
    "\n",
    "    def stats(name, a, b):\n",
    "        d = (a - b).abs()\n",
    "        mae = d.mean().item(); mx = d.max().item()\n",
    "        rel = d.norm().item() / max(a.norm().item(), 1e-12)\n",
    "        print(f\"[{precision_mode}][BWD:{name}] MAE={mae:.6e} | max|diff|={mx:.3e} | relL2={rel:.3e}\")\n",
    "\n",
    "    stats(\"dx\", dx_ref, dx_tri)\n",
    "    stats(\"dw\", dw_ref, dw_tri)\n",
    "    if use_bias: stats(\"db\", db_ref, db_tri)\n",
    "\n",
    "    # ---- Тайминг end-to-end ----\n",
    "    warmup = cfg.get(\"warmup\", 50); iters = cfg.get(\"iters\", 500)\n",
    "    x_b = torch.randn_like(x, requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        m_ref.weight.copy_(m_tri.weight.float())\n",
    "        if use_bias and (m_tri.bias is not None):\n",
    "            m_ref.bias.copy_(m_tri.bias.float())\n",
    "    gy_b = torch.randn_like(m_ref(x_b.float()), dtype=torch.float32)\n",
    "\n",
    "    def f_ref():\n",
    "        if x_b.grad is not None: x_b.grad = None\n",
    "        for p in m_ref.parameters():\n",
    "            if p.grad is not None: p.grad = None\n",
    "        y = m_ref(x_b.float())\n",
    "        (y.float() * gy_b).sum().backward()\n",
    "\n",
    "    def f_tri():\n",
    "        if x_b.grad is not None: x_b.grad = None\n",
    "        for p in m_tri.parameters():\n",
    "            if p.grad is not None: p.grad = None\n",
    "        y = m_tri(x_b)\n",
    "        (y.float() * gy_b).sum().backward()\n",
    "\n",
    "    t_ref = bench(f_ref, warmup=warmup, iters=iters)\n",
    "    t_tri = bench(f_tri, warmup=warmup, iters=iters)\n",
    "    print(f\"[{precision_mode}][TIME] Torch: {t_ref*1e3:.3f} ms/it | Triton: {t_tri*1e3:.3f} ms/it | speedup={t_ref/max(t_tri,1e-12):.3f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c18667-3de6-42e4-b8c4-133935849e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CASE: B=2 Cin=64 Cout=128 H=32 W=32 ks=5 dtype=torch.float16 bias=True ===\n",
      "[fp16_runtime][FWD] MAE=1.300658e-04 | max|diff|=1.188e-03\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cfg \u001B[38;5;129;01min\u001B[39;00m RUNS:\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== CASE:\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     14\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Cin=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCin\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Cout=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCout\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m H=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mH\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m W=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ks=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mks\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     15\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m bias=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_bias\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mcheck_forward_backward_with_precision\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfp16_runtime\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# W-FP32 (мастер), compute в FP16\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     check_forward_backward_with_precision(cfg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfp16_infer\u001B[39m\u001B[38;5;124m\"\u001B[39m)    \u001B[38;5;66;03m# W-FP16 (хранение и compute в FP16)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[3], line 64\u001B[0m, in \u001B[0;36mcheck_forward_backward_with_precision\u001B[0;34m(cfg, precision_mode)\u001B[0m\n\u001B[1;32m     62\u001B[0m (y\u001B[38;5;241m.\u001B[39mfloat() \u001B[38;5;241m*\u001B[39m gy)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     63\u001B[0m dx_tri \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mclone()\n\u001B[0;32m---> 64\u001B[0m dw_tri \u001B[38;5;241m=\u001B[39m \u001B[43mm_tri\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m()\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m     65\u001B[0m db_tri \u001B[38;5;241m=\u001B[39m m_tri\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mclone() \u001B[38;5;28;01mif\u001B[39;00m use_bias \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstats\u001B[39m(name, a, b):\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "\n",
    "RUNS = [\n",
    "        dict(B=2, Cin=64, Cout=128, H=32, W=32, ks=5,\n",
    "             stride=1, padding=1, dilation=1,\n",
    "             dtype=torch.float16, use_bias=True, warmup=50, iters=500,\n",
    "             BLOCK_M=64, BLOCK_N=64, BLOCK_K=32),\n",
    "\n",
    "        dict(B=2, Cin=64, Cout=128, H=64, W=64, ks=5,\n",
    "             stride=1, padding=1, dilation=1,\n",
    "             dtype=torch.float16, use_bias=False, warmup=50, iters=500,\n",
    "             BLOCK_M=64, BLOCK_N=64, BLOCK_K=32),\n",
    "    ]\n",
    "for cfg in RUNS:\n",
    "    print(\"\\n=== CASE:\",\n",
    "          f\"B={cfg['B']} Cin={cfg['Cin']} Cout={cfg['Cout']} H={cfg['H']} W={cfg['W']} ks={cfg['ks']}\",\n",
    "          f\"dtype={cfg['dtype']} bias={cfg['use_bias']} ===\")\n",
    "    check_forward_backward_with_precision(cfg, \"fp16_runtime\")  # W-FP32 (мастер), compute в FP16\n",
    "    check_forward_backward_with_precision(cfg, \"fp16_infer\")    # W-FP16 (хранение и compute в FP16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315b10e-e65f-42dc-b80e-a22a6e43862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch, triton\n",
    "from torch import nn\n",
    "from conv_gemm.triton_kernels.fp16.img2col_kernel import img2col_kernel as i2c_k\n",
    "from conv_gemm.triton_kernels.fp16.gemm_kernel    import gemm_kernel    as gemm_k\n",
    "from conv_gemm.triton_kernels.fp16.col2img_kernel import col2img_kernel as c2i_k\n",
    "\n",
    "def sync(): torch.cuda.synchronize()\n",
    "\n",
    "def bench(fn, warmup=50, iters=300):\n",
    "    for _ in range(warmup): fn()\n",
    "    sync()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters): fn()\n",
    "    sync()\n",
    "    return (time.perf_counter() - t0) / iters\n",
    "\n",
    "def out_hw(H, W, Kh, Kw, Sh, Sw, Ph, Pw, Dh, Dw):\n",
    "    Ho = (H + 2*Ph - Dh*(Kh - 1) - 1) // Sh + 1\n",
    "    Wo = (W + 2*Pw - Dw*(Kw - 1) - 1) // Sw + 1\n",
    "    return Ho, Wo\n",
    "\n",
    "def triton_img2col(x, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo, BLOCK_M=64, BLOCK_K=32, fp16=False):\n",
    "    K = Cin*Kh*Kw\n",
    "    cols = torch.empty((B*Ho*Wo, K), device=x.device, dtype=(torch.float16 if fp16 else torch.float32))\n",
    "    sN,sC,sH,sW = x.stride()\n",
    "    grid = (triton.cdiv(B*Ho*Wo, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "    i2c_k[grid](\n",
    "        x, cols, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "        sN,sC,sH,sW, K,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_K=BLOCK_K,\n",
    "        CAST_FP16=fp16, num_warps=4, num_stages=2,\n",
    "    )\n",
    "    return cols\n",
    "\n",
    "def triton_gemm(A, B, M, N, K, BLOCK_M=64, BLOCK_N=64, BLOCK_K=32, use_fp16=False):\n",
    "    C = torch.empty((M,N), device=A.device, dtype=torch.float32)  # acc fp16\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "    gemm_k[grid](\n",
    "        A, B, C, M,N,K,\n",
    "        K,1,  N,1,  N,1,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "        USE_FP16=use_fp16, num_warps=4, num_stages=2,\n",
    "    )\n",
    "    return C\n",
    "\n",
    "def triton_col2im(cols, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo, BLOCK_M=64, BLOCK_K=32):\n",
    "    K = Cin*Kh*Kw; M = B*Ho*Wo\n",
    "    x = torch.zeros((B,Cin,H,W), device=cols.device, dtype=torch.float32)\n",
    "    sN,sC,sH,sW = x.stride()\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "    c2i_k[grid](\n",
    "        cols, x, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "        sN,sC,sH,sW, K,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_K=BLOCK_K,\n",
    "        num_warps=4, num_stages=2,\n",
    "    )\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686d47e-f113-4948-9310-91d2a716b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_stages(cfg, precision_mode: str):\n",
    "    device = \"cuda\"\n",
    "    B, Cin, Cout, H, W, ks = cfg[\"B\"], cfg[\"Cin\"], cfg[\"Cout\"], cfg[\"H\"], cfg[\"W\"], cfg[\"ks\"]\n",
    "    stride, padding, dilation = cfg[\"stride\"], cfg[\"padding\"], cfg[\"dilation\"]\n",
    "    Sh,Sw = (stride, stride) if isinstance(stride,int) else stride\n",
    "    Ph,Pw = (padding, padding) if isinstance(padding,int) else padding\n",
    "    Dh,Dw = (dilation, dilation) if isinstance(dilation,int) else dilation\n",
    "    Kh=Kw=ks\n",
    "    Ho, Wo = out_hw(H,W,Kh,Kw,Sh,Sw,Ph,Pw,Dh,Dw)\n",
    "    L = Ho*Wo; M = B*L; K = Cin*Kh*Kw\n",
    "\n",
    "    dtype_in = cfg[\"dtype\"]\n",
    "    x_fp = torch.randn(B, Cin, H, W, device=device, dtype=dtype_in, requires_grad=True)\n",
    "    W_full = torch.randn(Cout, Cin, Kh, Kw, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Torch stage ops\n",
    "    unfold = nn.Unfold(kernel_size=(Kh,Kw), dilation=(Dh,Dw), padding=(Ph,Pw), stride=(Sh,Sw)).to(device)\n",
    "    fold   = nn.Fold(output_size=(H,W), kernel_size=(Kh,Kw), dilation=(Dh,Dw), padding=(Ph,Pw), stride=(Sh,Sw)).to(device)\n",
    "    W_mat32 = W_full.view(Cout,-1).t().contiguous()  # [K, Cout]\n",
    "\n",
    "    # Triton dtypes\n",
    "    fp16_mode = (precision_mode in (\"fp16_runtime\",\"fp16_infer\"))\n",
    "    x_tri = (x_fp.half() if fp16_mode else x_fp.float()).detach().requires_grad_(True)\n",
    "    W_tri = (W_full.half() if precision_mode==\"fp16_infer\" else W_full.float())\n",
    "\n",
    "    # --- FWD: Torch stages ---\n",
    "    def f_unfold(): unfold(x_fp.float())\n",
    "    def f_gemm_torch():\n",
    "        cols_ref = unfold(x_fp.float()).transpose(1,2).contiguous().view(-1,K).float()\n",
    "        return cols_ref @ W_mat32\n",
    "    def f_fold():\n",
    "        cols_ref = unfold(x_fp.float()).transpose(1,2).contiguous().view(-1,K).float()\n",
    "        y_mat = cols_ref @ W_mat32\n",
    "        return y_mat.view(B,L,Cout).transpose(1,2).contiguous()\n",
    "\n",
    "    t_unfold = bench(f_unfold, warmup=30, iters=2000)\n",
    "    t_gemm_t = bench(f_gemm_torch, warmup=30, iters=2000)\n",
    "    t_fold   = bench(f_fold, warmup=30, iters=2000)\n",
    "\n",
    "    # --- FWD: Triton stages ---\n",
    "    def f_i2c():\n",
    "        triton_img2col((x_tri.half() if fp16_mode else x_tri.float()),\n",
    "                       B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                       BLOCK_M=cfg.get(\"BLOCK_M\",64), BLOCK_K=cfg.get(\"BLOCK_K\",32), fp16=fp16_mode)\n",
    "    def f_gemm_tri():\n",
    "        cols = triton_img2col((x_tri.half() if fp16_mode else x_tri.float()),\n",
    "                              B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                              BLOCK_M=cfg.get(\"BLOCK_M\",64), BLOCK_K=cfg.get(\"BLOCK_K\",32), fp16=fp16_mode)\n",
    "        W_mat = W_tri.view(Cout,-1).t().contiguous()\n",
    "        if fp16_mode and W_mat.dtype != torch.float16: W_mat = W_mat.half()\n",
    "        triton_gemm(cols, W_mat, M, Cout, K,\n",
    "                    BLOCK_M=cfg.get(\"BLOCK_M\",64), BLOCK_N=cfg.get(\"BLOCK_N\",64), BLOCK_K=cfg.get(\"BLOCK_K\",32),\n",
    "                    use_fp16=fp16_mode)\n",
    "    def f_c2i():\n",
    "        cols = triton_img2col((x_tri.half() if fp16_mode else x_tri.float()),\n",
    "                              B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                              BLOCK_M=cfg.get(\"BLOCK_M\",64), BLOCK_K=cfg.get(\"BLOCK_K\",32), fp16=fp16_mode)\n",
    "        triton_col2im(cols.float(), B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                      BLOCK_M=cfg.get(\"BLOCK_M\",64), BLOCK_K=cfg.get(\"BLOCK_K\",32))\n",
    "\n",
    "    t_i2c  = bench(f_i2c,  warmup=30, iters=2000)\n",
    "    t_gemm = bench(f_gemm_tri, warmup=30, iters=2000)\n",
    "    t_c2i  = bench(f_c2i,  warmup=30, iters=2000)\n",
    "\n",
    "    print(f\"\\n[STAGES {precision_mode}] (ms/it)\")\n",
    "    print(f\"Torch   Unfold: {t_unfold*1e3:.3f} | GEMM@: {t_gemm_t*1e3:.3f} | Fold:  {t_fold*1e3:.3f}\")\n",
    "    print(f\"Triton  i2c:    {t_i2c*1e3:.3f} | GEMM:   {t_gemm*1e3:.3f} | c2i:   {t_c2i*1e3:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d9b89-8199-4dcf-aa75-f368944c6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_stages(cfg, \"fp16\")\n",
    "profile_stages(cfg, \"fp16_runtime\")\n",
    "profile_stages(cfg, \"fp16_infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda4ea1a-af14-4f93-8257-7ecf4036b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Глава 1: ImageToColumn (Unfold vs Triton img2col)\n",
      "\n",
      "[img2col:fp32]\n",
      "| Этап | FWD MAE | FWD max | BWD MAE(dx) | BWD max(dx) | Torch Unfold ms | Triton i2c ms | Speedup |\n",
      "| Знач | 0.000e+00 | 0.000e+00 | 4.188e-07 | 5.722e-06 | 0.128 | 0.140 | 0.91x |\n",
      "\n",
      "[img2col:fp16]\n",
      "| Этап | FWD MAE | FWD max | BWD MAE(dx) | BWD max(dx) | Torch Unfold ms | Triton i2c ms | Speedup |\n",
      "| Знач | 0.000e+00 | 0.000e+00 | 4.186e-07 | 7.629e-06 | 0.119 | 0.108 | 1.11x |\n",
      "\n",
      "# Глава 2: GEMM (torch @ vs Triton gemm)\n",
      "\n",
      "[GEMM:fp32]\n",
      "| Этап | FWD MAE | FWD max | dA MAE | dA max | dB MAE | dB max | Torch mm ms | Triton GEMM ms | Speedup |\n",
      "| Знач | 9.366e-03 | 6.069e-02 | 2.635e-03 | 2.032e-02 | 2.057e-02 | 1.216e-01 | 0.231 | 0.319 | 0.73x |\n",
      "\n",
      "[GEMM:fp16]\n",
      "| Этап | FWD MAE | FWD max | dA MAE | dA max | dB MAE | dB max | Torch mm ms | Triton GEMM ms | Speedup |\n",
      "| Знач | 7.932e-05 | 5.951e-04 | 1.866e-03 | 1.331e-02 | 1.455e-02 | 8.316e-02 | 0.251 | 0.111 | 2.26x |\n",
      "\n",
      "# Глава 3: ColumnToImage (Fold vs Triton col2im)\n",
      "\n",
      "[col2im:fp32]\n",
      "| Этап | FWD MAE | FWD max | BWD MAE(dcols) | BWD max(dcols) | Torch Fold ms | Triton c2i ms | Speedup |\n",
      "| Знач | 4.205e-07 | 5.722e-06 | 0.000e+00 | 0.000e+00 | 0.794 | 0.234 | 3.39x |\n",
      "\n",
      "[col2im:fp16]\n",
      "| Этап | FWD MAE | FWD max | BWD MAE(dcols) | BWD max(dcols) | Torch Fold ms | Triton c2i ms | Speedup |\n",
      "| Знач | 4.194e-07 | 5.722e-06 | 0.000e+00 | 0.000e+00 | 0.742 | 0.251 | 2.96x |\n"
     ]
    }
   ],
   "source": [
    "# tests/bench_units_img2col_gemm_col2im.py\n",
    "import time, torch, triton\n",
    "from torch import nn\n",
    "\n",
    "from conv_gemm.triton_kernels.fp16.img2col_kernel import img2col_kernel as i2c_k\n",
    "from conv_gemm.triton_kernels.fp16.col2img_kernel import col2img_kernel as c2i_k\n",
    "from conv_gemm.triton_kernels.fp16.gemm_kernel    import triton_gemm\n",
    "\n",
    "def _force_strict_fp32():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    torch.backends.cudnn.allow_tf32 = False\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "_force_strict_fp32()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = \"cuda\"\n",
    "def sync(): torch.cuda.synchronize()\n",
    "\n",
    "def bench(fn, warmup=50, iters=300):\n",
    "    for _ in range(warmup): fn()\n",
    "    sync()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters): fn()\n",
    "    sync()\n",
    "    return (time.perf_counter() - t0) / iters\n",
    "\n",
    "def out_hw(H, W, Kh, Kw, Sh, Sw, Ph, Pw, Dh, Dw):\n",
    "    Ho = (H + 2*Ph - Dh*(Kh - 1) - 1) // Sh + 1\n",
    "    Wo = (W + 2*Pw - Dw*(Kw - 1) - 1) // Sw + 1\n",
    "    return Ho, Wo\n",
    "\n",
    "def triton_img2col(x, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                   BLOCK_M=64, BLOCK_K=32, fp16=False):\n",
    "    K = Cin*Kh*Kw\n",
    "    cols_dtype = torch.float16 if fp16 else torch.float32\n",
    "    cols = torch.empty((B*Ho*Wo, K), device=x.device, dtype=cols_dtype)\n",
    "    sN,sC,sH,sW = x.stride()\n",
    "    grid = (triton.cdiv(B*Ho*Wo, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "    i2c_k[grid](\n",
    "        x, cols,\n",
    "        B,Cin,H,W,\n",
    "        Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw,\n",
    "        Ho,Wo,\n",
    "        sN,sC,sH,sW,\n",
    "        K,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_K=BLOCK_K,\n",
    "        CAST_FP16=fp16,\n",
    "        num_warps=4, num_stages=2\n",
    "    )\n",
    "    return cols\n",
    "\n",
    "def triton_col2im(cols, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                  BLOCK_M=64, BLOCK_K=32):\n",
    "    K = Cin*Kh*Kw; M = B*Ho*Wo\n",
    "    x = torch.zeros((B,Cin,H,W), device=cols.device, dtype=torch.float32)\n",
    "    sN,sC,sH,sW = x.stride()\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "    c2i_k[grid](\n",
    "        cols, x,\n",
    "        B,Cin,H,W,\n",
    "        Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw,\n",
    "        Ho,Wo,\n",
    "        sN,sC,sH,sW,\n",
    "        K,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_K=BLOCK_K,\n",
    "        num_warps=4, num_stages=2\n",
    "    )\n",
    "    return x\n",
    "\n",
    "def row(name, items):\n",
    "    print(\"| \" + name + \" | \" + \" | \".join(items) + \" |\")\n",
    "\n",
    "# ===== ГЛАВА 1: ImageToColumn =====\n",
    "def chapter_img2col(cfg):\n",
    "    print(\"\\n# Глава 1: ImageToColumn (Unfold vs Triton img2col)\")\n",
    "    B,Cin,Cout,H,W,ks = cfg[\"B\"],cfg[\"Cin\"],cfg[\"Cout\"],cfg[\"H\"],cfg[\"W\"],cfg[\"ks\"]\n",
    "    stride,padding,dilation = cfg[\"stride\"],cfg[\"padding\"],cfg[\"dilation\"]\n",
    "    Sh,Sw = (stride,stride) if isinstance(stride,int) else stride\n",
    "    Ph,Pw = (padding,padding) if isinstance(padding,int) else padding\n",
    "    Dh,Dw = (dilation,dilation) if isinstance(dilation,int) else dilation\n",
    "    Kh=Kw=ks\n",
    "    Ho,Wo = out_hw(H,W,Kh,Kw,Sh,Sw,Ph,Pw,Dh,Dw)\n",
    "    K = Cin*Kh*Kw\n",
    "\n",
    "    unfold = nn.Unfold((Kh,Kw), dilation=(Dh,Dw), padding=(Ph,Pw), stride=(Sh,Sw)).to(device)\n",
    "\n",
    "    for prec in (\"fp16\",\"fp16\"):\n",
    "        fp16 = (prec==\"fp16\")\n",
    "        dtype = torch.float16 if fp16 else torch.float32\n",
    "\n",
    "        x = torch.randn(B,Cin,H,W, device=device, dtype=dtype)\n",
    "\n",
    "        # Forward acc\n",
    "        with torch.no_grad():\n",
    "            cols_ref = unfold(x.float()).transpose(1,2).contiguous().view(-1,K).float()\n",
    "            cols_tri = triton_img2col(x if not fp16 else x.half(),\n",
    "                                      B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                                      BLOCK_M=64, BLOCK_K=32, fp16=fp16)\n",
    "            diff = (cols_ref - cols_tri.float()).abs()\n",
    "            f_mae = diff.mean().item(); f_mx = diff.max().item()\n",
    "\n",
    "        # Backward эквивалент\n",
    "        dcols = torch.randn_like(cols_ref, dtype=torch.float32)\n",
    "        x_ref = x.clone().detach().float().requires_grad_(True)\n",
    "        cols_ref2 = unfold(x_ref).transpose(1,2).contiguous().view(-1,K)\n",
    "        (cols_ref2 * dcols).sum().backward()\n",
    "        dx_ref = x_ref.grad.detach().float()\n",
    "        dx_tri = triton_col2im(dcols, B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                               BLOCK_M=64, BLOCK_K=32)\n",
    "        bd = (dx_ref - dx_tri).abs()\n",
    "        b_mae = bd.mean().item(); b_mx = bd.max().item()\n",
    "\n",
    "        # Timings\n",
    "        def f_unfold(): unfold(x.float())\n",
    "        def f_i2c(): triton_img2col(x if not fp16 else x.half(),\n",
    "                                    B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                                    BLOCK_M=64, BLOCK_K=32, fp16=fp16)\n",
    "        t_unfold = bench(f_unfold, warmup=30, iters=200)\n",
    "        t_i2c    = bench(f_i2c,    warmup=30, iters=200)\n",
    "\n",
    "        print(f\"\\n[img2col:{prec}]\")\n",
    "        row(\"Этап\", [\"FWD MAE\", \"FWD max\", \"BWD MAE(dx)\", \"BWD max(dx)\", \"Torch Unfold ms\", \"Triton i2c ms\", \"Speedup\"])\n",
    "        row(\"Знач\", [f\"{f_mae:.3e}\", f\"{f_mx:.3e}\", f\"{b_mae:.3e}\", f\"{b_mx:.3e}\",\n",
    "                     f\"{t_unfold*1e3:.3f}\", f\"{t_i2c*1e3:.3f}\", f\"{t_unfold/max(t_i2c,1e-12):.2f}x\"])\n",
    "\n",
    "# ===== ГЛАВА 2: GEMM =====\n",
    "def chapter_gemm(cfg):\n",
    "    print(\"\\n# Глава 2: GEMM (torch @ vs Triton gemm)\")\n",
    "    B,Cin,Cout,H,W,ks = cfg[\"B\"],cfg[\"Cin\"],cfg[\"Cout\"],cfg[\"H\"],cfg[\"W\"],cfg[\"ks\"]\n",
    "    stride,padding,dilation = cfg[\"stride\"],cfg[\"padding\"],cfg[\"dilation\"]\n",
    "    Sh,Sw = (stride,stride) if isinstance(stride,int) else stride\n",
    "    Ph,Pw = (padding,padding) if isinstance(padding,int) else padding\n",
    "    Dh,Dw = (dilation,dilation) if isinstance(dilation,int) else dilation\n",
    "    Kh=Kw=ks\n",
    "    Ho,Wo = out_hw(H,W,Kh,Kw,Sh,Sw,Ph,Pw,Dh,Dw)\n",
    "    M = B*Ho*Wo; K = Cin*Kh*Kw; N = Cout\n",
    "\n",
    "    for prec in (\"fp16\",\"fp16\"):\n",
    "        fp16 = (prec==\"fp16\")\n",
    "        A = torch.randn(M,K, device=device, dtype=(torch.float16 if fp16 else torch.float32))\n",
    "        Bm = torch.randn(K,N, device=device, dtype=(torch.float16 if fp16 else torch.float32))\n",
    "\n",
    "        # Референс — строгий fp16\n",
    "        A_ref = A.float().contiguous()\n",
    "        B_ref = Bm.float().contiguous()\n",
    "\n",
    "        # Forward acc\n",
    "        with torch.no_grad():\n",
    "            C_ref = (A_ref @ B_ref).float()\n",
    "            C_tri = triton_gemm(A, Bm, use_fp16=fp16)\n",
    "            dd = (C_ref - C_tri).abs()\n",
    "            f_mae = dd.mean().item(); f_mx = dd.max().item()\n",
    "\n",
    "        # Backward (по формулам)\n",
    "        R = torch.randn_like(C_ref, dtype=torch.float32)\n",
    "        dA_ref = R @ B_ref.t()\n",
    "        dB_ref = A_ref.t() @ R\n",
    "\n",
    "        dA_tri = triton_gemm(R, B_ref.t(), use_fp16=False)\n",
    "        dB_tri = triton_gemm(A_ref.t(), R, use_fp16=False)\n",
    "\n",
    "        dA_mae = (dA_ref - dA_tri).abs().mean().item(); dA_mx = (dA_ref - dA_tri).abs().max().item()\n",
    "        dB_mae = (dB_ref - dB_tri).abs().mean().item(); dB_mx = (dB_ref - dB_tri).abs().max().item()\n",
    "\n",
    "        # Timings\n",
    "        def f_torch(): A_ref @ B_ref\n",
    "        def f_tri():   triton_gemm(A, Bm, use_fp16=fp16)\n",
    "        t_torch = bench(f_torch, warmup=30, iters=200)\n",
    "        t_tri   = bench(f_tri,   warmup=30, iters=200)\n",
    "\n",
    "        print(f\"\\n[GEMM:{prec}]\")\n",
    "        row(\"Этап\", [\"FWD MAE\", \"FWD max\", \"dA MAE\", \"dA max\", \"dB MAE\", \"dB max\", \"Torch mm ms\", \"Triton GEMM ms\", \"Speedup\"])\n",
    "        row(\"Знач\", [f\"{f_mae:.3e}\", f\"{f_mx:.3e}\", f\"{dA_mae:.3e}\", f\"{dA_mx:.3e}\",\n",
    "                     f\"{dB_mae:.3e}\", f\"{dB_mx:.3e}\", f\"{t_torch*1e3:.3f}\", f\"{t_tri*1e3:.3f}\",\n",
    "                     f\"{t_torch/max(t_tri,1e-12):.2f}x\"])\n",
    "\n",
    "# ===== ГЛАВА 3: ColumnToImage =====\n",
    "def chapter_col2im(cfg):\n",
    "    print(\"\\n# Глава 3: ColumnToImage (Fold vs Triton col2im)\")\n",
    "    B,Cin,Cout,H,W,ks = cfg[\"B\"],cfg[\"Cin\"],cfg[\"Cout\"],cfg[\"H\"],cfg[\"W\"],cfg[\"ks\"]\n",
    "    stride,padding,dilation = cfg[\"stride\"],cfg[\"padding\"],cfg[\"dilation\"]\n",
    "    Sh,Sw = (stride,stride) if isinstance(stride,int) else stride\n",
    "    Ph,Pw = (padding,padding) if isinstance(padding,int) else padding\n",
    "    Dh,Dw = (dilation,dilation) if isinstance(dilation,int) else dilation\n",
    "    Kh=Kw=ks\n",
    "    Ho,Wo = out_hw(H,W,Kh,Kw,Sh,Sw,Ph,Pw,Dh,Dw)\n",
    "    K = Cin*Kh*Kw\n",
    "\n",
    "    fold = nn.Fold(output_size=(H,W), kernel_size=(Kh,Kw),\n",
    "                   dilation=(Dh,Dw), padding=(Ph,Pw), stride=(Sh,Sw)).to(device)\n",
    "\n",
    "    for prec in (\"fp16\",\"fp16\"):\n",
    "        cols = torch.randn(B*Ho*Wo, K, device=device, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            cols_3d = cols.view(B,Ho*Wo,K).transpose(1,2).contiguous()  # [B,K,L]\n",
    "            x_ref   = fold(cols_3d).float()\n",
    "            x_tri   = triton_col2im(cols.float(), B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                                    BLOCK_M=64, BLOCK_K=32)\n",
    "            dd = (x_ref - x_tri).abs()\n",
    "            f_mae = dd.mean().item(); f_mx = dd.max().item()\n",
    "\n",
    "        dx = torch.randn_like(x_ref, dtype=torch.float32)\n",
    "        cols_var = cols_3d.clone().detach().requires_grad_(True)\n",
    "        (fold(cols_var) * dx).sum().backward()\n",
    "        dcols_ref = cols_var.grad.detach().view(B, K, Ho*Wo).transpose(1,2).contiguous().view(-1, K).float()\n",
    "        dcols_tri = triton_img2col(dx.float(), B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                                   BLOCK_M=64, BLOCK_K=32, fp16=False)\n",
    "        bd = (dcols_ref - dcols_tri).abs()\n",
    "        b_mae = bd.mean().item(); b_mx = bd.max().item()\n",
    "\n",
    "        def f_fold():\n",
    "            c3d = cols.view(B,Ho*Wo,K).transpose(1,2).contiguous()\n",
    "            fold(c3d)\n",
    "        def f_c2i():\n",
    "            triton_col2im(cols.float(), B,Cin,H,W, Kh,Kw, Sh,Sw, Ph,Pw, Dh,Dw, Ho,Wo,\n",
    "                          BLOCK_M=64, BLOCK_K=32)\n",
    "        t_fold = bench(f_fold, warmup=30, iters=200)\n",
    "        t_c2i  = bench(f_c2i,  warmup=30, iters=200)\n",
    "\n",
    "        print(f\"\\n[col2im:{prec}]\")\n",
    "        row(\"Этап\", [\"FWD MAE\", \"FWD max\", \"BWD MAE(dcols)\", \"BWD max(dcols)\", \"Torch Fold ms\", \"Triton c2i ms\", \"Speedup\"])\n",
    "        row(\"Знач\", [f\"{f_mae:.3e}\", f\"{f_mx:.3e}\", f\"{b_mae:.3e}\", f\"{b_mx:.3e}\",\n",
    "                     f\"{t_fold*1e3:.3f}\", f\"{t_c2i*1e3:.3f}\", f\"{t_fold/max(t_c2i,1e-12):.2f}x\"])\n",
    "\n",
    "\n",
    "\n",
    "RUN = dict(B=2, Cin=64, Cout=128, H=64, W=64, ks=5, stride=1, padding=1, dilation=1)\n",
    "chapter_img2col(RUN)\n",
    "chapter_gemm(RUN)\n",
    "chapter_col2im(RUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5aa81ef-f64f-44a6-a978-2518277e593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allow_tf32: True\n",
      "f32 matmul precision: high\n"
     ]
    }
   ],
   "source": [
    "print(\"allow_tf32:\", torch.backends.cuda.matmul.allow_tf32)\n",
    "try: print(\"f32 matmul precision:\", torch.get_float32_matmul_precision())\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3b63f-d198-4387-8bda-44649d31b794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5cf005-611c-4ddb-9dc3-8563e762821e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
