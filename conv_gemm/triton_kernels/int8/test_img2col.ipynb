{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae47ebdf-26cd-4f6f-8db2-fadef3edf254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from img2col_int8_kernel import img2col_int8_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81843900-550e-4949-a53e-e3c593426a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_I2C_BLOCK_M = 64\n",
    "INT8_I2C_BLOCK_K = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f9b7e4-8612-4302-a3ce-5ed9aeb59996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2col_int8(\n",
    "    x_q: torch.Tensor,\n",
    "    Kh: int, Kw: int,\n",
    "    Sh: int, Sw: int,\n",
    "    Ph: int, Pw: int,\n",
    "    Dh: int, Dw: int,\n",
    "    BLOCK_M: int = INT8_I2C_BLOCK_M,\n",
    "    BLOCK_K: int = INT8_I2C_BLOCK_K,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    x_q: [N, Cin, H, W], int8\n",
    "    return cols_q: [M, K], int8, (Ho, Wo)\n",
    "    \"\"\"\n",
    "    assert x_q.is_cuda and x_q.dtype == torch.int8\n",
    "    N, Cin, H, W = x_q.shape\n",
    "\n",
    "    Ho = (H + 2 * Ph - Dh * (Kh - 1) - 1) // Sh + 1\n",
    "    Wo = (W + 2 * Pw - Dw * (Kw - 1) - 1) // Sw + 1\n",
    "    M = N * Ho * Wo\n",
    "    K = Cin * Kh * Kw\n",
    "\n",
    "    cols_q = torch.empty((M, K), device=x_q.device, dtype=torch.int8)\n",
    "    sN, sC, sH, sW = x_q.stride()\n",
    "\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "    img2col_int8_kernel[grid](\n",
    "        x_q, cols_q,\n",
    "        N, Cin, H, W,\n",
    "        Kh, Kw, Sh, Sw, Ph, Pw, Dh, Dw,\n",
    "        Ho, Wo,\n",
    "        sN, sC, sH, sW,\n",
    "        K,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "    return cols_q, (Ho, Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece5f934-1100-4789-952b-da4bc138223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23c98db-cc3a-468b-8e43-39e24f4ab5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_tile_once_img2col(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=50,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч ОДНОГО конфига тайлов:\n",
    "      - замеряем ТОЛЬКО время img2col_int8\n",
    "      - для корректности один раз сравниваем с F.unfold (но не меряем его время)\n",
    "    \"\"\"\n",
    "    x_q = torch.randint(-128, 127, (N, Cin, H, W),\n",
    "                        device=device, dtype=torch.int8)\n",
    "\n",
    "    # ---- ref через unfold (только для проверки, без тайминга) ----\n",
    "    x_f = x_q.float()\n",
    "    unf = F.unfold(\n",
    "        x_f,\n",
    "        kernel_size=(Kh, Kw),\n",
    "        dilation=(Dh, Dw),\n",
    "        padding=(Ph, Pw),\n",
    "        stride=(Sh, Sw),\n",
    "    )  # [N, KC, L]\n",
    "\n",
    "    N_, KC, L = unf.shape\n",
    "    assert N_ == N\n",
    "    assert KC == Cin * Kh * Kw\n",
    "\n",
    "    M = N * L          # ВАЖНО: M = N * L, а не L\n",
    "    K = KC             # == Cin * Kh * Kw\n",
    "\n",
    "    unf_N_L_KC = unf.permute(0, 2, 1).contiguous()   # [N, L, KC]\n",
    "    cols_ref = unf_N_L_KC.view(M, K)                 # [M, K]\n",
    "    cols_ref_q = cols_ref.to(torch.int8)\n",
    "\n",
    "    # ---- наш img2col_int8 ----\n",
    "    def _call_triton():\n",
    "        cols_q, (Ho, Wo) = img2col_int8(\n",
    "            x_q,\n",
    "            Kh, Kw,\n",
    "            Sh, Sw,\n",
    "            Ph, Pw,\n",
    "            Dh, Dw,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return cols_q\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        cols_q = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # корректность (один раз, после тайминга)\n",
    "    max_abs_err = (cols_q.to(torch.int16) - cols_ref_q.to(torch.int16)).abs().max().item()\n",
    "\n",
    "    # грубая оценка памяти: 2*M*K байт (load + store int8)\n",
    "    bytes_moved = 2.0 * M * K\n",
    "    bw_triton = bytes_moved / t_triton / 1e9  # GB/s\n",
    "\n",
    "    return {\n",
    "        \"N\": N,\n",
    "        \"Cin\": Cin,\n",
    "        \"H\": H, \"W\": W,\n",
    "        \"Kh\": Kh, \"Kw\": Kw,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "151419bd-ca4a-4ea9-bf3c-ae7dbe243e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_img2col_vs_unfold(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=50,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч для ОДНОГО shape и ОДНОГО набора тайлов:\n",
    "      - меряем t_triton_ms и t_unfold_ms\n",
    "      - считаем speed_vs_unfold\n",
    "      - проверяем max_abs_err\n",
    "    \"\"\"\n",
    "    x_q = torch.randint(-128, 127, (N, Cin, H, W),\n",
    "                        device=device, dtype=torch.int8)\n",
    "\n",
    "    # ref\n",
    "    x_f = x_q.float()\n",
    "    unf = F.unfold(\n",
    "        x_f,\n",
    "        kernel_size=(Kh, Kw),\n",
    "        dilation=(Dh, Dw),\n",
    "        padding=(Ph, Pw),\n",
    "        stride=(Sh, Sw),\n",
    "    )  # [N, KC, L]\n",
    "\n",
    "    N_, KC, L = unf.shape\n",
    "    assert N_ == N\n",
    "    assert KC == Cin * Kh * Kw\n",
    "\n",
    "    M = N * L          # ВАЖНО: M = N * L\n",
    "    K = KC\n",
    "\n",
    "    unf_N_L_KC = unf.permute(0, 2, 1).contiguous()   # [N,L,KC]\n",
    "    cols_ref = unf_N_L_KC.view(M, K)                 # [M,K]\n",
    "    cols_ref_q = cols_ref.to(torch.int8)\n",
    "\n",
    "    # наш\n",
    "    def _call_triton():\n",
    "        cols_q, (Ho, Wo) = img2col_int8(\n",
    "            x_q,\n",
    "            Kh, Kw,\n",
    "            Sh, Sw,\n",
    "            Ph, Pw,\n",
    "            Dh, Dw,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return cols_q\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        cols_q = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # unfold\n",
    "    def _call_unfold():\n",
    "        unf = F.unfold(\n",
    "            x_f,\n",
    "            kernel_size=(Kh, Kw),\n",
    "            dilation=(Dh, Dw),\n",
    "            padding=(Ph, Pw),\n",
    "            stride=(Sh, Sw),\n",
    "        )\n",
    "        unf_N_L_KC = unf.permute(0, 2, 1).contiguous()  # [N,L,KC]\n",
    "        cols = unf_N_L_KC.view(M, K)                    # [M,K]\n",
    "        return cols\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_unfold()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        cols_ref2 = _call_unfold()\n",
    "    torch.cuda.synchronize()\n",
    "    t_unfold = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # ошибка\n",
    "    max_abs_err = (cols_q.to(torch.int16) - cols_ref_q.to(torch.int16)).abs().max().item()\n",
    "\n",
    "    bytes_moved = 2.0 * M * K\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_unfold = bytes_moved / t_unfold / 1e9\n",
    "\n",
    "    return {\n",
    "        \"N\": N,\n",
    "        \"Cin\": Cin,\n",
    "        \"H\": H, \"W\": W,\n",
    "        \"Kh\": Kh, \"Kw\": Kw,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_unfold_ms\": t_unfold * 1e3,\n",
    "        \"speed_vs_unfold\": t_unfold / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_unfold_GBs\": bw_unfold,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d16d216-c891-41ce-ab54-62cd8bfa32f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_tiles_for_one_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_tiles, df_tiles \u001b[38;5;241m=\u001b[39m \u001b[43mtune_tiles_for_one_shape\u001b[49m(\n\u001b[1;32m      2\u001b[0m     N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, Cin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m,\n\u001b[1;32m      3\u001b[0m     Kh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, Kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      4\u001b[0m     Sh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, Sw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     Ph\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, Pw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m     Dh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, Dw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     blocks_M\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m      8\u001b[0m     blocks_K\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m      9\u001b[0m     warps\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     10\u001b[0m     stages\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     11\u001b[0m     iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     12\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST TILES:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_tiles)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tune_tiles_for_one_shape' is not defined"
     ]
    }
   ],
   "source": [
    "best_tiles, df_tiles = tune_tiles_for_one_shape(\n",
    "    N=4, Cin=64, H=224, W=224,\n",
    "    Kh=3, Kw=3,\n",
    "    Sh=1, Sw=1,\n",
    "    Ph=1, Pw=1,\n",
    "    Dh=1, Dw=1,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=500,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "print(\"BEST TILES:\", best_tiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8dd059f-f03a-4381-984a-585b893f4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_I2C_BEST_BLOCK_M = 128\n",
    "INT8_I2C_BEST_BLOCK_K = 32\n",
    "INT8_I2C_BEST_WARPS   = 2\n",
    "INT8_I2C_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6dcfb7-b055-411e-b947-56b2e3c8d385",
   "metadata": {},
   "source": [
    "# BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73d1cd-1a92-4362-bd22-305b91022c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_img2col_vs_unfold(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=50,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч для ОДНОГО shape и ОДНОГО набора тайлов:\n",
    "      - меряем t_triton_ms и t_unfold_ms\n",
    "      - считаем speed_vs_unfold\n",
    "      - проверяем max_abs_err\n",
    "    \"\"\"\n",
    "    x_q = torch.randint(-128, 127, (N, Cin, H, W),\n",
    "                        device=device, dtype=torch.int8)\n",
    "\n",
    "    # ref\n",
    "    x_f = x_q.float()\n",
    "    unf = F.unfold(\n",
    "        x_f,\n",
    "        kernel_size=(Kh, Kw),\n",
    "        dilation=(Dh, Dw),\n",
    "        padding=(Ph, Pw),\n",
    "        stride=(Sh, Sw),\n",
    "    )  # [N, KC, L]\n",
    "\n",
    "    N_, KC, L = unf.shape\n",
    "    assert N_ == N\n",
    "    assert KC == Cin * Kh * Kw\n",
    "\n",
    "    M = N * L          # ВАЖНО: M = N * L\n",
    "    K = KC\n",
    "\n",
    "    unf_N_L_KC = unf.permute(0, 2, 1).contiguous()   # [N,L,KC]\n",
    "    cols_ref = unf_N_L_KC.view(M, K)                 # [M,K]\n",
    "    cols_ref_q = cols_ref.to(torch.int8)\n",
    "\n",
    "    # наш\n",
    "    def _call_triton():\n",
    "        cols_q, (Ho, Wo) = img2col_int8(\n",
    "            x_q,\n",
    "            Kh, Kw,\n",
    "            Sh, Sw,\n",
    "            Ph, Pw,\n",
    "            Dh, Dw,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return cols_q\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        cols_q = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # unfold\n",
    "    def _call_unfold():\n",
    "        unf = F.unfold(\n",
    "            x_f,\n",
    "            kernel_size=(Kh, Kw),\n",
    "            dilation=(Dh, Dw),\n",
    "            padding=(Ph, Pw),\n",
    "            stride=(Sh, Sw),\n",
    "        )\n",
    "        unf_N_L_KC = unf.permute(0, 2, 1).contiguous()  # [N,L,KC]\n",
    "        cols = unf_N_L_KC.view(M, K)                    # [M,K]\n",
    "        return cols\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_unfold()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        cols_ref2 = _call_unfold()\n",
    "    torch.cuda.synchronize()\n",
    "    t_unfold = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # ошибка\n",
    "    max_abs_err = (cols_q.to(torch.int16) - cols_ref_q.to(torch.int16)).abs().max().item()\n",
    "\n",
    "    bytes_moved = 2.0 * M * K\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_unfold = bytes_moved / t_unfold / 1e9\n",
    "\n",
    "    return {\n",
    "        \"N\": N,\n",
    "        \"Cin\": Cin,\n",
    "        \"H\": H, \"W\": W,\n",
    "        \"Kh\": Kh, \"Kw\": Kw,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_unfold_ms\": t_unfold * 1e3,\n",
    "        \"speed_vs_unfold\": t_unfold / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_unfold_GBs\": bw_unfold,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c85aa33d-db91-421b-b6ef-a9bf7ba971f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_img2col_best_tiles(\n",
    "    tiles_cfg,\n",
    "    image_sizes=(112, 1024),\n",
    "    batch_sizes=(1, 2, 4),\n",
    "    channel_pairs=((1, 3), (3, 3), (3, 8), (8, 16), (32, 32)),\n",
    "    kernels=(1, 3, 5, 7, 9, 11),\n",
    "    iters_per_shape=20,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Сравнение img2col_int8 vs F.unfold на сетке параметров,\n",
    "    с фиксированными лучшими тайлами tiles_cfg.\n",
    "\n",
    "    tiles_cfg: dict с ключами:\n",
    "      - BLOCK_M\n",
    "      - BLOCK_K\n",
    "      - num_warps\n",
    "      - num_stages\n",
    "    \"\"\"\n",
    "    BM = 128\n",
    "    BK = 32\n",
    "    NW = 2\n",
    "    NS = 3\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for H in image_sizes:\n",
    "        W = H\n",
    "        for N in batch_sizes:\n",
    "            for (Cin, Cout) in channel_pairs:\n",
    "                for k in kernels:\n",
    "                    Ph = Pw = k // 2\n",
    "                    print(f\"=== SHAPE: N={N}, Cin={Cin}, Cout={Cout}, H=W={H}, k={k}x{k} ===\")\n",
    "\n",
    "                    try:\n",
    "                        rec = bench_once_img2col_vs_unfold(\n",
    "                            N, Cin, H, W,\n",
    "                            k, k,\n",
    "                            1, 1,       # Sh, Sw\n",
    "                            Ph, Pw,     # padding\n",
    "                            1, 1,       # Dh, Dw\n",
    "                            BLOCK_M=BM,\n",
    "                            BLOCK_K=BK,\n",
    "                            num_warps=NW,\n",
    "                            num_stages=NS,\n",
    "                            iters=iters_per_shape,\n",
    "                            device=device,\n",
    "                        )\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"[SKIP] N={N}, Cin={Cin}, Cout={Cout}, H={H}, k={k}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # проверка корректности\n",
    "                    if rec[\"max_abs_err\"] != 0:\n",
    "                        print(f\"[WRONG] N={N}, Cin={Cin}, Cout={Cout}, H={H}, k={k}, err={rec['max_abs_err']}\")\n",
    "                        continue\n",
    "\n",
    "                    # добавляем Cout в запись (bench_once его не знает)\n",
    "                    rec[\"Cout\"] = Cout\n",
    "                    records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        print(\"[WARN] no successful records\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36a05690-ff34-48d8-8e24-7c437ead4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=112, k=11: CUDA out of memory. Tried to allocate 742.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 290.69 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.81 GiB memory in use. Of the allocated memory 3.29 GiB is allocated by PyTorch, and 123.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=9x9 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=256, k=9: CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 235.06 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 4.01 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 751.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=11x11 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=256, k=11: CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 293.50 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.85 GiB memory in use. Of the allocated memory 3.34 GiB is allocated by PyTorch, and 121.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=7x7 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=256, k=7: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 66.19 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.90 GiB memory in use. Of the allocated memory 3.48 GiB is allocated by PyTorch, and 22.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=9x9 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=256, k=9: CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 331.06 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.20 GiB is allocated by PyTorch, and 22.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=11x11 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=256, k=11: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.59 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.35 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 22.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=9x9 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=256, k=9: CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 688.06 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 1.62 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=11x11 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=256, k=11: CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 884.56 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.94 GiB memory in use. Of the allocated memory 2.40 GiB is allocated by PyTorch, and 145.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=5x5 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=5: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 181.75 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.72 GiB memory in use. Of the allocated memory 2.79 GiB is allocated by PyTorch, and 544.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=7x7 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=7: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.31 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 1.59 GiB is allocated by PyTorch, and 612.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=9x9 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 687.25 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.22 GiB memory in use. Of the allocated memory 2.59 GiB is allocated by PyTorch, and 244.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.20 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 710.00 MiB memory in use. Of the allocated memory 89.77 MiB is allocated by PyTorch, and 212.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=9x9 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=512, k=9: CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 522.56 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 31.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=11x11 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=512, k=11: CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 1.02 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.40 GiB is allocated by PyTorch, and 23.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=5x5 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=5: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 616.75 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.56 GiB memory in use. Of the allocated memory 2.79 GiB is allocated by PyTorch, and 380.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=7x7 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=7: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 492.50 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.71 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 202.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=9x9 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 901.50 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.18 GiB memory in use. Of the allocated memory 2.59 GiB is allocated by PyTorch, and 202.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=11x11 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.40 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 668.00 MiB memory in use. Of the allocated memory 89.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=11x11 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=512, k=11: CUDA out of memory. Tried to allocate 726.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 57.31 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 3.22 GiB is allocated by PyTorch, and 33.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=9x9 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=512, k=9: CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 547.12 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.47 GiB memory in use. Of the allocated memory 2.16 GiB is allocated by PyTorch, and 932.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=11x11 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=512, k=11: CUDA out of memory. Tried to allocate 726.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 40.12 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.47 GiB memory in use. Of the allocated memory 2.51 GiB is allocated by PyTorch, and 578.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=5x5 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=512, k=5: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 397.31 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.47 GiB memory in use. Of the allocated memory 1.79 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=7x7 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=512, k=7: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 619.31 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 22.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=9x9 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=512, k=9: CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 416.06 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 22.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=11x11 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=512, k=11: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.25 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.35 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 22.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=3x3 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=3: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 256.56 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.47 GiB memory in use. Of the allocated memory 2.63 GiB is allocated by PyTorch, and 458.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=5x5 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=5: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 790.69 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.91 GiB memory in use. Of the allocated memory 1.66 GiB is allocated by PyTorch, and 874.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=7x7 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=7: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.66 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 161.77 MiB is allocated by PyTorch, and 474.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=9x9 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=9: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.65 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 161.77 MiB is allocated by PyTorch, and 474.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=11x11 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=11: CUDA out of memory. Tried to allocate 7.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.65 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 161.77 MiB is allocated by PyTorch, and 474.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=512, k=9: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 551.69 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.36 GiB is allocated by PyTorch, and 27.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=512, k=11: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 226.00 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 4.00 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 27.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=512, k=9: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 815.12 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 2.42 GiB is allocated by PyTorch, and 853.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=512, k=11: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 243.12 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 4.00 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 27.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=5x5 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=5: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 397.31 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=7x7 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=7: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 165.31 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.88 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 366.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 709.25 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 2.59 GiB is allocated by PyTorch, and 366.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.22 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 832.00 MiB memory in use. Of the allocated memory 89.77 MiB is allocated by PyTorch, and 334.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=3x3 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=3: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 31.44 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.11 GiB is allocated by PyTorch, and 246.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=5x5 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=5: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=7x7 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=7: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=9: CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=11: CUDA out of memory. Tried to allocate 15.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=3, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 998.12 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.83 GiB memory in use. Of the allocated memory 2.42 GiB is allocated by PyTorch, and 15.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=3, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 150.88 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 3.22 GiB is allocated by PyTorch, and 27.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=1, Cin=3, Cout=8, H=1024, k=7: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 84.00 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 2.62 GiB is allocated by PyTorch, and 649.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=3, Cout=8, H=1024, k=9: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 780.75 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 2.42 GiB is allocated by PyTorch, and 853.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=3, Cout=8, H=1024, k=11: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 229.88 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 4.00 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 27.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=5: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 394.00 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.65 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=7: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 161.94 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.88 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 366.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 705.88 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 2.59 GiB is allocated by PyTorch, and 366.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.22 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 832.00 MiB memory in use. Of the allocated memory 89.77 MiB is allocated by PyTorch, and 334.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=3: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 30.69 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.11 GiB is allocated by PyTorch, and 246.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=5: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=7: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=9: CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=11: CUDA out of memory. Tried to allocate 15.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.92 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 884.00 MiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 170.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=1, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 38.88 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.34 GiB is allocated by PyTorch, and 31.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=5: CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 272.19 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.53 GiB memory in use. Of the allocated memory 2.10 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=7: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 7.68 GiB of which 244.25 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 2.94 GiB is allocated by PyTorch, and 282.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.48 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.36 GiB memory in use. Of the allocated memory 1.95 GiB is allocated by PyTorch, and 12.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 2.84 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.47 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.36 GiB memory in use. Of the allocated memory 71.77 MiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=5: CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 526.12 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.94 GiB memory in use. Of the allocated memory 2.10 GiB is allocated by PyTorch, and 456.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=7: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.10 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.36 GiB memory in use. Of the allocated memory 1.20 GiB is allocated by PyTorch, and 780.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=9: CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.10 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.36 GiB memory in use. Of the allocated memory 71.77 MiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=11: CUDA out of memory. Tried to allocate 2.84 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.10 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.36 GiB memory in use. Of the allocated memory 71.77 MiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=3: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 219.81 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.63 GiB is allocated by PyTorch, and 466.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=5: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 835.81 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 1.66 GiB is allocated by PyTorch, and 882.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=7: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.72 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 161.77 MiB is allocated by PyTorch, and 474.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=9: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.73 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 161.77 MiB is allocated by PyTorch, and 474.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=11: CUDA out of memory. Tried to allocate 7.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.73 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.02 GiB memory in use. Of the allocated memory 161.77 MiB is allocated by PyTorch, and 474.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=3: CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.21 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.98 GiB memory in use. Of the allocated memory 2.58 GiB is allocated by PyTorch, and 4.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=5: CUDA out of memory. Tried to allocate 6.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.21 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.98 GiB memory in use. Of the allocated memory 593.77 MiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=7: CUDA out of memory. Tried to allocate 12.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.21 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.98 GiB memory in use. Of the allocated memory 593.77 MiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=9: CUDA out of memory. Tried to allocate 20.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.20 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.98 GiB memory in use. Of the allocated memory 593.77 MiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=11: CUDA out of memory. Tried to allocate 30.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.20 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.98 GiB memory in use. Of the allocated memory 593.77 MiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=1, Cout=3, H=1024, k=7: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 288.31 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 648.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=1, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.31 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.73 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=1, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.32 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.73 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 416.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=5: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 7.68 GiB of which 35.06 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.90 GiB memory in use. Of the allocated memory 3.05 GiB is allocated by PyTorch, and 464.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=7: CUDA out of memory. Tried to allocate 2.30 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.28 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.65 GiB memory in use. Of the allocated memory 125.77 MiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.28 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.65 GiB memory in use. Of the allocated memory 125.77 MiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 5.67 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.28 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.65 GiB memory in use. Of the allocated memory 125.77 MiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=5: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 7.68 GiB of which 465.06 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.05 GiB is allocated by PyTorch, and 332.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=7: CUDA out of memory. Tried to allocate 2.30 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.45 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.78 GiB memory in use. Of the allocated memory 2.37 GiB is allocated by PyTorch, and 8.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=9: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.45 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.78 GiB memory in use. Of the allocated memory 125.77 MiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=11: CUDA out of memory. Tried to allocate 5.67 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.45 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.78 GiB memory in use. Of the allocated memory 125.77 MiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=3: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 791.62 MiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 3.90 GiB memory in use. Of the allocated memory 3.11 GiB is allocated by PyTorch, and 404.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=5: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.06 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 932.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=7: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.05 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 932.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=9: CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.05 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 932.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=11: CUDA out of memory. Tried to allocate 15.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.05 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 305.77 MiB is allocated by PyTorch, and 932.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=3: CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.60 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 4.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=5: CUDA out of memory. Tried to allocate 12.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.60 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 4.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=7: CUDA out of memory. Tried to allocate 24.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.60 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 4.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=9: CUDA out of memory. Tried to allocate 40.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.60 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 4.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=11: CUDA out of memory. Tried to allocate 60.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.59 GiB is free. Process 85059 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 4.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "=== img2col_int8 vs F.unfold (fixed best tiles) ===\n",
      "\n",
      " N  Cin  Cout    H  Kh  BLOCK_M  BLOCK_K  t_triton_ms  t_unfold_ms  speed_vs_unfold  bw_triton_GBs\n",
      " 1   32    32  512   3      128       32        0.448        7.393           16.493        336.864\n",
      " 4   32    32 1024   1      128       32        0.513        8.460           16.486        523.124\n",
      " 4   32    32  512   1      128       32        0.136        2.007           14.707        491.865\n",
      " 2   32    32 1024   1      128       32        0.263        3.779           14.361        510.045\n",
      " 2   32    32  512   1      128       32        0.073        0.988           13.483        458.142\n",
      " 4   32    32  256   1      128       32        0.040        0.527           13.307        423.332\n",
      " 1   32    32 1024   1      128       32        0.158        1.917           12.109        423.808\n",
      " 1    3     3 1024   7      128       32        1.126       13.576           12.056        273.751\n",
      " 4    3     8  512   7      128       32        1.171       13.110           11.193        263.211\n",
      " 1   32    32  256   3      128       32        0.137        1.488           10.830        274.676\n",
      " 1    1     3 1024  11      128       32        0.838        8.997           10.737        302.812\n",
      " 1    8    16  512   7      128       32        0.932        9.858           10.576        220.502\n",
      " 1    8    16  512   5      128       32        0.414        4.336           10.477        253.345\n",
      " 1    3     3  512   9      128       32        0.538        5.587           10.380        236.693\n",
      " 4   32    32  256   3      128       32        0.470        4.852           10.327        321.333\n",
      " 2    3     3  512   9      128       32        1.094       11.214           10.252        232.949\n",
      " 1    3     8  512   7      128       32        0.274        2.789           10.166        280.944\n",
      " 2    3     3  512   7      128       32        0.558        5.637           10.110        276.434\n",
      " 1    3     8  512   9      128       32        0.542        5.459           10.071        235.035\n",
      " 1    3     8  512  11      128       32        0.876        8.817           10.070        217.364\n",
      " 1    3     3  512   7      128       32        0.287        2.838            9.899        268.846\n",
      " 4    3     3  512   7      128       32        1.223       11.837            9.681        252.133\n",
      " 1   32    32  256   7      128       32        0.820        7.776            9.481        250.598\n",
      " 2   32    32  256   3      128       32        0.257        2.412            9.366        293.206\n",
      " 2    1     3  512  11      128       32        0.440        4.059            9.227        288.413\n",
      " 1    1     3  512  11      128       32        0.246        2.251            9.140        257.542\n",
      " 1    8    16  256   5      128       32        0.089        0.805            9.075        295.710\n",
      " 4    1     3  512  11      128       32        0.926        8.258            8.921        274.126\n",
      " 1    3     3  512  11      128       32        0.959        8.527            8.891        198.423\n",
      " 1   32    32  256   5      128       32        0.431        3.820            8.870        243.484\n",
      " 2    3     8  512   7      128       32        0.730        5.956            8.159        211.136\n",
      " 1   32    32  112  11      128       32        0.275        2.239            8.141        353.263\n",
      " 2    3     3  256   9      128       32        0.242        1.964            8.132        263.768\n",
      " 2    1     3  256  11      128       32        0.117        0.921            7.862        270.635\n",
      " 1    3     8  256  11      128       32        0.204        1.575            7.729        233.460\n",
      " 2    3     3  256   7      128       32        0.126        0.948            7.502        305.128\n",
      " 1   32    32  112   7      128       32        0.103        0.775            7.490        380.316\n",
      " 2   32    32  256   5      128       32        0.942        6.897            7.322        222.631\n",
      " 1    3     3  256  11      128       32        0.242        1.769            7.310        196.575\n",
      " 2    8    16  256  11      128       32        1.151        8.286            7.197        220.394\n",
      " 4    1     3  256  11      128       32        0.200        1.424            7.124        317.391\n",
      " 2    3     8  256   7      128       32        0.129        0.911            7.072        299.263\n",
      " 4    8    16  256   7      128       32        0.904        6.333            7.006        227.351\n",
      " 1   32    32  512   1      128       32        0.076        0.527            6.942        221.064\n",
      " 4    8    16  256   5      128       32        0.404        2.775            6.874        259.724\n",
      " 2   32    32  112  11      128       32        0.662        4.532            6.849        293.632\n",
      " 2    8    16  256   5      128       32        0.212        1.444            6.818        247.488\n",
      " 1   32    32  112   5      128       32        0.059        0.403            6.818        339.335\n",
      " 2    8    16  256   7      128       32        0.459        3.131            6.817        223.737\n",
      " 2    8    16  256   9      128       32        0.780        5.299            6.797        217.911\n",
      " 1   32    32  112   9      128       32        0.197        1.331            6.773        330.916\n",
      " 1    3     3  256   7      128       32        0.093        0.615            6.621        207.533\n",
      " 4   32    32  112   3      128       32        0.091        0.602            6.604        317.279\n",
      " 1    1     3 1024   9      128       32        0.661        4.337            6.561        256.977\n",
      " 2    1     3  512   9      128       32        0.296        1.916            6.463        286.554\n",
      " 2    1     3 1024   9      128       32        1.460        9.333            6.392        232.683\n",
      " 4    3     3  256  11      128       32        0.923        5.781            6.265        206.254\n",
      " 1    1     3  512   9      128       32        0.158        0.987            6.237        268.486\n",
      " 1    8    16  256  11      128       32        0.698        4.349            6.234        181.878\n",
      " 1    8    16  256   7      128       32        0.308        1.918            6.233        166.967\n",
      " 4    3     3  512   5      128       32        0.644        4.016            6.232        244.113\n",
      " 1    3     8 1024   5      128       32        0.662        4.112            6.210        237.551\n",
      " 4    3     3  256   9      128       32        0.585        3.633            6.208        217.689\n",
      " 2   32    32  256   1      128       32        0.045        0.281            6.208        185.318\n",
      " 1    8    16  256   9      128       32        0.419        2.598            6.194        202.520\n",
      " 4    3     3  256   7      128       32        0.299        1.841            6.165        258.041\n",
      " 2   32    32  112   3      128       32        0.045        0.278            6.153        319.335\n",
      " 4    3     8  256   9      128       32        0.583        3.504            6.009        218.500\n",
      " 4    3     8  256  11      128       32        0.947        5.672            5.990        201.011\n",
      " 1    3     3 1024   5      128       32        0.637        3.779            5.930        246.775\n",
      " 4    8    16  512   3      128       32        0.660        3.884            5.888        228.901\n",
      " 2    3     3  256  11      128       32        0.505        2.969            5.879        188.397\n",
      " 2    8    16  112   7      128       32        0.071        0.418            5.856        275.521\n",
      " 1    1     3  256  11      128       32        0.075        0.437            5.852        212.525\n",
      " 4    3     8  256   7      128       32        0.313        1.827            5.845        246.509\n",
      " 2    3     8  256  11      128       32        0.512        2.921            5.700        185.729\n",
      " 4    1     3  512   9      128       32        0.674        3.834            5.686        251.949\n",
      " 1    3     8  256   7      128       32        0.101        0.570            5.645        190.760\n",
      " 1    3     3  256   9      128       32        0.166        0.936            5.635        191.780\n",
      " 2    3     8  256   9      128       32        0.320        1.777            5.549        198.929\n",
      " 2    3     8  512   5      128       32        0.323        1.771            5.485        243.485\n",
      " 1    8    16  112   9      128       32        0.052        0.280            5.356        311.323\n",
      " 2    8    16  112   5      128       32        0.041        0.216            5.260        244.287\n",
      " 2   32    32  112   5      128       32        0.137        0.719            5.250        293.071\n",
      " 1    8    16  512   3      128       32        0.158        0.818            5.183        239.273\n",
      " 2    8    16  112  11      128       32        0.199        1.014            5.106        244.541\n",
      " 1    3     3  512   5      128       32        0.170        0.864            5.086        231.433\n",
      " 2    3     3  512   5      128       32        0.333        1.679            5.042        236.220\n",
      " 2    8    16 1024   1      128       32        0.200        0.989            4.939        167.511\n",
      " 4    3     8  512   5      128       32        0.723        3.527            4.877        217.487\n",
      " 2    3     3  112  11      128       32        0.069        0.337            4.857        262.663\n",
      " 1    3     8  512   5      128       32        0.176        0.849            4.813        222.915\n",
      " 4   32    32  112   9      128       32        1.150        5.494            4.778        226.211\n",
      " 2   32    32  112   9      128       32        0.540        2.569            4.760        240.988\n",
      " 1    3     8  256   9      128       32        0.190        0.901            4.743        167.709\n",
      " 4    3     8  112   7      128       32        0.054        0.254            4.737        275.220\n",
      " 2   32    32  112   7      128       32        0.317        1.479            4.669        248.364\n",
      " 4   32    32  112   7      128       32        0.626        2.917            4.659        251.337\n",
      " 4    3     3  112   9      128       32        0.103        0.478            4.629        236.024\n",
      " 4    1     3  112  11      128       32        0.054        0.248            4.602        225.168\n",
      " 4    3     8  112   9      128       32        0.095        0.432            4.536        256.189\n",
      " 4   32    32  112   5      128       32        0.313        1.404            4.491        256.842\n",
      " 4    3     3  112   7      128       32        0.055        0.246            4.491        269.420\n",
      " 1    8    16  112  11      128       32        0.095        0.419            4.400        255.036\n",
      " 1    3     8  112  11      128       32        0.039        0.169            4.363        235.759\n",
      " 4    8    16  512   1      128       32        0.114        0.496            4.362        147.577\n",
      " 1    8    16 1024   3      128       32        0.902        3.920            4.345        167.353\n",
      " 4    8    16  112   7      128       32        0.167        0.716            4.286        235.386\n",
      " 2    8    16  112   9      128       32        0.153        0.652            4.254        211.991\n",
      " 1    8    16  112   7      128       32        0.044        0.184            4.224        225.464\n",
      " 4    8    16 1024   1      128       32        0.450        1.897            4.215        149.079\n",
      " 2    3     8  112   9      128       32        0.055        0.228            4.147        222.036\n",
      " 2    8    16  512   1      128       32        0.080        0.317            3.969        104.983\n",
      " 2    1     3 1024   7      128       32        0.894        3.511            3.927        229.870\n",
      " 1    8    16 1024   1      128       32        0.139        0.546            3.927        120.764\n",
      " 2    8    16  512   3      128       32        0.420        1.649            3.926        179.790\n",
      " 2    3     3  112   9      128       32        0.058        0.229            3.924        209.146\n",
      " 2    3     8  112  11      128       32        0.089        0.350            3.924        204.134\n",
      " 1    3     3 1024   3      128       32        0.210        0.816            3.880        269.241\n",
      " 2    3     3  512   3      128       32        0.100        0.384            3.830        282.386\n",
      " 4    3     8  512   3      128       32        0.246        0.937            3.802        229.759\n",
      " 4    1     3  256   9      128       32        0.165        0.622            3.766        256.919\n",
      " 1    1     3 1024   7      128       32        0.462        1.739            3.765        222.434\n",
      " 4    3     3  256   3      128       32        0.052        0.196            3.747        270.176\n",
      " 4    3     3  112  11      128       32        0.170        0.626            3.681        214.131\n",
      " 4    3     8  256   3      128       32        0.052        0.192            3.657        269.990\n",
      " 4    3     3 1024   3      128       32        0.847        3.094            3.653        267.358\n",
      " 2    3     3 1024   3      128       32        0.427        1.552            3.638        265.469\n",
      " 4    8    16  112  11      128       32        0.495        1.787            3.612        196.306\n",
      " 4    1     3  512   7      128       32        0.435        1.557            3.580        236.285\n",
      " 4    8    16  256   1      128       32        0.038        0.135            3.548        110.622\n",
      " 4    3     8  112  11      128       32        0.193        0.683            3.542        188.774\n",
      " 4    3     3  512   3      128       32        0.234        0.814            3.472        241.554\n",
      " 2    3     8 1024   3      128       32        0.443        1.528            3.454        255.919\n",
      " 4    1     3  256   7      128       32        0.105        0.363            3.441        243.813\n",
      " 1    1     3 1024   5      128       32        0.212        0.726            3.425        247.356\n",
      " 4    3     8  256   5      128       32        0.157        0.535            3.414        250.870\n",
      " 4    8    16  112   9      128       32        0.344        1.174            3.409        188.794\n",
      " 4    3     8 1024   3      128       32        0.917        3.083            3.362        246.970\n",
      " 1    3     8 1024   3      128       32        0.239        0.802            3.361        237.268\n",
      " 1    1     3  256   9      128       32        0.057        0.188            3.328        187.825\n",
      " 2    1     3  512   7      128       32        0.239        0.792            3.312        214.919\n",
      " 4    3     3  256   5      128       32        0.170        0.558            3.275        230.709\n",
      " 4    1     3  112   9      128       32        0.049        0.161            3.261        164.527\n",
      " 1    1     3  512   5      128       32        0.059        0.189            3.238        223.941\n",
      " 2    3     8  256   5      128       32        0.075        0.241            3.210        261.515\n",
      " 2    3     8  512   3      128       32        0.124        0.397            3.203        228.623\n",
      " 2    1     3 1024   5      128       32        0.428        1.364            3.188        245.040\n",
      " 4    8    16  112   3      128       32        0.037        0.116            3.163        196.506\n",
      " 1    3     3  112  11      128       32        0.051        0.161            3.153        178.253\n",
      " 1    1     3  512   7      128       32        0.123        0.383            3.119        209.339\n",
      " 1    3     8  256   5      128       32        0.049        0.149            3.064        201.563\n",
      " 2    8    16  256   3      128       32        0.081        0.249            3.062        232.029\n",
      " 2    3     3  256   3      128       32        0.044        0.134            3.052        161.392\n",
      " 4    8    16  256   3      128       32        0.174        0.530            3.048        216.972\n",
      " 4    1     3  256   5      128       32        0.071        0.217            3.041        184.075\n",
      " 2    3     3  256   5      128       32        0.098        0.298            3.039        200.225\n",
      " 2    1     3  256   7      128       32        0.069        0.210            3.035        185.800\n",
      " 4    1     3 1024   5      128       32        0.914        2.751            3.010        229.426\n",
      " 4    3     8  112   5      128       32        0.045        0.134            2.975        166.608\n",
      " 1    3     8  512   3      128       32        0.064        0.185            2.874        220.386\n",
      " 1    3     3  512   3      128       32        0.070        0.201            2.855        200.928\n",
      " 2    3     8  112   7      128       32        0.041        0.118            2.850        178.029\n",
      " 2    1     3  256   9      128       32        0.113        0.320            2.822        187.176\n",
      " 1    8    16  112   5      128       32        0.038        0.105            2.764        131.522\n",
      " 1    8    16  256   3      128       32        0.056        0.149            2.678        169.434\n",
      " 4    1     3  512   5      128       32        0.247        0.659            2.671        212.579\n",
      " 1    3     8  112   9      128       32        0.038        0.099            2.627        161.080\n",
      " 1    8    16  512   1      128       32        0.045        0.116            2.595         93.543\n",
      " 1   32    32  112   3      128       32        0.052        0.132            2.554        139.425\n",
      " 2    3     3  112   7      128       32        0.051        0.131            2.543        143.384\n",
      " 2    1     3  512   5      128       32        0.127        0.321            2.537        207.020\n",
      " 1    3     3  112   9      128       32        0.051        0.125            2.439        118.543\n",
      " 2    1     3  256   5      128       32        0.051        0.112            2.177        127.751\n",
      " 2    1     3  112  11      128       32        0.052        0.111            2.125        116.383\n",
      " 2    3     8  256   3      128       32        0.051        0.108            2.120        139.523\n",
      " 4    3     3  112   5      128       32        0.051        0.102            2.002        148.010\n",
      " 1    3     3  256   5      128       32        0.077        0.152            1.981        128.118\n",
      " 4    3     3  512   1      128       32        0.099        0.193            1.946         63.580\n",
      " 1    3     8 1024   1      128       32        0.126        0.230            1.829         49.944\n",
      " 4    3     3 1024   1      128       32        0.415        0.759            1.826         60.579\n",
      " 1    3     8  112   7      128       32        0.038        0.069            1.797         96.496\n",
      " 2    3     3  512   1      128       32        0.053        0.093            1.771         59.810\n",
      " 1    1     3  256   7      128       32        0.052        0.093            1.765        122.363\n",
      " 4    3     8  512   1      128       32        0.126        0.213            1.692         50.016\n",
      " 1    3     3 1024   1      128       32        0.096        0.160            1.660         65.259\n",
      " 4    1     3  112   7      128       32        0.057        0.094            1.647         86.325\n",
      " 4    3     8 1024   1      128       32        0.456        0.726            1.592         55.184\n",
      " 4   32    32  112   1      128       32        0.060        0.094            1.574         53.613\n",
      " 2    3     8  112   5      128       32        0.036        0.057            1.573        103.375\n",
      " 2    8    16  112   3      128       32        0.041        0.062            1.527         88.493\n",
      " 1    3     3  112   7      128       32        0.050        0.077            1.520         73.198\n",
      " 2    3     8 1024   1      128       32        0.246        0.373            1.514         51.082\n",
      " 4    3     3  256   1      128       32        0.037        0.055            1.499         42.570\n",
      " 4    3     8  112   3      128       32        0.038        0.056            1.491         72.050\n",
      " 4    1     3  256   3      128       32        0.066        0.098            1.489         71.356\n",
      " 1    3     8  256   3      128       32        0.036        0.053            1.465         97.629\n",
      " 4    8    16  112   5      128       32        0.280        0.409            1.459         71.579\n",
      " 2    3     3  112   5      128       32        0.047        0.067            1.425         80.172\n",
      " 2   32    32  112   1      128       32        0.038        0.054            1.424         42.295\n",
      " 2    3     3 1024   1      128       32        0.293        0.411            1.402         42.896\n",
      " 2    1     3  512   3      128       32        0.117        0.162            1.392         81.001\n",
      " 4    1     3  112   5      128       32        0.052        0.071            1.377         48.502\n",
      " 2    1     3  112   9      128       32        0.053        0.073            1.369         76.018\n",
      " 1    1     3  512   3      128       32        0.060        0.078            1.301         78.948\n",
      " 2    3     8  512   1      128       32        0.071        0.092            1.287         44.220\n",
      " 2    1     3 1024   3      128       32        0.432        0.549            1.271         87.454\n",
      " 1    3     8  512   1      128       32        0.037        0.043            1.161         42.195\n",
      " 4    1     3  112   1      128       32        0.046        0.054            1.158          2.167\n",
      " 1    1     3 1024   3      128       32        0.210        0.240            1.143         89.838\n",
      " 1    1     3  256   5      128       32        0.052        0.059            1.130         62.813\n",
      " 4    1     3 1024   3      128       32        0.909        1.024            1.127         83.036\n",
      " 1   32    32  256   1      128       32        0.129        0.139            1.075         32.407\n",
      " 4    1     3  512   3      128       32        0.242        0.256            1.058         78.013\n",
      " 1    8    16  112   3      128       32        0.037        0.039            1.035         48.562\n",
      " 4    3     3  112   3      128       32        0.050        0.051            1.031         54.402\n",
      " 4    1     3  512   1      128       32        0.047        0.049            1.031         44.305\n",
      " 2    1     3  112   3      128       32        0.043        0.044            1.006         10.424\n",
      " 1    3     3  512   1      128       32        0.051        0.051            1.005         30.731\n",
      " 4    3     8  112   1      128       32        0.036        0.036            0.999          8.429\n",
      " 4    3     8  256   1      128       32        0.051        0.051            0.984         30.560\n",
      " 2    1     3  112   7      128       32        0.052        0.050            0.953         47.273\n",
      " 2    8    16  256   1      128       32        0.071        0.068            0.952         29.561\n",
      " 2    1     3  256   3      128       32        0.051        0.048            0.941         45.942\n",
      " 2    3     3  112   3      128       32        0.039        0.037            0.939         34.593\n",
      " 4    1     3 1024   1      128       32        0.196        0.179            0.912         42.743\n",
      " 1    3     8  112   5      128       32        0.037        0.033            0.893         50.243\n",
      " 4    3     3  112   1      128       32        0.051        0.044            0.857          5.879\n",
      " 2    1     3  112   5      128       32        0.051        0.044            0.856         24.358\n",
      " 2    3     8  112   3      128       32        0.036        0.031            0.851         37.199\n",
      " 1    1     3 1024   1      128       32        0.059        0.049            0.832         35.650\n",
      " 4    8    16  112   1      128       32        0.038        0.031            0.828         21.265\n",
      " 1    8    16  256   1      128       32        0.053        0.041            0.771         19.650\n",
      " 1    3     8  112   3      128       32        0.038        0.029            0.763         17.847\n",
      " 2    1     3 1024   1      128       32        0.105        0.080            0.762         40.062\n",
      " 1    3     3  112   5      128       32        0.051        0.039            0.755         36.760\n",
      " 1    3     3  112   3      128       32        0.050        0.037            0.742         13.461\n",
      " 1    1     3  112   3      128       32        0.050        0.037            0.737          4.511\n",
      " 1    1     3  112   9      128       32        0.053        0.039            0.737         38.339\n",
      " 1    1     3  112   7      128       32        0.051        0.037            0.733         24.326\n",
      " 1    1     3  112   5      128       32        0.051        0.037            0.731         12.408\n",
      " 1    1     3  256   3      128       32        0.053        0.039            0.731         22.294\n",
      " 2    8    16  112   1      128       32        0.045        0.032            0.715          9.007\n",
      " 1    3     3  256   3      128       32        0.073        0.050            0.677         48.347\n",
      " 2    3     3  256   1      128       32        0.055        0.035            0.633         14.265\n",
      " 2    3     8  112   1      128       32        0.040        0.024            0.611          3.788\n",
      " 1    3     3  256   1      128       32        0.079        0.048            0.608          4.959\n",
      " 1   32    32  112   1      128       32        0.052        0.031            0.607         15.563\n",
      " 1    3     8  112   1      128       32        0.038        0.023            0.596          1.981\n",
      " 1    3     3  112   1      128       32        0.051        0.030            0.593          1.466\n",
      " 1    3     8  256   1      128       32        0.038        0.023            0.588         10.275\n",
      " 4    1     3  112   3      128       32        0.086        0.050            0.582         10.508\n",
      " 4    1     3  256   1      128       32        0.037        0.021            0.566         14.069\n",
      " 1    8    16  112   1      128       32        0.039        0.022            0.553          5.156\n",
      " 2    1     3  112   1      128       32        0.040        0.021            0.521          1.250\n",
      " 2    3     8  256   1      128       32        0.067        0.034            0.517         11.798\n",
      " 2    1     3  256   1      128       32        0.060        0.024            0.409          4.403\n",
      " 2    1     3  512   1      128       32        0.109        0.038            0.350          9.603\n",
      " 1    1     3  112   1      128       32        0.051        0.017            0.325          0.488\n",
      " 2    3     3  112   1      128       32        0.093        0.030            0.323          1.617\n",
      " 1    1     3  512   1      128       32        0.048        0.014            0.298         10.924\n",
      " 1    1     3  256   1      128       32        0.065        0.017            0.269          2.032\n",
      " 1    1     3  112  11      128       32        0.259        0.064            0.248         11.731\n"
     ]
    }
   ],
   "source": [
    "df_global = benchmark_img2col_best_tiles(\n",
    "    best_tiles,\n",
    "    image_sizes=(112, 256, 512, 1024),\n",
    "    batch_sizes=(1, 2, 4),\n",
    "    channel_pairs=((1, 3), (3, 3), (3, 8), (8, 16), (32, 32)),\n",
    "    kernels=(1, 3, 5, 7, 9, 11),\n",
    "    iters_per_shape=500,\n",
    "    device=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cc84a36-4b5e-4c62-a464-4108da1aa289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Cin</th>\n",
       "      <th>H</th>\n",
       "      <th>W</th>\n",
       "      <th>Kh</th>\n",
       "      <th>Kw</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_unfold_ms</th>\n",
       "      <th>speed_vs_unfold</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_unfold_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "      <th>Cout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.448237</td>\n",
       "      <td>7.392587</td>\n",
       "      <td>16.492597</td>\n",
       "      <td>336.864315</td>\n",
       "      <td>20.425183</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.513139</td>\n",
       "      <td>8.459648</td>\n",
       "      <td>16.486076</td>\n",
       "      <td>523.124304</td>\n",
       "      <td>31.731280</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.136437</td>\n",
       "      <td>2.006526</td>\n",
       "      <td>14.706562</td>\n",
       "      <td>491.865300</td>\n",
       "      <td>33.445296</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.263149</td>\n",
       "      <td>3.779029</td>\n",
       "      <td>14.360815</td>\n",
       "      <td>510.045315</td>\n",
       "      <td>35.516461</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.073240</td>\n",
       "      <td>0.987524</td>\n",
       "      <td>13.483335</td>\n",
       "      <td>458.141601</td>\n",
       "      <td>33.978359</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N  Cin     H     W  Kh  Kw  BLOCK_M  BLOCK_K  num_warps  num_stages  \\\n",
       "191  1   32   512   512   3   3      128       32          2           3   \n",
       "262  4   32  1024  1024   1   1      128       32          2           3   \n",
       "226  4   32   512   512   1   1      128       32          2           3   \n",
       "253  2   32  1024  1024   1   1      128       32          2           3   \n",
       "209  2   32   512   512   1   1      128       32          2           3   \n",
       "\n",
       "     t_triton_ms  t_unfold_ms  speed_vs_unfold  bw_triton_GBs  bw_unfold_GBs  \\\n",
       "191     0.448237     7.392587        16.492597     336.864315      20.425183   \n",
       "262     0.513139     8.459648        16.486076     523.124304      31.731280   \n",
       "226     0.136437     2.006526        14.706562     491.865300      33.445296   \n",
       "253     0.263149     3.779029        14.360815     510.045315      35.516461   \n",
       "209     0.073240     0.987524        13.483335     458.141601      33.978359   \n",
       "\n",
       "     max_abs_err  Cout  \n",
       "191            0    32  \n",
       "262            0    32  \n",
       "226            0    32  \n",
       "253            0    32  \n",
       "209            0    32  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global.sort_values(\"speed_vs_unfold\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a9246-6bb0-4207-bf0f-fe39719b63fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
