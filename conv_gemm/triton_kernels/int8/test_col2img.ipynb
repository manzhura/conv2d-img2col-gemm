{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31843a0d-7d7d-4ab6-8af8-15983e4a2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from col2img_int8_kernel import col2img_int32_kernel\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e9f03-300c-42bf-adc8-76f1ef88ad3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6ded5a-de1b-46d0-812b-3723f9126125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Обёртка над col2img_int32_kernel\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def col2img_int32(\n",
    "    cols_i32: torch.Tensor,  # [M, K] int32, M = N*Ho*Wo, K = Cin*Kh*Kw\n",
    "    N: int, Cin: int, H: int, W: int,\n",
    "    Kh: int, Kw: int,\n",
    "    Sh: int, Sw: int,\n",
    "    Ph: int, Pw: int,\n",
    "    Dh: int, Dw: int,\n",
    "    BLOCK_M: int,\n",
    "    BLOCK_K: int,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Обёртка для col2img_int32_kernel.\n",
    "    Восстанавливает x_i32: [N, Cin, H, W] из cols_i32[int32][M,K].\n",
    "    \"\"\"\n",
    "    assert cols_i32.is_cuda\n",
    "    assert cols_i32.dtype == torch.int32\n",
    "    cols_i32 = cols_i32.contiguous()\n",
    "\n",
    "    # считаем Ho, Wo так же, как в img2col\n",
    "    Ho = (H + 2 * Ph - Dh * (Kh - 1) - 1) // Sh + 1\n",
    "    Wo = (W + 2 * Pw - Dw * (Kw - 1) - 1) // Sw + 1\n",
    "    M = N * Ho * Wo\n",
    "    K = Cin * Kh * Kw\n",
    "\n",
    "    assert cols_i32.shape == (M, K), f\"cols shape {cols_i32.shape}, expected {(M, K)}\"\n",
    "\n",
    "    x_i32 = torch.zeros((N, Cin, H, W), device=cols_i32.device, dtype=torch.int32)\n",
    "    sN, sC, sH, sW = x_i32.stride()\n",
    "\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(K, BLOCK_K))\n",
    "\n",
    "    col2img_int32_kernel[grid](\n",
    "        cols_i32, x_i32,\n",
    "        N, Cin, H, W,\n",
    "        Kh, Kw, Sh, Sw, Ph, Pw, Dh, Dw,\n",
    "        Ho, Wo,\n",
    "        sN, sC, sH, sW,\n",
    "        K,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return x_i32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53dc111-7f76-4036-9b86-275f6eaa0fc9",
   "metadata": {},
   "source": [
    "# title search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c2527-e976-430a-ac77-334d30c0e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_col2img_int32_tiles_for_shape(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Перебираем BLOCK_M / BLOCK_K / num_warps / num_stages\n",
    "    для одного фиксированного shape, возвращаем DataFrame\n",
    "    со всеми результатами.\n",
    "    Ты потом вручную выберешь лучшие тайлы.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for BM in blocks_M:\n",
    "        for BK in blocks_K:\n",
    "            for W_ in warps:\n",
    "                for S_ in stages:\n",
    "                    try:\n",
    "                        rec = bench_once_col2img_vs_fold(\n",
    "                            N, Cin, H, W,\n",
    "                            Kh, Kw,\n",
    "                            Sh, Sw,\n",
    "                            Ph, Pw,\n",
    "                            Dh, Dw,\n",
    "                            BLOCK_M=BM,\n",
    "                            BLOCK_K=BK,\n",
    "                            num_warps=W_,\n",
    "                            num_stages=S_,\n",
    "                            iters=iters,\n",
    "                            device=device,\n",
    "                        )\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"[SKIP] BM={BM}, BK={BK}, W={W_}, S={S_}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    print(\n",
    "                        f\"BM={BM}, BK={BK}, W={W_}, S={S_}: \"\n",
    "                        f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                        f\"speed_vs_fold={rec['speed_vs_fold']:.3f}x, \"\n",
    "                        f\"err={rec['max_abs_err']}\"\n",
    "                    )\n",
    "                    records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this col2img_int32 shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    # можно сразу отсортировать по времени, чтобы топ был вверху\n",
    "    df = df.sort_values(\"t_triton_ms\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921dab21-0270-4106-9b4c-cddd7a22b78c",
   "metadata": {},
   "source": [
    "# bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef77636d-4978-4b29-9a4e-64a6a68ca3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Бенч ОДНОГО shape: col2img_int32 vs F.fold\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def bench_once_col2img_vs_fold(\n",
    "    N, Cin, H, W,\n",
    "    Kh, Kw,\n",
    "    Sh, Sw,\n",
    "    Ph, Pw,\n",
    "    Dh, Dw,\n",
    "    BLOCK_M,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч для ОДНОГО shape и ОДНОГО набора тайлов:\n",
    "      - генерим cols_i32 [M,K] int32\n",
    "      - наш col2img_int32 -> x_i32 [N,Cin,H,W]\n",
    "      - реф: F.fold (fp16) -> x_ref -> int32\n",
    "      - меряем:\n",
    "        t_triton_ms, t_fold_ms,\n",
    "        speed_vs_fold,\n",
    "        max_abs_err,\n",
    "        bw_triton_GBs, bw_fold_GBs\n",
    "    \"\"\"\n",
    "    # считаем Ho, Wo\n",
    "    Ho = (H + 2 * Ph - Dh * (Kh - 1) - 1) // Sh + 1\n",
    "    Wo = (W + 2 * Pw - Dw * (Kw - 1) - 1) // Sw + 1\n",
    "    if Ho <= 0 or Wo <= 0:\n",
    "        raise RuntimeError(f\"Invalid Ho/Wo: Ho={Ho}, Wo={Wo}\")\n",
    "\n",
    "    M = N * Ho * Wo\n",
    "    K = Cin * Kh * Kw\n",
    "\n",
    "    # случайные int32 \"колонки\"\n",
    "    cols_i32 = torch.randint(\n",
    "        low=-1000, high=1000,\n",
    "        size=(M, K),\n",
    "        device=device,\n",
    "        dtype=torch.int32,\n",
    "    )\n",
    "\n",
    "    # ---------- наш col2img_int32 ----------\n",
    "    def _call_triton():\n",
    "        x_i32 = col2img_int32(\n",
    "            cols_i32,\n",
    "            N, Cin, H, W,\n",
    "            Kh, Kw,\n",
    "            Sh, Sw,\n",
    "            Ph, Pw,\n",
    "            Dh, Dw,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return x_i32\n",
    "\n",
    "    # прогрев\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        x_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # ---------- референс через F.fold ----------\n",
    "    # cols_i32: [M,K] -> [N, L, K] -> [N, K, L]\n",
    "    L = Ho * Wo\n",
    "    assert M == N * L\n",
    "    cols_view = cols_i32.view(N, L, K)          # [N,L,K]\n",
    "    inp_fold = cols_view.permute(0, 2, 1).contiguous()  # [N,K,L]\n",
    "\n",
    "    inp_fold_f = inp_fold.float()\n",
    "    # прогрев\n",
    "    def _call_fold():\n",
    "        x_f = F.fold(\n",
    "            inp_fold_f,\n",
    "            output_size=(H, W),\n",
    "            kernel_size=(Kh, Kw),\n",
    "            dilation=(Dh, Dw),\n",
    "            padding=(Ph, Pw),\n",
    "            stride=(Sh, Sw),\n",
    "        )  # [N,Cin,H,W] fp16\n",
    "        return x_f\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_fold()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        x_ref_f = _call_fold()\n",
    "    torch.cuda.synchronize()\n",
    "    t_fold = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    x_ref_i32 = x_ref_f.to(torch.int32)\n",
    "\n",
    "    # ---------- ошибка ----------\n",
    "    max_abs_err = (x_i32 - x_ref_i32).abs().max().item()\n",
    "\n",
    "    # ---------- BW (грубо) ----------\n",
    "    # читаем cols (int32) и пишем x (int32)\n",
    "    bytes_moved = cols_i32.numel() * 4 + x_i32.numel() * 4\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_fold = bytes_moved / t_fold / 1e9\n",
    "\n",
    "    return {\n",
    "        \"N\": N,\n",
    "        \"Cin\": Cin,\n",
    "        \"H\": H, \"W\": W,\n",
    "        \"Kh\": Kh, \"Kw\": Kw,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_fold_ms\": t_fold * 1e3,\n",
    "        \"speed_vs_fold\": t_fold / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_fold_GBs\": bw_fold,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b6dd45-8f18-452d-aa18-5e3484754cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9e50e8-e1c9-4bff-8f48-a8c884502225",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def benchmark_col2img_int32_best_tiles(\n",
    "    tiles_cfg: dict,\n",
    "    image_sizes=(112, 256, 512, 1024),\n",
    "    batch_sizes=(1, 2, 4),\n",
    "    channel_pairs=((1, 3), (3, 3), (3, 8), (8, 16), (32, 32)),\n",
    "    kernels=(1, 3, 5, 7, 9, 11),\n",
    "    iters_per_shape: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Сравнение col2img_int32 vs F.fold на сетке параметров\n",
    "    для фиксированных лучших тайлов tiles_cfg.\n",
    "\n",
    "    tiles_cfg:\n",
    "      - \"BLOCK_M\"\n",
    "      - \"BLOCK_K\"\n",
    "      - \"num_warps\"\n",
    "      - \"num_stages\"\n",
    "\n",
    "    channel_pairs: (Cin, Cout) — Cout используется только для логов\n",
    "    (по аналогии с img2col).\n",
    "    \"\"\"\n",
    "    BM = tiles_cfg[\"BLOCK_M\"]\n",
    "    BK = tiles_cfg[\"BLOCK_K\"]\n",
    "    NW = tiles_cfg[\"num_warps\"]\n",
    "    NS = tiles_cfg[\"num_stages\"]\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for H in image_sizes:\n",
    "        W = H\n",
    "        for N in batch_sizes:\n",
    "            for (Cin, Cout) in channel_pairs:\n",
    "                for k in kernels:\n",
    "                    Ph = Pw = k // 2\n",
    "                    print(f\"=== SHAPE: N={N}, Cin={Cin}, Cout={Cout}, H=W={H}, k={k}x{k} ===\")\n",
    "                    try:\n",
    "                        rec = bench_once_col2img_vs_fold(\n",
    "                            N, Cin, H, W,\n",
    "                            k, k,\n",
    "                            1, 1,       # Sh, Sw\n",
    "                            Ph, Pw,     # padding\n",
    "                            1, 1,       # Dh, Dw\n",
    "                            BLOCK_M=BM,\n",
    "                            BLOCK_K=BK,\n",
    "                            num_warps=NW,\n",
    "                            num_stages=NS,\n",
    "                            iters=iters_per_shape,\n",
    "                            device=device,\n",
    "                        )\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"[SKIP] N={N}, Cin={Cin}, Cout={Cout}, H={H}, k={k}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    if rec[\"max_abs_err\"] != 0:\n",
    "                        print(f\"[WRONG] N={N}, Cin={Cin}, Cout={Cout}, H={H}, k={k}, err={rec['max_abs_err']}\")\n",
    "                        continue\n",
    "\n",
    "                    rec[\"Cout\"] = Cout\n",
    "                    records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        print(\"[WARN] no successful records for col2img_int32\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd33acc-d9e0-4569-9b03-476136096d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BK=32, W=2, S=2: t_triton=2.340 ms, speed_vs_fold=1.159x, err=0\n",
      "BM=32, BK=32, W=2, S=3: t_triton=2.287 ms, speed_vs_fold=1.198x, err=0\n",
      "BM=32, BK=32, W=4, S=2: t_triton=1.471 ms, speed_vs_fold=1.864x, err=0\n",
      "BM=32, BK=32, W=4, S=3: t_triton=1.454 ms, speed_vs_fold=1.883x, err=0\n",
      "BM=32, BK=32, W=8, S=2: t_triton=1.566 ms, speed_vs_fold=1.802x, err=0\n",
      "BM=32, BK=32, W=8, S=3: t_triton=1.563 ms, speed_vs_fold=1.757x, err=0\n",
      "BM=32, BK=64, W=2, S=2: t_triton=4.805 ms, speed_vs_fold=0.566x, err=0\n",
      "BM=32, BK=64, W=2, S=3: t_triton=4.771 ms, speed_vs_fold=0.579x, err=0\n",
      "BM=32, BK=64, W=4, S=2: t_triton=2.355 ms, speed_vs_fold=1.182x, err=0\n",
      "BM=32, BK=64, W=4, S=3: t_triton=2.424 ms, speed_vs_fold=1.131x, err=0\n",
      "BM=32, BK=64, W=8, S=2: t_triton=1.502 ms, speed_vs_fold=1.826x, err=0\n",
      "BM=32, BK=64, W=8, S=3: t_triton=1.491 ms, speed_vs_fold=1.815x, err=0\n",
      "BM=32, BK=128, W=2, S=2: t_triton=5.348 ms, speed_vs_fold=0.513x, err=0\n",
      "BM=32, BK=128, W=2, S=3: t_triton=5.417 ms, speed_vs_fold=0.515x, err=0\n",
      "BM=32, BK=128, W=4, S=2: t_triton=4.924 ms, speed_vs_fold=0.557x, err=0\n",
      "BM=32, BK=128, W=4, S=3: t_triton=4.800 ms, speed_vs_fold=0.572x, err=0\n",
      "BM=32, BK=128, W=8, S=2: t_triton=2.997 ms, speed_vs_fold=0.908x, err=0\n",
      "BM=32, BK=128, W=8, S=3: t_triton=3.041 ms, speed_vs_fold=0.890x, err=0\n",
      "BM=64, BK=32, W=2, S=2: t_triton=3.347 ms, speed_vs_fold=0.837x, err=0\n",
      "BM=64, BK=32, W=2, S=3: t_triton=3.288 ms, speed_vs_fold=0.829x, err=0\n",
      "BM=64, BK=32, W=4, S=2: t_triton=2.304 ms, speed_vs_fold=1.188x, err=0\n",
      "BM=64, BK=32, W=4, S=3: t_triton=2.308 ms, speed_vs_fold=1.192x, err=0\n",
      "BM=64, BK=32, W=8, S=2: t_triton=1.531 ms, speed_vs_fold=1.784x, err=0\n",
      "BM=64, BK=32, W=8, S=3: t_triton=1.489 ms, speed_vs_fold=1.868x, err=0\n",
      "BM=64, BK=64, W=2, S=2: t_triton=5.706 ms, speed_vs_fold=0.497x, err=0\n",
      "BM=64, BK=64, W=2, S=3: t_triton=5.642 ms, speed_vs_fold=0.483x, err=0\n",
      "BM=64, BK=64, W=4, S=2: t_triton=4.787 ms, speed_vs_fold=0.576x, err=0\n",
      "BM=64, BK=64, W=4, S=3: t_triton=4.766 ms, speed_vs_fold=0.572x, err=0\n",
      "BM=64, BK=64, W=8, S=2: t_triton=3.035 ms, speed_vs_fold=0.909x, err=0\n",
      "BM=64, BK=64, W=8, S=3: t_triton=3.032 ms, speed_vs_fold=0.906x, err=0\n",
      "BM=64, BK=128, W=2, S=2: t_triton=20.797 ms, speed_vs_fold=0.131x, err=0\n",
      "BM=64, BK=128, W=2, S=3: t_triton=20.826 ms, speed_vs_fold=0.131x, err=0\n",
      "BM=64, BK=128, W=4, S=2: t_triton=5.179 ms, speed_vs_fold=0.520x, err=0\n",
      "BM=64, BK=128, W=4, S=3: t_triton=5.117 ms, speed_vs_fold=0.525x, err=0\n",
      "BM=64, BK=128, W=8, S=2: t_triton=5.463 ms, speed_vs_fold=0.500x, err=0\n",
      "BM=64, BK=128, W=8, S=3: t_triton=5.515 ms, speed_vs_fold=0.487x, err=0\n",
      "BM=128, BK=32, W=2, S=2: t_triton=4.702 ms, speed_vs_fold=0.570x, err=0\n",
      "BM=128, BK=32, W=2, S=3: t_triton=4.689 ms, speed_vs_fold=0.579x, err=0\n",
      "BM=128, BK=32, W=4, S=2: t_triton=4.592 ms, speed_vs_fold=0.596x, err=0\n",
      "BM=128, BK=32, W=4, S=3: t_triton=4.522 ms, speed_vs_fold=0.603x, err=0\n",
      "BM=128, BK=32, W=8, S=2: t_triton=2.807 ms, speed_vs_fold=0.971x, err=0\n",
      "BM=128, BK=32, W=8, S=3: t_triton=2.821 ms, speed_vs_fold=0.975x, err=0\n",
      "BM=128, BK=64, W=2, S=2: t_triton=12.449 ms, speed_vs_fold=0.229x, err=0\n",
      "BM=128, BK=64, W=2, S=3: t_triton=12.407 ms, speed_vs_fold=0.221x, err=0\n",
      "BM=128, BK=64, W=4, S=2: t_triton=5.596 ms, speed_vs_fold=0.484x, err=0\n",
      "BM=128, BK=64, W=4, S=3: t_triton=5.491 ms, speed_vs_fold=0.493x, err=0\n",
      "BM=128, BK=64, W=8, S=2: t_triton=5.176 ms, speed_vs_fold=0.521x, err=0\n",
      "BM=128, BK=64, W=8, S=3: t_triton=5.177 ms, speed_vs_fold=0.524x, err=0\n",
      "BM=128, BK=128, W=2, S=2: t_triton=29.010 ms, speed_vs_fold=0.094x, err=0\n",
      "BM=128, BK=128, W=2, S=3: t_triton=29.656 ms, speed_vs_fold=0.092x, err=0\n",
      "BM=128, BK=128, W=4, S=2: t_triton=21.345 ms, speed_vs_fold=0.129x, err=0\n",
      "BM=128, BK=128, W=4, S=3: t_triton=21.289 ms, speed_vs_fold=0.128x, err=0\n",
      "BM=128, BK=128, W=8, S=2: t_triton=5.724 ms, speed_vs_fold=0.478x, err=0\n",
      "BM=128, BK=128, W=8, S=3: t_triton=5.725 ms, speed_vs_fold=0.474x, err=0\n",
      "   N  Cin    H    W  Kh  Kw  BLOCK_M  BLOCK_K  num_warps  num_stages  \\\n",
      "0  4   64  224  224   3   3       32       32          4           3   \n",
      "1  4   64  224  224   3   3       32       32          4           2   \n",
      "2  4   64  224  224   3   3       64       32          8           3   \n",
      "3  4   64  224  224   3   3       32       64          8           3   \n",
      "4  4   64  224  224   3   3       32       64          8           2   \n",
      "5  4   64  224  224   3   3       64       32          8           2   \n",
      "6  4   64  224  224   3   3       32       32          8           3   \n",
      "7  4   64  224  224   3   3       32       32          8           2   \n",
      "8  4   64  224  224   3   3       32       32          2           3   \n",
      "9  4   64  224  224   3   3       64       32          4           2   \n",
      "\n",
      "   t_triton_ms  t_fold_ms  speed_vs_fold  bw_triton_GBs  bw_fold_GBs  \\\n",
      "0     1.454050   2.737633       1.882764     353.359325   187.681170   \n",
      "1     1.470514   2.740385       1.863555     349.403066   187.492748   \n",
      "2     1.488509   2.779814       1.867516     345.179194   184.833319   \n",
      "3     1.490962   2.706579       1.815324     344.611229   189.834581   \n",
      "4     1.502034   2.742225       1.825674     342.070910   187.366899   \n",
      "5     1.530844   2.730468       1.783636     335.633254   188.173661   \n",
      "6     1.562804   2.746174       1.757210     328.769548   187.097474   \n",
      "7     1.566289   2.823060       1.802388     328.038034   182.001881   \n",
      "8     2.286667   2.739539       1.198049     224.694822   187.550633   \n",
      "9     2.303792   2.736450       1.187803     223.024608   187.762355   \n",
      "\n",
      "   max_abs_err  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "5            0  \n",
      "6            0  \n",
      "7            0  \n",
      "8            0  \n",
      "9            0  \n"
     ]
    }
   ],
   "source": [
    "# 1) Тюним тайлы для одного shape, например:\n",
    "df_tiles = tune_col2img_int32_tiles_for_shape(\n",
    "    N=4, Cin=64, H=224, W=224,\n",
    "    Kh=3, Kw=3,\n",
    "    Sh=1, Sw=1,\n",
    "    Ph=1, Pw=1,\n",
    "    Dh=1, Dw=1,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=50,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# смотри top-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad86652-5c84-49b5-a37b-2db647c8a9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Cin</th>\n",
       "      <th>H</th>\n",
       "      <th>W</th>\n",
       "      <th>Kh</th>\n",
       "      <th>Kw</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_fold_ms</th>\n",
       "      <th>speed_vs_fold</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_fold_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.454050</td>\n",
       "      <td>2.737633</td>\n",
       "      <td>1.882764</td>\n",
       "      <td>353.359325</td>\n",
       "      <td>187.681170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.470514</td>\n",
       "      <td>2.740385</td>\n",
       "      <td>1.863555</td>\n",
       "      <td>349.403066</td>\n",
       "      <td>187.492748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.488509</td>\n",
       "      <td>2.779814</td>\n",
       "      <td>1.867516</td>\n",
       "      <td>345.179194</td>\n",
       "      <td>184.833319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.490962</td>\n",
       "      <td>2.706579</td>\n",
       "      <td>1.815324</td>\n",
       "      <td>344.611229</td>\n",
       "      <td>189.834581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.502034</td>\n",
       "      <td>2.742225</td>\n",
       "      <td>1.825674</td>\n",
       "      <td>342.070910</td>\n",
       "      <td>187.366899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.530844</td>\n",
       "      <td>2.730468</td>\n",
       "      <td>1.783636</td>\n",
       "      <td>335.633254</td>\n",
       "      <td>188.173661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.562804</td>\n",
       "      <td>2.746174</td>\n",
       "      <td>1.757210</td>\n",
       "      <td>328.769548</td>\n",
       "      <td>187.097474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.566289</td>\n",
       "      <td>2.823060</td>\n",
       "      <td>1.802388</td>\n",
       "      <td>328.038034</td>\n",
       "      <td>182.001881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.286667</td>\n",
       "      <td>2.739539</td>\n",
       "      <td>1.198049</td>\n",
       "      <td>224.694822</td>\n",
       "      <td>187.550633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.303792</td>\n",
       "      <td>2.736450</td>\n",
       "      <td>1.187803</td>\n",
       "      <td>223.024608</td>\n",
       "      <td>187.762355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N  Cin    H    W  Kh  Kw  BLOCK_M  BLOCK_K  num_warps  num_stages  \\\n",
       "0  4   64  224  224   3   3       32       32          4           3   \n",
       "1  4   64  224  224   3   3       32       32          4           2   \n",
       "2  4   64  224  224   3   3       64       32          8           3   \n",
       "3  4   64  224  224   3   3       32       64          8           3   \n",
       "4  4   64  224  224   3   3       32       64          8           2   \n",
       "5  4   64  224  224   3   3       64       32          8           2   \n",
       "6  4   64  224  224   3   3       32       32          8           3   \n",
       "7  4   64  224  224   3   3       32       32          8           2   \n",
       "8  4   64  224  224   3   3       32       32          2           3   \n",
       "9  4   64  224  224   3   3       64       32          4           2   \n",
       "\n",
       "   t_triton_ms  t_fold_ms  speed_vs_fold  bw_triton_GBs  bw_fold_GBs  \\\n",
       "0     1.454050   2.737633       1.882764     353.359325   187.681170   \n",
       "1     1.470514   2.740385       1.863555     349.403066   187.492748   \n",
       "2     1.488509   2.779814       1.867516     345.179194   184.833319   \n",
       "3     1.490962   2.706579       1.815324     344.611229   189.834581   \n",
       "4     1.502034   2.742225       1.825674     342.070910   187.366899   \n",
       "5     1.530844   2.730468       1.783636     335.633254   188.173661   \n",
       "6     1.562804   2.746174       1.757210     328.769548   187.097474   \n",
       "7     1.566289   2.823060       1.802388     328.038034   182.001881   \n",
       "8     2.286667   2.739539       1.198049     224.694822   187.550633   \n",
       "9     2.303792   2.736450       1.187803     223.024608   187.762355   \n",
       "\n",
       "   max_abs_err  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2900af94-a15f-4de3-8090-b9eeece789c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_COL2IMG_INT32_TILES = {\n",
    "    \"BLOCK_M\": 32,\n",
    "    \"BLOCK_K\": 32,\n",
    "    \"num_warps\": 4,\n",
    "    \"num_stages\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98266231-4bad-462a-93e9-5ffce56fea6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=112, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=9x9 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=256, k=9: CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 7.68 GiB of which 434.25 MiB is free. Including non-PyTorch memory, this process has 5.51 GiB memory in use. Of the allocated memory 5.08 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=256, k=11x11 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=256, k=11: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 4.23 GiB memory in use. Of the allocated memory 3.80 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=256, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=7x7 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=7: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 661.62 MiB is free. Including non-PyTorch memory, this process has 5.74 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 704.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=9x9 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 5.52 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=256, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=256, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.23 GiB is free. Including non-PyTorch memory, this process has 4.24 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=7x7 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=7: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 882.69 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 3.09 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=9x9 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 842.56 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 2.56 GiB is allocated by PyTorch, and 2.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=512, k=11x11 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=512, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 920.12 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 936.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=9x9 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=512, k=9: CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 7.68 GiB of which 543.06 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 2.55 GiB is allocated by PyTorch, and 2.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=512, k=11x11 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=512, k=11: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.68 GiB of which 443.19 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 3.80 GiB is allocated by PyTorch, and 952.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=5x5 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=5: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 338.25 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 3.19 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=7x7 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=7: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 4.21 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 672.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=9x9 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=9: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=512, k=11x11 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=512, k=11: CUDA out of memory. Tried to allocate 7.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=512, k=11x11 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=9x9 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=512, k=11: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 7.68 GiB of which 568.25 MiB is free. Including non-PyTorch memory, this process has 6.11 GiB memory in use. Of the allocated memory 5.68 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=512, k=9: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 497.88 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=512, k=11: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 7.68 GiB of which 483.88 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=5x5 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=5: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 447.94 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.16 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=7x7 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=7: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.40 GiB is free. Including non-PyTorch memory, this process has 5.05 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 855.31 MiB is free. Including non-PyTorch memory, this process has 5.52 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=512, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 4.24 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=3x3 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=3: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 5.34 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 288.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=5x5 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=5: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.14 GiB is free. Including non-PyTorch memory, this process has 4.21 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 544.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=7x7 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=7: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.91 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=9x9 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=9: CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.91 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=512, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=512, k=11: CUDA out of memory. Tried to allocate 15.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.90 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=1, Cout=3, H=W=1024, k=11x11 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=9x9 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=3, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 7.68 GiB of which 341.50 MiB is free. Including non-PyTorch memory, this process has 6.11 GiB memory in use. Of the allocated memory 5.68 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=3, Cout=8, H=1024, k=9: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 167.81 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=3, Cout=8, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=3, Cout=8, H=1024, k=11: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 7.68 GiB of which 351.56 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=5: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 289.56 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.16 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=7: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 5.05 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=9: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 7.68 GiB of which 946.56 MiB is free. Including non-PyTorch memory, this process has 5.52 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=8, Cout=16, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=8, Cout=16, H=1024, k=11: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.31 GiB is free. Including non-PyTorch memory, this process has 4.24 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=3: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.17 GiB is free. Including non-PyTorch memory, this process has 5.34 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 288.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=5: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.19 GiB is free. Including non-PyTorch memory, this process has 4.21 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 544.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=7: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.97 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=9: CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.97 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=1, Cin=32, Cout=32, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=1, Cin=32, Cout=32, H=1024, k=11: CUDA out of memory. Tried to allocate 15.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 5.97 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=9x9 ===\n",
      "=== SHAPE: N=2, Cin=1, Cout=3, H=W=1024, k=11x11 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=7: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 5.05 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.85 GiB is free. Including non-PyTorch memory, this process has 4.25 GiB memory in use. Of the allocated memory 3.82 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=3, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 2.84 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.74 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=7: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 7.68 GiB of which 884.44 MiB is free. Including non-PyTorch memory, this process has 5.00 GiB memory in use. Of the allocated memory 3.47 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=9: CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacity of 7.68 GiB of which 899.94 MiB is free. Including non-PyTorch memory, this process has 5.75 GiB memory in use. Of the allocated memory 3.82 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=3, Cout=8, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=3, Cout=8, H=1024, k=11: CUDA out of memory. Tried to allocate 2.84 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.85 GiB is free. Including non-PyTorch memory, this process has 3.85 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 576.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=5: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 5.41 GiB memory in use. Of the allocated memory 3.19 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=7: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 3.10 GiB is free. Including non-PyTorch memory, this process has 3.55 GiB memory in use. Of the allocated memory 3.12 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=9: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.23 GiB is free. Including non-PyTorch memory, this process has 5.55 GiB memory in use. Of the allocated memory 5.12 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=8, Cout=16, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=8, Cout=16, H=1024, k=11: CUDA out of memory. Tried to allocate 7.56 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.36 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=3: CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.56 GiB is free. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.75 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=5: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 46.31 MiB is free. Including non-PyTorch memory, this process has 6.68 GiB memory in use. Of the allocated memory 6.25 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=7: CUDA out of memory. Tried to allocate 12.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.29 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=9: CUDA out of memory. Tried to allocate 20.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.29 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=2, Cin=32, Cout=32, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=2, Cin=32, Cout=32, H=1024, k=11: CUDA out of memory. Tried to allocate 30.25 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.29 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=5x5 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=7x7 ===\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=1, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 7.68 GiB of which 390.62 MiB is free. Including non-PyTorch memory, this process has 6.26 GiB memory in use. Of the allocated memory 5.08 GiB is allocated by PyTorch, and 768.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=1, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=1, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.68 GiB of which 623.94 MiB is free. Including non-PyTorch memory, this process has 6.12 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=5: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 7.68 GiB of which 760.75 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=7: CUDA out of memory. Tried to allocate 2.30 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.73 GiB is free. Including non-PyTorch memory, this process has 5.07 GiB memory in use. Of the allocated memory 4.64 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=9: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 7.68 GiB of which 2.46 GiB is free. Including non-PyTorch memory, this process has 4.27 GiB memory in use. Of the allocated memory 3.84 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=3, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=3, H=1024, k=11: CUDA out of memory. Tried to allocate 5.67 GiB. GPU 0 has a total capacity of 7.68 GiB of which 490.56 MiB is free. Including non-PyTorch memory, this process has 6.15 GiB memory in use. Of the allocated memory 5.72 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=3x3 ===\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=5: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 7.68 GiB of which 467.25 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 4.73 GiB is allocated by PyTorch, and 960.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=7: CUDA out of memory. Tried to allocate 2.30 GiB. GPU 0 has a total capacity of 7.68 GiB of which 513.56 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 4.64 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=9: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 7.68 GiB of which 732.88 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.84 GiB is allocated by PyTorch, and 1.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=3, Cout=8, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=3, Cout=8, H=1024, k=11: CUDA out of memory. Tried to allocate 5.67 GiB. GPU 0 has a total capacity of 7.68 GiB of which 659.94 MiB is free. Including non-PyTorch memory, this process has 6.15 GiB memory in use. Of the allocated memory 5.72 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=3: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 760.19 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=5: CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 720.19 MiB is free. Including non-PyTorch memory, this process has 6.10 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=7: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 97.56 MiB is free. Including non-PyTorch memory, this process has 6.68 GiB memory in use. Of the allocated memory 6.25 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=9: CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.36 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=8, Cout=16, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=8, Cout=16, H=1024, k=11: CUDA out of memory. Tried to allocate 15.12 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.36 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=1x1 ===\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=3x3 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=3: CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 1.43 GiB is free. Including non-PyTorch memory, this process has 5.43 GiB memory in use. Of the allocated memory 5.00 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=5x5 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=5: CUDA out of memory. Tried to allocate 12.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.42 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=7x7 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=7: CUDA out of memory. Tried to allocate 24.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.43 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=9x9 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=9: CUDA out of memory. Tried to allocate 40.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.43 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "=== SHAPE: N=4, Cin=32, Cout=32, H=W=1024, k=11x11 ===\n",
      "[SKIP] N=4, Cin=32, Cout=32, H=1024, k=11: CUDA out of memory. Tried to allocate 60.50 GiB. GPU 0 has a total capacity of 7.68 GiB of which 6.44 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "df_global = benchmark_col2img_int32_best_tiles(\n",
    "    BEST_COL2IMG_INT32_TILES,\n",
    "    image_sizes=(112, 256, 512, 1024),\n",
    "    batch_sizes=(1, 2, 4),\n",
    "    channel_pairs=((1, 3), (3, 3), (3, 8), (8, 16), (32, 32)),\n",
    "    kernels=(1, 3, 5, 7, 9, 11),\n",
    "    iters_per_shape=50,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4637be62-0806-4fa7-b38d-51bdfe421635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Cin</th>\n",
       "      <th>H</th>\n",
       "      <th>W</th>\n",
       "      <th>Kh</th>\n",
       "      <th>Kw</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_fold_ms</th>\n",
       "      <th>speed_vs_fold</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_fold_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "      <th>Cout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.119208</td>\n",
       "      <td>0.287150</td>\n",
       "      <td>2.408815</td>\n",
       "      <td>350.198770</td>\n",
       "      <td>145.382150</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.076214</td>\n",
       "      <td>0.174012</td>\n",
       "      <td>2.283196</td>\n",
       "      <td>263.341795</td>\n",
       "      <td>115.339119</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.114009</td>\n",
       "      <td>0.255653</td>\n",
       "      <td>2.242389</td>\n",
       "      <td>280.517875</td>\n",
       "      <td>125.097771</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.337929</td>\n",
       "      <td>0.730580</td>\n",
       "      <td>2.161935</td>\n",
       "      <td>389.613981</td>\n",
       "      <td>180.215360</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104523</td>\n",
       "      <td>0.224446</td>\n",
       "      <td>2.147335</td>\n",
       "      <td>288.028748</td>\n",
       "      <td>134.133123</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.279496</td>\n",
       "      <td>0.592216</td>\n",
       "      <td>2.118872</td>\n",
       "      <td>240.106912</td>\n",
       "      <td>113.318283</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.193782</td>\n",
       "      <td>0.410003</td>\n",
       "      <td>2.115798</td>\n",
       "      <td>339.716727</td>\n",
       "      <td>160.561970</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103478</td>\n",
       "      <td>0.216384</td>\n",
       "      <td>2.091125</td>\n",
       "      <td>318.092819</td>\n",
       "      <td>152.115619</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.176070</td>\n",
       "      <td>0.367422</td>\n",
       "      <td>2.086800</td>\n",
       "      <td>309.684150</td>\n",
       "      <td>148.401459</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.236362</td>\n",
       "      <td>0.482924</td>\n",
       "      <td>2.043159</td>\n",
       "      <td>354.905764</td>\n",
       "      <td>173.704445</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.545701</td>\n",
       "      <td>5.130019</td>\n",
       "      <td>2.015170</td>\n",
       "      <td>342.701415</td>\n",
       "      <td>170.060817</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.284244</td>\n",
       "      <td>2.576421</td>\n",
       "      <td>2.006177</td>\n",
       "      <td>339.661017</td>\n",
       "      <td>169.307600</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.065782</td>\n",
       "      <td>0.129509</td>\n",
       "      <td>1.968781</td>\n",
       "      <td>250.187560</td>\n",
       "      <td>127.077400</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.062026</td>\n",
       "      <td>4.056882</td>\n",
       "      <td>1.967425</td>\n",
       "      <td>305.110343</td>\n",
       "      <td>155.081057</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.911701</td>\n",
       "      <td>1.781274</td>\n",
       "      <td>1.953792</td>\n",
       "      <td>368.041929</td>\n",
       "      <td>188.373180</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.061051</td>\n",
       "      <td>0.119199</td>\n",
       "      <td>1.952467</td>\n",
       "      <td>300.806183</td>\n",
       "      <td>154.064642</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.231124</td>\n",
       "      <td>4.339550</td>\n",
       "      <td>1.945006</td>\n",
       "      <td>240.628011</td>\n",
       "      <td>123.715790</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.524888</td>\n",
       "      <td>1.020308</td>\n",
       "      <td>1.943858</td>\n",
       "      <td>318.135922</td>\n",
       "      <td>163.662114</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.083698</td>\n",
       "      <td>0.162647</td>\n",
       "      <td>1.943256</td>\n",
       "      <td>294.948410</td>\n",
       "      <td>151.780541</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.461872</td>\n",
       "      <td>0.894917</td>\n",
       "      <td>1.937586</td>\n",
       "      <td>363.243481</td>\n",
       "      <td>187.472217</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.668446</td>\n",
       "      <td>1.289535</td>\n",
       "      <td>1.929155</td>\n",
       "      <td>326.285053</td>\n",
       "      <td>169.133689</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.501866</td>\n",
       "      <td>8.670414</td>\n",
       "      <td>1.925960</td>\n",
       "      <td>238.510377</td>\n",
       "      <td>123.839743</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.649828</td>\n",
       "      <td>5.095945</td>\n",
       "      <td>1.923123</td>\n",
       "      <td>329.234632</td>\n",
       "      <td>171.197935</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.584950</td>\n",
       "      <td>1.118552</td>\n",
       "      <td>1.912220</td>\n",
       "      <td>229.451805</td>\n",
       "      <td>119.992371</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304309</td>\n",
       "      <td>0.581897</td>\n",
       "      <td>1.912192</td>\n",
       "      <td>220.528672</td>\n",
       "      <td>115.327705</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.215375</td>\n",
       "      <td>0.410749</td>\n",
       "      <td>1.907136</td>\n",
       "      <td>372.753402</td>\n",
       "      <td>195.451933</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.202575</td>\n",
       "      <td>0.382924</td>\n",
       "      <td>1.890284</td>\n",
       "      <td>310.574495</td>\n",
       "      <td>164.300461</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.356408</td>\n",
       "      <td>0.671352</td>\n",
       "      <td>1.883662</td>\n",
       "      <td>305.974785</td>\n",
       "      <td>162.436152</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.280553</td>\n",
       "      <td>0.525676</td>\n",
       "      <td>1.873718</td>\n",
       "      <td>239.202448</td>\n",
       "      <td>127.661960</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.139318</td>\n",
       "      <td>2.130494</td>\n",
       "      <td>1.869973</td>\n",
       "      <td>235.610709</td>\n",
       "      <td>125.996835</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.118061</td>\n",
       "      <td>0.220479</td>\n",
       "      <td>1.867500</td>\n",
       "      <td>230.922677</td>\n",
       "      <td>123.653402</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.726503</td>\n",
       "      <td>1.349275</td>\n",
       "      <td>1.857218</td>\n",
       "      <td>362.453438</td>\n",
       "      <td>195.159310</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.930720</td>\n",
       "      <td>3.582136</td>\n",
       "      <td>1.855337</td>\n",
       "      <td>347.584685</td>\n",
       "      <td>187.343148</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.192695</td>\n",
       "      <td>2.205870</td>\n",
       "      <td>1.849484</td>\n",
       "      <td>225.066338</td>\n",
       "      <td>121.691445</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.279992</td>\n",
       "      <td>0.517831</td>\n",
       "      <td>1.849451</td>\n",
       "      <td>349.808494</td>\n",
       "      <td>189.141815</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.716718</td>\n",
       "      <td>1.324086</td>\n",
       "      <td>1.847429</td>\n",
       "      <td>304.309064</td>\n",
       "      <td>164.720301</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.957797</td>\n",
       "      <td>1.769417</td>\n",
       "      <td>1.847383</td>\n",
       "      <td>350.329434</td>\n",
       "      <td>189.635550</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.146033</td>\n",
       "      <td>0.267036</td>\n",
       "      <td>1.828596</td>\n",
       "      <td>335.346728</td>\n",
       "      <td>183.390247</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.214840</td>\n",
       "      <td>2.198820</td>\n",
       "      <td>1.809966</td>\n",
       "      <td>220.963575</td>\n",
       "      <td>122.081594</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.184934</td>\n",
       "      <td>0.334047</td>\n",
       "      <td>1.806305</td>\n",
       "      <td>347.288174</td>\n",
       "      <td>192.264444</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N  Cin     H     W  Kh  Kw  BLOCK_M  BLOCK_K  num_warps  num_stages  \\\n",
       "26   1   32   112   112   5   5       32       32          4           3   \n",
       "21   1    8   112   112   7   7       32       32          4           3   \n",
       "95   1    1   256   256  11  11       32       32          4           3   \n",
       "28   1   32   112   112   9   9       32       32          4           3   \n",
       "69   4    3   112   112   7   7       32       32          4           3   \n",
       "199  1   32   512   512   1   1       32       32          4           3   \n",
       "52   2    8   112   112   9   9       32       32          4           3   \n",
       "22   1    8   112   112   9   9       32       32          4           3   \n",
       "204  2    1   512   512   5   5       32       32          4           3   \n",
       "115  1   32   256   256   3   3       32       32          4           3   \n",
       "201  1   32   512   512   5   5       32       32          4           3   \n",
       "146  2   32   256   256   5   5       32       32          4           3   \n",
       "64   4    1   112   112   9   9       32       32          4           3   \n",
       "235  4    3   512   512   7   7       32       32          4           3   \n",
       "173  4   32   256   256   3   3       32       32          4           3   \n",
       "17   1    3   112   112  11  11       32       32          4           3   \n",
       "276  2   32  1024  1024   1   1       32       32          4           3   \n",
       "86   4   32   112   112   5   5       32       32          4           3   \n",
       "46   2    3   112   112   9   9       32       32          4           3   \n",
       "145  2   32   256   256   3   3       32       32          4           3   \n",
       "116  1   32   256   256   5   5       32       32          4           3   \n",
       "286  4   32  1024  1024   1   1       32       32          4           3   \n",
       "174  4   32   256   256   5   5       32       32          4           3   \n",
       "224  2   32   512   512   1   1       32       32          4           3   \n",
       "259  1    8  1024  1024   1   1       32       32          4           3   \n",
       "27   1   32   112   112   7   7       32       32          4           3   \n",
       "209  2    3   512   512   3   3       32       32          4           3   \n",
       "246  1    1  1024  1024   5   5       32       32          4           3   \n",
       "172  4   32   256   256   1   1       32       32          4           3   \n",
       "261  1   32  1024  1024   1   1       32       32          4           3   \n",
       "150  4    1   256   256   5   5       32       32          4           3   \n",
       "58   2   32   112   112   9   9       32       32          4           3   \n",
       "225  2   32   512   512   3   3       32       32          4           3   \n",
       "285  4    8  1024  1024   1   1       32       32          4           3   \n",
       "53   2    8   112   112  11  11       32       32          4           3   \n",
       "168  4    8   256   256   5   5       32       32          4           3   \n",
       "200  1   32   512   512   3   3       32       32          4           3   \n",
       "23   1    8   112   112  11  11       32       32          4           3   \n",
       "243  4   32   512   512   1   1       32       32          4           3   \n",
       "85   4   32   112   112   3   3       32       32          4           3   \n",
       "\n",
       "     t_triton_ms  t_fold_ms  speed_vs_fold  bw_triton_GBs  bw_fold_GBs  \\\n",
       "26      0.119208   0.287150       2.408815     350.198770   145.382150   \n",
       "21      0.076214   0.174012       2.283196     263.341795   115.339119   \n",
       "95      0.114009   0.255653       2.242389     280.517875   125.097771   \n",
       "28      0.337929   0.730580       2.161935     389.613981   180.215360   \n",
       "69      0.104523   0.224446       2.147335     288.028748   134.133123   \n",
       "199     0.279496   0.592216       2.118872     240.106912   113.318283   \n",
       "52      0.193782   0.410003       2.115798     339.716727   160.561970   \n",
       "22      0.103478   0.216384       2.091125     318.092819   152.115619   \n",
       "204     0.176070   0.367422       2.086800     309.684150   148.401459   \n",
       "115     0.236362   0.482924       2.043159     354.905764   173.704445   \n",
       "201     2.545701   5.130019       2.015170     342.701415   170.060817   \n",
       "146     1.284244   2.576421       2.006177     339.661017   169.307600   \n",
       "64      0.065782   0.129509       1.968781     250.187560   127.077400   \n",
       "235     2.062026   4.056882       1.967425     305.110343   155.081057   \n",
       "173     0.911701   1.781274       1.953792     368.041929   188.373180   \n",
       "17      0.061051   0.119199       1.952467     300.806183   154.064642   \n",
       "276     2.231124   4.339550       1.945006     240.628011   123.715790   \n",
       "86      0.524888   1.020308       1.943858     318.135922   163.662114   \n",
       "46      0.083698   0.162647       1.943256     294.948410   151.780541   \n",
       "145     0.461872   0.894917       1.937586     363.243481   187.472217   \n",
       "116     0.668446   1.289535       1.929155     326.285053   169.133689   \n",
       "286     4.501866   8.670414       1.925960     238.510377   123.839743   \n",
       "174     2.649828   5.095945       1.923123     329.234632   171.197935   \n",
       "224     0.584950   1.118552       1.912220     229.451805   119.992371   \n",
       "259     0.304309   0.581897       1.912192     220.528672   115.327705   \n",
       "27      0.215375   0.410749       1.907136     372.753402   195.451933   \n",
       "209     0.202575   0.382924       1.890284     310.574495   164.300461   \n",
       "246     0.356408   0.671352       1.883662     305.974785   162.436152   \n",
       "172     0.280553   0.525676       1.873718     239.202448   127.661960   \n",
       "261     1.139318   2.130494       1.869973     235.610709   125.996835   \n",
       "150     0.118061   0.220479       1.867500     230.922677   123.653402   \n",
       "58      0.726503   1.349275       1.857218     362.453438   195.159310   \n",
       "225     1.930720   3.582136       1.855337     347.584685   187.343148   \n",
       "285     1.192695   2.205870       1.849484     225.066338   121.691445   \n",
       "53      0.279992   0.517831       1.849451     349.808494   189.141815   \n",
       "168     0.716718   1.324086       1.847429     304.309064   164.720301   \n",
       "200     0.957797   1.769417       1.847383     350.329434   189.635550   \n",
       "23      0.146033   0.267036       1.828596     335.346728   183.390247   \n",
       "243     1.214840   2.198820       1.809966     220.963575   122.081594   \n",
       "85      0.184934   0.334047       1.806305     347.288174   192.264444   \n",
       "\n",
       "     max_abs_err  Cout  \n",
       "26             0    32  \n",
       "21             0    16  \n",
       "95             0     3  \n",
       "28             0    32  \n",
       "69             0     3  \n",
       "199            0    32  \n",
       "52             0    16  \n",
       "22             0    16  \n",
       "204            0     3  \n",
       "115            0    32  \n",
       "201            0    32  \n",
       "146            0    32  \n",
       "64             0     3  \n",
       "235            0     3  \n",
       "173            0    32  \n",
       "17             0     8  \n",
       "276            0    32  \n",
       "86             0    32  \n",
       "46             0     8  \n",
       "145            0    32  \n",
       "116            0    32  \n",
       "286            0    32  \n",
       "174            0    32  \n",
       "224            0    32  \n",
       "259            0    16  \n",
       "27             0    32  \n",
       "209            0     3  \n",
       "246            0     3  \n",
       "172            0    32  \n",
       "261            0    32  \n",
       "150            0     3  \n",
       "58             0    32  \n",
       "225            0    32  \n",
       "285            0    16  \n",
       "53             0    16  \n",
       "168            0    16  \n",
       "200            0    32  \n",
       "23             0    16  \n",
       "243            0    32  \n",
       "85             0    32  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global.sort_values(\"speed_vs_fold\", ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c84c-7b94-49dd-84eb-327907e293be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
