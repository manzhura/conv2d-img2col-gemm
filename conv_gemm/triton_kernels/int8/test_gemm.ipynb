{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2a8f0f-02c0-43df-ba16-d0619ca0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import time\n",
    "from gemm_int8_kernel import gemm_int8_tc_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d9441b-f60b-4973-87ab-3f824eb054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemm_int8_tc(\n",
    "    A_q: torch.Tensor,   # [M,K] int8\n",
    "    B_q: torch.Tensor,   # [K,N] int8\n",
    "    *,\n",
    "    BLOCK_M: int = 64,\n",
    "    BLOCK_N: int = 64,\n",
    "    BLOCK_K: int = 32,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Совместимая с твоей gemm_int8() версия.\n",
    "    Возвращает C_i32: [M,N] int32.\n",
    "    \"\"\"\n",
    "    assert A_q.is_cuda and B_q.is_cuda, \"A_q и B_q должны лежать на CUDA\"\n",
    "    assert A_q.dtype == torch.int8 and B_q.dtype == torch.int8, \"Оба тензора должны быть int8\"\n",
    "\n",
    "    if not A_q.is_contiguous():\n",
    "        A_q = A_q.contiguous()\n",
    "    if not B_q.is_contiguous():\n",
    "        B_q = B_q.contiguous()\n",
    "\n",
    "    M, K1 = A_q.shape\n",
    "    K2, N = B_q.shape\n",
    "    assert K1 == K2, f\"K mismatch: {K1} vs {K2}\"\n",
    "\n",
    "    assert K1 % 4 == 0, f\"K={K1} must be divisible by 4 for INT8 dot\"\n",
    "    assert BLOCK_K % 4 == 0, f\"BLOCK_K={BLOCK_K} must be divisible by 4\"\n",
    "\n",
    "    C_i32 = torch.empty((M, N), dtype=torch.int32, device=A_q.device)\n",
    "\n",
    "    a_m, a_k = A_q.stride()\n",
    "    b_k, b_n = B_q.stride()\n",
    "    c_m, c_n = C_i32.stride()\n",
    "\n",
    "    grid = (\n",
    "        triton.cdiv(M, BLOCK_M),\n",
    "        triton.cdiv(N, BLOCK_N),\n",
    "    )\n",
    "\n",
    "    gemm_int8_tc_kernel[grid](\n",
    "        A_q, B_q, C_i32,\n",
    "        M, N, K1,\n",
    "        a_m, a_k,\n",
    "        b_k, b_n,\n",
    "        c_m, c_n,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return C_i32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cceb33-ae6a-41c3-af13-4b6c12f62f2c",
   "metadata": {},
   "source": [
    "# title search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24134f7-47f2-4d16-b951-2fb19db1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_gemm_int8_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч ОДНОГО размера GEMM:\n",
    "      - C = A @ B\n",
    "      - A [M,K] int8, B [K,N] int8\n",
    "      - наша реализация vs torch (fp32 ref)\n",
    "    \"\"\"\n",
    "    # генерим int8\n",
    "    A_q = torch.randint(-128, 127, (M, K), device=device, dtype=torch.int8)\n",
    "    B_q = torch.randint(-128, 127, (K, N), device=device, dtype=torch.int8)\n",
    "\n",
    "    # fp32 ref\n",
    "    A_f = A_q.float()\n",
    "    B_f = B_q.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        C_ref = A_f @ B_f  # [M,N] fp32\n",
    "\n",
    "    # --- warmup & timing: Triton ---\n",
    "    def _call_triton():\n",
    "        C_i32 = gemm_int8_tc(\n",
    "            A_q, B_q,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return C_i32\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # --- torch matmul (fp32) ---\n",
    "    def _call_torch():\n",
    "        return A_f @ B_f\n",
    "\n",
    "    for _ in range(5):\n",
    "        _ = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref2 = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # ошибка (переводим наш int32 -> fp32, сравниваем с fp32 ref)\n",
    "    C_triton_f = C_i32.float()\n",
    "    max_abs_err = (C_triton_f - C_ref).abs().max().item()\n",
    "\n",
    "    # простая оценка bandwidth: читаем A,B, пишем C (байты/сек)\n",
    "    bytes_moved = A_q.numel() + B_q.numel()  # int8\n",
    "    bytes_moved += C_i32.numel() * 4        # int32\n",
    "    bytes_moved = float(bytes_moved)\n",
    "\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e230b854-ec14-467d-a3c6-d55a1937a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def tune_gemm_int8_tiles_for_shape(\n",
    "    M, K, N,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Перебираем набор тайлов и возвращаем:\n",
    "      - df: DataFrame со всеми результатами (по одной строке на конфиг тайлов)\n",
    "    Никакого выбора \"лучшего\" внутри — это ты делаешь снаружи.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for BM in blocks_M:\n",
    "        for BN in blocks_N:\n",
    "            for BK in blocks_K:\n",
    "                # K и BLOCK_K должны быть кратны 4\n",
    "                if (K % 4 != 0) or (BK % 4 != 0):\n",
    "                    print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}: K/BK not multiple of 4\")\n",
    "                    continue\n",
    "\n",
    "                for W in warps:\n",
    "                    for S in stages:\n",
    "                        try:\n",
    "                            rec = bench_once_gemm_int8_vs_torch(\n",
    "                                M, K, N,\n",
    "                                BLOCK_M=BM,\n",
    "                                BLOCK_N=BN,\n",
    "                                BLOCK_K=BK,\n",
    "                                num_warps=W,\n",
    "                                num_stages=S,\n",
    "                                iters=iters,\n",
    "                                device=device,\n",
    "                            )\n",
    "                        except RuntimeError as e:\n",
    "                            print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        print(\n",
    "                            f\"BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: \"\n",
    "                            f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                            f\"speed_vs_torch={rec['speed_vs_torch']:.3f}x, \"\n",
    "                            f\"err={rec['max_abs_err']}\"\n",
    "                        )\n",
    "                        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this GEMM shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c363e02-f7c4-4911-a6da-ed7305f15f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BN=32, BK=32, W=2, S=2: t_triton=0.236 ms, speed_vs_torch=2.716x, err=0.0\n",
      "BM=32, BN=32, BK=32, W=2, S=3: t_triton=0.245 ms, speed_vs_torch=2.578x, err=0.0\n",
      "BM=32, BN=32, BK=32, W=4, S=2: t_triton=0.391 ms, speed_vs_torch=1.613x, err=0.0\n",
      "BM=32, BN=32, BK=32, W=4, S=3: t_triton=0.408 ms, speed_vs_torch=1.511x, err=0.0\n",
      "BM=32, BN=32, BK=32, W=8, S=2: t_triton=0.504 ms, speed_vs_torch=1.267x, err=0.0\n",
      "BM=32, BN=32, BK=32, W=8, S=3: t_triton=0.470 ms, speed_vs_torch=1.340x, err=0.0\n",
      "BM=32, BN=32, BK=64, W=2, S=2: t_triton=0.214 ms, speed_vs_torch=2.972x, err=0.0\n",
      "BM=32, BN=32, BK=64, W=2, S=3: t_triton=0.220 ms, speed_vs_torch=2.920x, err=0.0\n",
      "BM=32, BN=32, BK=64, W=4, S=2: t_triton=0.312 ms, speed_vs_torch=2.012x, err=0.0\n",
      "BM=32, BN=32, BK=64, W=4, S=3: t_triton=0.323 ms, speed_vs_torch=1.925x, err=0.0\n",
      "BM=32, BN=32, BK=64, W=8, S=2: t_triton=0.373 ms, speed_vs_torch=1.708x, err=0.0\n",
      "BM=32, BN=32, BK=64, W=8, S=3: t_triton=0.376 ms, speed_vs_torch=1.658x, err=0.0\n",
      "BM=32, BN=32, BK=128, W=2, S=2: t_triton=0.213 ms, speed_vs_torch=3.041x, err=0.0\n",
      "BM=32, BN=32, BK=128, W=2, S=3: t_triton=0.239 ms, speed_vs_torch=2.643x, err=0.0\n",
      "BM=32, BN=32, BK=128, W=4, S=2: t_triton=0.314 ms, speed_vs_torch=2.032x, err=0.0\n",
      "BM=32, BN=32, BK=128, W=4, S=3: t_triton=0.322 ms, speed_vs_torch=1.942x, err=0.0\n",
      "BM=32, BN=32, BK=128, W=8, S=2: t_triton=0.341 ms, speed_vs_torch=1.858x, err=0.0\n",
      "BM=32, BN=32, BK=128, W=8, S=3: t_triton=0.347 ms, speed_vs_torch=1.816x, err=0.0\n",
      "BM=32, BN=64, BK=32, W=2, S=2: t_triton=0.181 ms, speed_vs_torch=3.475x, err=0.0\n",
      "BM=32, BN=64, BK=32, W=2, S=3: t_triton=0.176 ms, speed_vs_torch=3.559x, err=0.0\n",
      "BM=32, BN=64, BK=32, W=4, S=2: t_triton=0.212 ms, speed_vs_torch=2.965x, err=0.0\n",
      "BM=32, BN=64, BK=32, W=4, S=3: t_triton=0.205 ms, speed_vs_torch=3.052x, err=0.0\n",
      "BM=32, BN=64, BK=32, W=8, S=2: t_triton=0.336 ms, speed_vs_torch=1.941x, err=0.0\n",
      "BM=32, BN=64, BK=32, W=8, S=3: t_triton=0.330 ms, speed_vs_torch=1.900x, err=0.0\n",
      "BM=32, BN=64, BK=64, W=2, S=2: t_triton=0.165 ms, speed_vs_torch=3.781x, err=0.0\n",
      "BM=32, BN=64, BK=64, W=2, S=3: t_triton=0.172 ms, speed_vs_torch=3.754x, err=0.0\n",
      "BM=32, BN=64, BK=64, W=4, S=2: t_triton=0.189 ms, speed_vs_torch=3.356x, err=0.0\n",
      "BM=32, BN=64, BK=64, W=4, S=3: t_triton=0.189 ms, speed_vs_torch=3.345x, err=0.0\n",
      "BM=32, BN=64, BK=64, W=8, S=2: t_triton=0.302 ms, speed_vs_torch=2.118x, err=0.0\n",
      "BM=32, BN=64, BK=64, W=8, S=3: t_triton=0.298 ms, speed_vs_torch=2.098x, err=0.0\n",
      "BM=32, BN=64, BK=128, W=2, S=2: t_triton=0.169 ms, speed_vs_torch=3.754x, err=0.0\n",
      "BM=32, BN=64, BK=128, W=2, S=3: t_triton=0.179 ms, speed_vs_torch=3.516x, err=0.0\n",
      "BM=32, BN=64, BK=128, W=4, S=2: t_triton=0.190 ms, speed_vs_torch=3.317x, err=0.0\n",
      "BM=32, BN=64, BK=128, W=4, S=3: t_triton=0.292 ms, speed_vs_torch=2.158x, err=0.0\n",
      "BM=32, BN=64, BK=128, W=8, S=2: t_triton=0.284 ms, speed_vs_torch=2.211x, err=0.0\n",
      "BM=32, BN=64, BK=128, W=8, S=3: t_triton=0.286 ms, speed_vs_torch=2.202x, err=0.0\n",
      "BM=32, BN=128, BK=32, W=2, S=2: t_triton=0.168 ms, speed_vs_torch=3.728x, err=0.0\n",
      "BM=32, BN=128, BK=32, W=2, S=3: t_triton=0.157 ms, speed_vs_torch=4.088x, err=0.0\n",
      "BM=32, BN=128, BK=32, W=4, S=2: t_triton=0.185 ms, speed_vs_torch=3.406x, err=0.0\n",
      "BM=32, BN=128, BK=32, W=4, S=3: t_triton=0.180 ms, speed_vs_torch=3.555x, err=0.0\n",
      "BM=32, BN=128, BK=32, W=8, S=2: t_triton=0.218 ms, speed_vs_torch=2.899x, err=0.0\n",
      "BM=32, BN=128, BK=32, W=8, S=3: t_triton=0.217 ms, speed_vs_torch=2.920x, err=0.0\n",
      "BM=32, BN=128, BK=64, W=2, S=2: t_triton=0.157 ms, speed_vs_torch=4.024x, err=0.0\n",
      "BM=32, BN=128, BK=64, W=2, S=3: t_triton=0.153 ms, speed_vs_torch=4.137x, err=0.0\n",
      "BM=32, BN=128, BK=64, W=4, S=2: t_triton=0.184 ms, speed_vs_torch=3.426x, err=0.0\n",
      "BM=32, BN=128, BK=64, W=4, S=3: t_triton=0.161 ms, speed_vs_torch=3.971x, err=0.0\n",
      "BM=32, BN=128, BK=64, W=8, S=2: t_triton=0.198 ms, speed_vs_torch=3.198x, err=0.0\n",
      "BM=32, BN=128, BK=64, W=8, S=3: t_triton=0.182 ms, speed_vs_torch=3.442x, err=0.0\n",
      "BM=32, BN=128, BK=128, W=2, S=2: t_triton=0.162 ms, speed_vs_torch=3.899x, err=0.0\n",
      "BM=32, BN=128, BK=128, W=2, S=3: t_triton=0.223 ms, speed_vs_torch=2.836x, err=0.0\n",
      "BM=32, BN=128, BK=128, W=4, S=2: t_triton=0.167 ms, speed_vs_torch=3.826x, err=0.0\n",
      "BM=32, BN=128, BK=128, W=4, S=3: t_triton=0.180 ms, speed_vs_torch=3.478x, err=0.0\n",
      "BM=32, BN=128, BK=128, W=8, S=2: t_triton=0.318 ms, speed_vs_torch=1.988x, err=0.0\n",
      "BM=32, BN=128, BK=128, W=8, S=3: t_triton=0.184 ms, speed_vs_torch=3.400x, err=0.0\n",
      "BM=64, BN=32, BK=32, W=2, S=2: t_triton=0.193 ms, speed_vs_torch=3.276x, err=0.0\n",
      "BM=64, BN=32, BK=32, W=2, S=3: t_triton=0.207 ms, speed_vs_torch=3.057x, err=0.0\n",
      "BM=64, BN=32, BK=32, W=4, S=2: t_triton=0.232 ms, speed_vs_torch=2.717x, err=0.0\n",
      "BM=64, BN=32, BK=32, W=4, S=3: t_triton=0.223 ms, speed_vs_torch=2.830x, err=0.0\n",
      "BM=64, BN=32, BK=32, W=8, S=2: t_triton=0.346 ms, speed_vs_torch=1.805x, err=0.0\n",
      "BM=64, BN=32, BK=32, W=8, S=3: t_triton=0.358 ms, speed_vs_torch=1.772x, err=0.0\n",
      "BM=64, BN=32, BK=64, W=2, S=2: t_triton=0.191 ms, speed_vs_torch=3.308x, err=0.0\n",
      "BM=64, BN=32, BK=64, W=2, S=3: t_triton=0.184 ms, speed_vs_torch=3.446x, err=0.0\n",
      "BM=64, BN=32, BK=64, W=4, S=2: t_triton=0.188 ms, speed_vs_torch=3.381x, err=0.0\n",
      "BM=64, BN=32, BK=64, W=4, S=3: t_triton=0.192 ms, speed_vs_torch=3.313x, err=0.0\n",
      "BM=64, BN=32, BK=64, W=8, S=2: t_triton=0.278 ms, speed_vs_torch=2.289x, err=0.0\n",
      "BM=64, BN=32, BK=64, W=8, S=3: t_triton=0.290 ms, speed_vs_torch=2.181x, err=0.0\n",
      "BM=64, BN=32, BK=128, W=2, S=2: t_triton=0.195 ms, speed_vs_torch=3.303x, err=0.0\n",
      "BM=64, BN=32, BK=128, W=2, S=3: t_triton=0.191 ms, speed_vs_torch=3.356x, err=0.0\n",
      "BM=64, BN=32, BK=128, W=4, S=2: t_triton=0.196 ms, speed_vs_torch=3.245x, err=0.0\n",
      "BM=64, BN=32, BK=128, W=4, S=3: t_triton=0.222 ms, speed_vs_torch=2.865x, err=0.0\n",
      "BM=64, BN=32, BK=128, W=8, S=2: t_triton=0.269 ms, speed_vs_torch=2.397x, err=0.0\n",
      "BM=64, BN=32, BK=128, W=8, S=3: t_triton=0.298 ms, speed_vs_torch=2.117x, err=0.0\n",
      "BM=64, BN=64, BK=32, W=2, S=2: t_triton=0.122 ms, speed_vs_torch=5.182x, err=0.0\n",
      "BM=64, BN=64, BK=32, W=2, S=3: t_triton=0.127 ms, speed_vs_torch=4.949x, err=0.0\n",
      "BM=64, BN=64, BK=32, W=4, S=2: t_triton=0.177 ms, speed_vs_torch=3.591x, err=0.0\n",
      "BM=64, BN=64, BK=32, W=4, S=3: t_triton=0.167 ms, speed_vs_torch=3.774x, err=0.0\n",
      "BM=64, BN=64, BK=32, W=8, S=2: t_triton=0.206 ms, speed_vs_torch=3.068x, err=0.0\n",
      "BM=64, BN=64, BK=32, W=8, S=3: t_triton=0.198 ms, speed_vs_torch=3.205x, err=0.0\n",
      "BM=64, BN=64, BK=64, W=2, S=2: t_triton=0.119 ms, speed_vs_torch=5.411x, err=0.0\n",
      "BM=64, BN=64, BK=64, W=2, S=3: t_triton=0.139 ms, speed_vs_torch=4.615x, err=0.0\n",
      "BM=64, BN=64, BK=64, W=4, S=2: t_triton=0.164 ms, speed_vs_torch=3.870x, err=0.0\n",
      "BM=64, BN=64, BK=64, W=4, S=3: t_triton=0.270 ms, speed_vs_torch=2.323x, err=0.0\n",
      "BM=64, BN=64, BK=64, W=8, S=2: t_triton=0.173 ms, speed_vs_torch=3.640x, err=0.0\n",
      "BM=64, BN=64, BK=64, W=8, S=3: t_triton=0.169 ms, speed_vs_torch=3.810x, err=0.0\n",
      "BM=64, BN=64, BK=128, W=2, S=2: t_triton=0.130 ms, speed_vs_torch=4.971x, err=0.0\n",
      "BM=64, BN=64, BK=128, W=2, S=3: t_triton=0.277 ms, speed_vs_torch=2.288x, err=0.0\n",
      "BM=64, BN=64, BK=128, W=4, S=2: t_triton=0.158 ms, speed_vs_torch=3.983x, err=0.0\n",
      "BM=64, BN=64, BK=128, W=4, S=3: t_triton=0.162 ms, speed_vs_torch=3.900x, err=0.0\n",
      "BM=64, BN=64, BK=128, W=8, S=2: t_triton=0.174 ms, speed_vs_torch=3.613x, err=0.0\n",
      "BM=64, BN=64, BK=128, W=8, S=3: t_triton=0.164 ms, speed_vs_torch=3.884x, err=0.0\n",
      "BM=64, BN=128, BK=32, W=2, S=2: t_triton=0.122 ms, speed_vs_torch=5.193x, err=0.0\n",
      "BM=64, BN=128, BK=32, W=2, S=3: t_triton=0.114 ms, speed_vs_torch=5.571x, err=0.0\n",
      "BM=64, BN=128, BK=32, W=4, S=2: t_triton=0.132 ms, speed_vs_torch=4.785x, err=0.0\n",
      "BM=64, BN=128, BK=32, W=4, S=3: t_triton=0.126 ms, speed_vs_torch=5.103x, err=0.0\n",
      "BM=64, BN=128, BK=32, W=8, S=2: t_triton=0.185 ms, speed_vs_torch=3.428x, err=0.0\n",
      "BM=64, BN=128, BK=32, W=8, S=3: t_triton=0.177 ms, speed_vs_torch=3.649x, err=0.0\n",
      "BM=64, BN=128, BK=64, W=2, S=2: t_triton=0.111 ms, speed_vs_torch=5.681x, err=0.0\n",
      "BM=64, BN=128, BK=64, W=2, S=3: t_triton=0.257 ms, speed_vs_torch=2.425x, err=0.0\n",
      "BM=64, BN=128, BK=64, W=4, S=2: t_triton=0.107 ms, speed_vs_torch=5.972x, err=0.0\n",
      "BM=64, BN=128, BK=64, W=4, S=3: t_triton=0.105 ms, speed_vs_torch=6.052x, err=0.0\n",
      "BM=64, BN=128, BK=64, W=8, S=2: t_triton=0.171 ms, speed_vs_torch=3.715x, err=0.0\n",
      "BM=64, BN=128, BK=64, W=8, S=3: t_triton=0.159 ms, speed_vs_torch=4.041x, err=0.0\n",
      "BM=64, BN=128, BK=128, W=2, S=2: t_triton=0.131 ms, speed_vs_torch=4.806x, err=0.0\n",
      "BM=64, BN=128, BK=128, W=2, S=3: t_triton=0.182 ms, speed_vs_torch=3.477x, err=0.0\n",
      "BM=64, BN=128, BK=128, W=4, S=2: t_triton=0.115 ms, speed_vs_torch=5.515x, err=0.0\n",
      "BM=64, BN=128, BK=128, W=4, S=3: t_triton=0.120 ms, speed_vs_torch=5.263x, err=0.0\n",
      "BM=64, BN=128, BK=128, W=8, S=2: t_triton=0.167 ms, speed_vs_torch=3.807x, err=0.0\n",
      "BM=64, BN=128, BK=128, W=8, S=3: t_triton=0.147 ms, speed_vs_torch=4.280x, err=0.0\n",
      "BM=128, BN=32, BK=32, W=2, S=2: t_triton=0.137 ms, speed_vs_torch=4.620x, err=0.0\n",
      "BM=128, BN=32, BK=32, W=2, S=3: t_triton=0.134 ms, speed_vs_torch=4.807x, err=0.0\n",
      "BM=128, BN=32, BK=32, W=4, S=2: t_triton=0.189 ms, speed_vs_torch=3.357x, err=0.0\n",
      "BM=128, BN=32, BK=32, W=4, S=3: t_triton=0.178 ms, speed_vs_torch=3.549x, err=0.0\n",
      "BM=128, BN=32, BK=32, W=8, S=2: t_triton=0.218 ms, speed_vs_torch=2.902x, err=0.0\n",
      "BM=128, BN=32, BK=32, W=8, S=3: t_triton=0.209 ms, speed_vs_torch=2.996x, err=0.0\n",
      "BM=128, BN=32, BK=64, W=2, S=2: t_triton=0.127 ms, speed_vs_torch=5.010x, err=0.0\n",
      "BM=128, BN=32, BK=64, W=2, S=3: t_triton=0.132 ms, speed_vs_torch=4.904x, err=0.0\n",
      "BM=128, BN=32, BK=64, W=4, S=2: t_triton=0.156 ms, speed_vs_torch=4.030x, err=0.0\n",
      "BM=128, BN=32, BK=64, W=4, S=3: t_triton=0.170 ms, speed_vs_torch=3.729x, err=0.0\n",
      "BM=128, BN=32, BK=64, W=8, S=2: t_triton=0.189 ms, speed_vs_torch=3.332x, err=0.0\n",
      "BM=128, BN=32, BK=64, W=8, S=3: t_triton=0.180 ms, speed_vs_torch=3.521x, err=0.0\n",
      "BM=128, BN=32, BK=128, W=2, S=2: t_triton=0.135 ms, speed_vs_torch=4.782x, err=0.0\n",
      "BM=128, BN=32, BK=128, W=2, S=3: t_triton=0.171 ms, speed_vs_torch=3.706x, err=0.0\n",
      "BM=128, BN=32, BK=128, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=3.471x, err=0.0\n",
      "BM=128, BN=32, BK=128, W=4, S=3: t_triton=0.199 ms, speed_vs_torch=3.202x, err=0.0\n",
      "BM=128, BN=32, BK=128, W=8, S=2: t_triton=0.181 ms, speed_vs_torch=3.507x, err=0.0\n",
      "BM=128, BN=32, BK=128, W=8, S=3: t_triton=0.187 ms, speed_vs_torch=3.403x, err=0.0\n",
      "BM=128, BN=64, BK=32, W=2, S=2: t_triton=0.138 ms, speed_vs_torch=4.582x, err=0.0\n",
      "BM=128, BN=64, BK=32, W=2, S=3: t_triton=0.220 ms, speed_vs_torch=2.853x, err=0.0\n",
      "BM=128, BN=64, BK=32, W=4, S=2: t_triton=0.133 ms, speed_vs_torch=4.761x, err=0.0\n",
      "BM=128, BN=64, BK=32, W=4, S=3: t_triton=0.139 ms, speed_vs_torch=4.558x, err=0.0\n",
      "BM=128, BN=64, BK=32, W=8, S=2: t_triton=0.217 ms, speed_vs_torch=2.915x, err=0.0\n",
      "BM=128, BN=64, BK=32, W=8, S=3: t_triton=0.177 ms, speed_vs_torch=3.595x, err=0.0\n",
      "BM=128, BN=64, BK=64, W=2, S=2: t_triton=0.130 ms, speed_vs_torch=4.891x, err=0.0\n",
      "BM=128, BN=64, BK=64, W=2, S=3: t_triton=0.108 ms, speed_vs_torch=5.994x, err=0.0\n",
      "BM=128, BN=64, BK=64, W=4, S=2: t_triton=0.111 ms, speed_vs_torch=5.754x, err=0.0\n",
      "BM=128, BN=64, BK=64, W=4, S=3: t_triton=0.132 ms, speed_vs_torch=4.770x, err=0.0\n",
      "BM=128, BN=64, BK=64, W=8, S=2: t_triton=0.190 ms, speed_vs_torch=3.392x, err=0.0\n",
      "BM=128, BN=64, BK=64, W=8, S=3: t_triton=0.146 ms, speed_vs_torch=4.452x, err=0.0\n",
      "BM=128, BN=64, BK=128, W=2, S=2: t_triton=0.213 ms, speed_vs_torch=3.016x, err=0.0\n",
      "BM=128, BN=64, BK=128, W=2, S=3: t_triton=0.216 ms, speed_vs_torch=2.949x, err=0.0\n",
      "BM=128, BN=64, BK=128, W=4, S=2: t_triton=0.136 ms, speed_vs_torch=4.670x, err=0.0\n",
      "BM=128, BN=64, BK=128, W=4, S=3: t_triton=0.132 ms, speed_vs_torch=4.842x, err=0.0\n",
      "BM=128, BN=64, BK=128, W=8, S=2: t_triton=0.152 ms, speed_vs_torch=4.174x, err=0.0\n",
      "BM=128, BN=64, BK=128, W=8, S=3: t_triton=0.180 ms, speed_vs_torch=3.553x, err=0.0\n",
      "BM=128, BN=128, BK=32, W=2, S=2: t_triton=0.374 ms, speed_vs_torch=1.709x, err=0.0\n",
      "BM=128, BN=128, BK=32, W=2, S=3: t_triton=0.449 ms, speed_vs_torch=1.406x, err=0.0\n",
      "BM=128, BN=128, BK=32, W=4, S=2: t_triton=0.128 ms, speed_vs_torch=5.010x, err=0.0\n",
      "BM=128, BN=128, BK=32, W=4, S=3: t_triton=0.111 ms, speed_vs_torch=5.750x, err=0.0\n",
      "BM=128, BN=128, BK=32, W=8, S=2: t_triton=0.166 ms, speed_vs_torch=3.891x, err=0.0\n",
      "BM=128, BN=128, BK=32, W=8, S=3: t_triton=0.145 ms, speed_vs_torch=4.410x, err=0.0\n",
      "BM=128, BN=128, BK=64, W=2, S=2: t_triton=0.526 ms, speed_vs_torch=1.208x, err=0.0\n",
      "BM=128, BN=128, BK=64, W=2, S=3: t_triton=0.580 ms, speed_vs_torch=1.105x, err=0.0\n",
      "BM=128, BN=128, BK=64, W=4, S=2: t_triton=0.117 ms, speed_vs_torch=5.425x, err=0.0\n",
      "BM=128, BN=128, BK=64, W=4, S=3: t_triton=0.106 ms, speed_vs_torch=5.982x, err=0.0\n",
      "BM=128, BN=128, BK=64, W=8, S=2: t_triton=0.175 ms, speed_vs_torch=3.629x, err=0.0\n",
      "BM=128, BN=128, BK=64, W=8, S=3: t_triton=0.129 ms, speed_vs_torch=4.936x, err=0.0\n",
      "BM=128, BN=128, BK=128, W=2, S=2: t_triton=0.597 ms, speed_vs_torch=1.072x, err=0.0\n",
      "BM=128, BN=128, BK=128, W=2, S=3: t_triton=0.628 ms, speed_vs_torch=1.028x, err=0.0\n",
      "BM=128, BN=128, BK=128, W=4, S=2: t_triton=0.153 ms, speed_vs_torch=4.152x, err=0.0\n",
      "BM=128, BN=128, BK=128, W=4, S=3: t_triton=0.182 ms, speed_vs_torch=3.488x, err=0.0\n",
      "BM=128, BN=128, BK=128, W=8, S=2: t_triton=0.171 ms, speed_vs_torch=3.710x, err=0.0\n",
      "BM=128, BN=128, BK=128, W=8, S=3: t_triton=0.127 ms, speed_vs_torch=4.902x, err=0.0\n"
     ]
    }
   ],
   "source": [
    "df_gemm_tiles = tune_gemm_int8_tiles_for_shape(\n",
    "    M=4096, K=1024, N=1024,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c29522-a1df-4062-adbc-9edc3a7e4709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch_ms</th>\n",
       "      <th>speed_vs_torch</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_torch_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104641</td>\n",
       "      <td>0.633273</td>\n",
       "      <td>6.051837</td>\n",
       "      <td>210.433932</td>\n",
       "      <td>34.771910</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.106228</td>\n",
       "      <td>0.635428</td>\n",
       "      <td>5.981746</td>\n",
       "      <td>207.291235</td>\n",
       "      <td>34.653970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.638751</td>\n",
       "      <td>5.972279</td>\n",
       "      <td>205.886332</td>\n",
       "      <td>34.473660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.108423</td>\n",
       "      <td>0.649938</td>\n",
       "      <td>5.994492</td>\n",
       "      <td>203.095065</td>\n",
       "      <td>33.880281</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.110898</td>\n",
       "      <td>0.630017</td>\n",
       "      <td>5.681029</td>\n",
       "      <td>198.560855</td>\n",
       "      <td>34.951565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.111004</td>\n",
       "      <td>0.638280</td>\n",
       "      <td>5.750068</td>\n",
       "      <td>198.372238</td>\n",
       "      <td>34.499112</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.111353</td>\n",
       "      <td>0.640743</td>\n",
       "      <td>5.754159</td>\n",
       "      <td>197.750308</td>\n",
       "      <td>34.366501</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113844</td>\n",
       "      <td>0.634232</td>\n",
       "      <td>5.571065</td>\n",
       "      <td>193.423634</td>\n",
       "      <td>34.719327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.114519</td>\n",
       "      <td>0.631522</td>\n",
       "      <td>5.514569</td>\n",
       "      <td>192.283510</td>\n",
       "      <td>34.868277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.117167</td>\n",
       "      <td>0.635612</td>\n",
       "      <td>5.424834</td>\n",
       "      <td>187.937391</td>\n",
       "      <td>34.643900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        M     K     N  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "99   4096  1024  1024       64      128       64          4           3   \n",
       "153  4096  1024  1024      128      128       64          4           3   \n",
       "98   4096  1024  1024       64      128       64          4           2   \n",
       "133  4096  1024  1024      128       64       64          2           3   \n",
       "96   4096  1024  1024       64      128       64          2           2   \n",
       "147  4096  1024  1024      128      128       32          4           3   \n",
       "134  4096  1024  1024      128       64       64          4           2   \n",
       "91   4096  1024  1024       64      128       32          2           3   \n",
       "104  4096  1024  1024       64      128      128          4           2   \n",
       "152  4096  1024  1024      128      128       64          4           2   \n",
       "\n",
       "     t_triton_ms  t_torch_ms  speed_vs_torch  bw_triton_GBs  bw_torch_GBs  \\\n",
       "99      0.104641    0.633273        6.051837     210.433932     34.771910   \n",
       "153     0.106228    0.635428        5.981746     207.291235     34.653970   \n",
       "98      0.106953    0.638751        5.972279     205.886332     34.473660   \n",
       "133     0.108423    0.649938        5.994492     203.095065     33.880281   \n",
       "96      0.110898    0.630017        5.681029     198.560855     34.951565   \n",
       "147     0.111004    0.638280        5.750068     198.372238     34.499112   \n",
       "134     0.111353    0.640743        5.754159     197.750308     34.366501   \n",
       "91      0.113844    0.634232        5.571065     193.423634     34.719327   \n",
       "104     0.114519    0.631522        5.514569     192.283510     34.868277   \n",
       "152     0.117167    0.635612        5.424834     187.937391     34.643900   \n",
       "\n",
       "     max_abs_err  \n",
       "99           0.0  \n",
       "153          0.0  \n",
       "98           0.0  \n",
       "133          0.0  \n",
       "96           0.0  \n",
       "147          0.0  \n",
       "134          0.0  \n",
       "91           0.0  \n",
       "104          0.0  \n",
       "152          0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemm_tiles.sort_values(\"t_triton_ms\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb321830-e82a-43ae-96f5-691be9a9ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa596d30-56f6-4fce-ae4e-684416db22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_GEMM_BEST_BLOCK_M = 64\n",
    "INT8_GEMM_BEST_BLOCK_N = 128\n",
    "INT8_GEMM_BEST_BLOCK_K = 64\n",
    "INT8_GEMM_BEST_WARPS   = 4\n",
    "INT8_GEMM_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ac81a-3b19-4e5b-b47a-fabdbdac525b",
   "metadata": {},
   "source": [
    "# BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78cf0a0-ba04-4581-99ca-b910ad54c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Ожидаем, что где-то выше уже есть:\n",
    "# from your_module import gemm_int8_tc\n",
    "# (тот, что берёт A_q[int8][M,K], B_q[int8][K,N] и возвращает C_i32[int32][M,N])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench_once_gemm_int8_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч для ОДНОГО GEMM-шейпа и ОДНОГО набора тайлов:\n",
    "      - A_q: [M, K] int8\n",
    "      - B_q: [K, N] int8\n",
    "      - Triton: gemm_int8_tc -> C_i32 [M,N] int32\n",
    "      - Torch: (A_q.float() @ B_q.float()).to(int32)\n",
    "      - считаем:\n",
    "          t_triton_ms, t_torch_ms,\n",
    "          speed_vs_torch,\n",
    "          max_abs_err\n",
    "    \"\"\"\n",
    "    # --- проверка кратности для INT8 dot ---\n",
    "    if (K % 4) != 0 or (BLOCK_K % 4) != 0:\n",
    "        raise RuntimeError(f\"K={K} или BLOCK_K={BLOCK_K} не кратны 4 — INT8 dot невалиден\")\n",
    "\n",
    "    # --- данные ---\n",
    "    A_q = torch.randint(-128, 127, (M, K), device=device, dtype=torch.int8)\n",
    "    B_q = torch.randint(-128, 127, (K, N), device=device, dtype=torch.int8)\n",
    "\n",
    "    # референс: fp32 GEMM + cast к int32\n",
    "    A_f = A_q.float()\n",
    "    B_f = B_q.float()\n",
    "\n",
    "    def _call_triton():\n",
    "        C_i32 = gemm_int8_tc(\n",
    "            A_q, B_q,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return C_i32\n",
    "\n",
    "    def _call_torch():\n",
    "        C_ref_f = A_f @ B_f          # [M,N] fp32, cuBLAS\n",
    "        C_ref_i32 = C_ref_f.to(torch.int32)\n",
    "        return C_ref_i32\n",
    "\n",
    "    # --- прогрев Triton ---\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # измеряем Triton\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # --- прогрев Torch ---\n",
    "    for _ in range(5):\n",
    "        _ = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # измеряем Torch\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref_i32 = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # --- ошибка ---\n",
    "    max_abs_err = (C_i32 - C_ref_i32).abs().max().item()\n",
    "\n",
    "    # простая оценка \"байт\" (очень грубо): читаем A и B, пишем C\n",
    "    bytes_moved = A_q.numel() + B_q.numel() + C_i32.numel()  # в \"элементах\"\n",
    "    # переводим в байты: int8=1 байт, int32=4 байта\n",
    "    bytes_moved = A_q.numel() * 1 + B_q.numel() * 1 + C_i32.numel() * 4\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch = bytes_moved / t_torch / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M,\n",
    "        \"K\": K,\n",
    "        \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_gemm_int8_best_tiles(\n",
    "    tiles_cfg: dict,\n",
    "    Ms=(1024, 2048, 4096),\n",
    "    Ks=(256, 512, 1024),\n",
    "    Ns=(256, 512, 1024),\n",
    "    iters_per_shape: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Сравнение gemm_int8_tc vs torch матрицы на сетке (M, K, N)\n",
    "    при фиксированных лучших тайлах tiles_cfg.\n",
    "\n",
    "    tiles_cfg: dict с ключами:\n",
    "      - \"BLOCK_M\"\n",
    "      - \"BLOCK_N\"\n",
    "      - \"BLOCK_K\"\n",
    "      - \"num_warps\"\n",
    "      - \"num_stages\"\n",
    "    \"\"\"\n",
    "    BM = tiles_cfg[\"BLOCK_M\"]\n",
    "    BN = tiles_cfg[\"BLOCK_N\"]\n",
    "    BK = tiles_cfg[\"BLOCK_K\"]\n",
    "    NW = tiles_cfg[\"num_warps\"]\n",
    "    NS = tiles_cfg[\"num_stages\"]\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for M in Ms:\n",
    "        for K in Ks:\n",
    "            for N in Ns:\n",
    "                print(f\"=== GEMM SHAPE: M={M}, K={K}, N={N} ===\")\n",
    "\n",
    "                # Проверяем кратность для INT8 dot\n",
    "                if (K % 4) != 0 or (BK % 4) != 0:\n",
    "                    print(f\"[SKIP] M={M}, K={K}, N={N}: K/BLOCK_K not multiple of 4\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    rec = bench_once_gemm_int8_vs_torch(\n",
    "                        M, K, N,\n",
    "                        BLOCK_M=BM,\n",
    "                        BLOCK_N=BN,\n",
    "                        BLOCK_K=BK,\n",
    "                        num_warps=NW,\n",
    "                        num_stages=NS,\n",
    "                        iters=iters_per_shape,\n",
    "                        device=device,\n",
    "                    )\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"[SKIP] M={M}, K={K}, N={N}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if rec[\"max_abs_err\"] != 0:\n",
    "                    print(f\"[WRONG] M={M}, K={K}, N={N}, err={rec['max_abs_err']}\")\n",
    "                    continue\n",
    "\n",
    "                records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        print(\"[WARN] no successful GEMM records\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58d9d9d-5ab5-4988-8bcc-905e4829f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GEMM SHAPE: M=512, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=512, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=512, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=512, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=512, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=512, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=512, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=512, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=512, K=1024, N=1024 ===\n",
      "=== GEMM SHAPE: M=1024, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=1024, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=1024, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=1024, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=1024, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=1024, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=1024, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=1024, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=1024, K=1024, N=1024 ===\n",
      "=== GEMM SHAPE: M=2048, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=2048, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=2048, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=2048, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=2048, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=2048, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=2048, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=2048, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=2048, K=1024, N=1024 ===\n",
      "=== GEMM SHAPE: M=4096, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=4096, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=4096, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=4096, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=4096, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=4096, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=4096, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=4096, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=4096, K=1024, N=1024 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch_ms</th>\n",
       "      <th>speed_vs_torch</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_torch_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.060889</td>\n",
       "      <td>0.364229</td>\n",
       "      <td>5.981854</td>\n",
       "      <td>189.432118</td>\n",
       "      <td>31.667794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113535</td>\n",
       "      <td>0.664686</td>\n",
       "      <td>5.854477</td>\n",
       "      <td>193.950395</td>\n",
       "      <td>33.128560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040263</td>\n",
       "      <td>0.232254</td>\n",
       "      <td>5.768443</td>\n",
       "      <td>449.245510</td>\n",
       "      <td>77.879862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>0.342030</td>\n",
       "      <td>5.742029</td>\n",
       "      <td>220.044549</td>\n",
       "      <td>38.321744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.204391</td>\n",
       "      <td>5.580816</td>\n",
       "      <td>271.994050</td>\n",
       "      <td>48.737327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071848</td>\n",
       "      <td>0.372878</td>\n",
       "      <td>5.189799</td>\n",
       "      <td>269.994717</td>\n",
       "      <td>52.024117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046445</td>\n",
       "      <td>0.230206</td>\n",
       "      <td>4.956555</td>\n",
       "      <td>135.460993</td>\n",
       "      <td>27.329667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045830</td>\n",
       "      <td>0.224259</td>\n",
       "      <td>4.893245</td>\n",
       "      <td>188.755678</td>\n",
       "      <td>38.574744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045775</td>\n",
       "      <td>0.203730</td>\n",
       "      <td>4.450675</td>\n",
       "      <td>234.797943</td>\n",
       "      <td>52.755586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.058133</td>\n",
       "      <td>0.248542</td>\n",
       "      <td>4.275406</td>\n",
       "      <td>117.243935</td>\n",
       "      <td>27.422875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>0.145036</td>\n",
       "      <td>3.943190</td>\n",
       "      <td>149.668854</td>\n",
       "      <td>37.956285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>0.127764</td>\n",
       "      <td>3.712996</td>\n",
       "      <td>266.639698</td>\n",
       "      <td>71.812550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042685</td>\n",
       "      <td>0.158031</td>\n",
       "      <td>3.702224</td>\n",
       "      <td>150.461656</td>\n",
       "      <td>40.640886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.033742</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>3.581086</td>\n",
       "      <td>132.074132</td>\n",
       "      <td>36.881032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.051731</td>\n",
       "      <td>0.154078</td>\n",
       "      <td>2.978472</td>\n",
       "      <td>184.963389</td>\n",
       "      <td>62.100095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.129231</td>\n",
       "      <td>2.773855</td>\n",
       "      <td>78.774574</td>\n",
       "      <td>28.398953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054154</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>2.254669</td>\n",
       "      <td>96.813734</td>\n",
       "      <td>42.939216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034469</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>2.189505</td>\n",
       "      <td>140.694761</td>\n",
       "      <td>64.258711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.098944</td>\n",
       "      <td>2.153211</td>\n",
       "      <td>79.866921</td>\n",
       "      <td>37.092007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042350</td>\n",
       "      <td>0.079822</td>\n",
       "      <td>1.884822</td>\n",
       "      <td>77.374153</td>\n",
       "      <td>41.051175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       M     K     N  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "26  2048  1024  1024       64      128       64          4           3   \n",
       "35  4096  1024  1024       64      128       64          4           3   \n",
       "29  4096   256  1024       64      128       64          4           3   \n",
       "34  4096  1024   512       64      128       64          4           3   \n",
       "23  2048   512  1024       64      128       64          4           3   \n",
       "32  4096   512  1024       64      128       64          4           3   \n",
       "17  1024  1024  1024       64      128       64          4           3   \n",
       "33  4096  1024   256       64      128       64          4           3   \n",
       "31  4096   512   512       64      128       64          4           3   \n",
       "25  2048  1024   512       64      128       64          4           3   \n",
       "22  2048   512   512       64      128       64          4           3   \n",
       "20  2048   256  1024       64      128       64          4           3   \n",
       "30  4096   512   256       64      128       64          4           3   \n",
       "24  2048  1024   256       64      128       64          4           3   \n",
       "28  4096   256   512       64      128       64          4           3   \n",
       "8    512  1024  1024       64      128       64          4           3   \n",
       "14  1024   512  1024       64      128       64          4           3   \n",
       "19  2048   256   512       64      128       64          4           3   \n",
       "16  1024  1024   512       64      128       64          4           3   \n",
       "21  2048   512   256       64      128       64          4           3   \n",
       "\n",
       "    t_triton_ms  t_torch_ms  speed_vs_torch  bw_triton_GBs  bw_torch_GBs  \\\n",
       "26     0.060889    0.364229        5.981854     189.432118     31.667794   \n",
       "35     0.113535    0.664686        5.854477     193.950395     33.128560   \n",
       "29     0.040263    0.232254        5.768443     449.245510     77.879862   \n",
       "34     0.059566    0.342030        5.742029     220.044549     38.321744   \n",
       "23     0.036624    0.204391        5.580816     271.994050     48.737327   \n",
       "32     0.071848    0.372878        5.189799     269.994717     52.024117   \n",
       "17     0.046445    0.230206        4.956555     135.460993     27.329667   \n",
       "33     0.045830    0.224259        4.893245     188.755678     38.574744   \n",
       "31     0.045775    0.203730        4.450675     234.797943     52.755586   \n",
       "25     0.058133    0.248542        4.275406     117.243935     27.422875   \n",
       "22     0.036781    0.145036        3.943190     149.668854     37.956285   \n",
       "20     0.034410    0.127764        3.712996     266.639698     71.812550   \n",
       "30     0.042685    0.158031        3.702224     150.461656     40.640886   \n",
       "24     0.033742    0.120833        3.581086     132.074132     36.881032   \n",
       "28     0.051731    0.154078        2.978472     184.963389     62.100095   \n",
       "8      0.046589    0.129231        2.773855      78.774574     28.398953   \n",
       "14     0.054154    0.122100        2.254669      96.813734     42.939216   \n",
       "19     0.034469    0.075471        2.189505     140.694761     64.258711   \n",
       "16     0.045952    0.098944        2.153211      79.866921     37.092007   \n",
       "21     0.042350    0.079822        1.884822      77.374153     41.051175   \n",
       "\n",
       "    max_abs_err  \n",
       "26            0  \n",
       "35            0  \n",
       "29            0  \n",
       "34            0  \n",
       "23            0  \n",
       "32            0  \n",
       "17            0  \n",
       "33            0  \n",
       "31            0  \n",
       "25            0  \n",
       "22            0  \n",
       "20            0  \n",
       "30            0  \n",
       "24            0  \n",
       "28            0  \n",
       "8             0  \n",
       "14            0  \n",
       "19            0  \n",
       "16            0  \n",
       "21            0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gemm_tiles = {\n",
    "    \"BLOCK_M\": 64,\n",
    "    \"BLOCK_N\": 128,\n",
    "    \"BLOCK_K\": 64,\n",
    "    \"num_warps\": 4,\n",
    "    \"num_stages\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df_gemm = benchmark_gemm_int8_best_tiles(\n",
    "    best_gemm_tiles,\n",
    "    Ms=(512, 1024, 2048, 4096),\n",
    "    Ks=(256, 512, 1024),\n",
    "    Ns=(256, 512, 1024),\n",
    "    iters_per_shape=50,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "df_gemm.sort_values(\"speed_vs_torch\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc783a-dfe5-4306-9f8b-b4e9eee2cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588e49e-84e6-4dd0-b9a0-73010622b059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de16cb-e4b7-4598-9f28-5bfd9c6045b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942eb61-f0a3-4b67-bd66-e82b70f809bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
