{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f2a8f0f-02c0-43df-ba16-d0619ca0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from gemm_int8_kernel import gemm_int8_tc_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d9441b-f60b-4973-87ab-3f824eb054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemm_int8_tc(\n",
    "    A_q: torch.Tensor,   \n",
    "    B_q: torch.Tensor,  \n",
    "    *,\n",
    "    BLOCK_M: int = 64,\n",
    "    BLOCK_N: int = 64,\n",
    "    BLOCK_K: int = 32,\n",
    "    num_warps: int = 4,\n",
    "    num_stages: int = 2,\n",
    "):\n",
    "    if not A_q.is_contiguous():\n",
    "        A_q = A_q.contiguous()\n",
    "    if not B_q.is_contiguous():\n",
    "        B_q = B_q.contiguous()\n",
    "\n",
    "    M, K1 = A_q.shape\n",
    "    K2, N = B_q.shape\n",
    "    assert K1 == K2, f\"K mismatch: {K1} vs {K2}\"\n",
    "\n",
    "    assert K1 % 4 == 0, f\"K={K1} must be divisible by 4 for INT8 dot\"\n",
    "    assert BLOCK_K % 4 == 0, f\"BLOCK_K={BLOCK_K} must be divisible by 4\"\n",
    "\n",
    "    C_i32 = torch.empty((M, N), dtype=torch.int32, device=A_q.device)\n",
    "\n",
    "    a_m, a_k = A_q.stride()\n",
    "    b_k, b_n = B_q.stride()\n",
    "    c_m, c_n = C_i32.stride()\n",
    "\n",
    "    grid = (\n",
    "        triton.cdiv(M, BLOCK_M),\n",
    "        triton.cdiv(N, BLOCK_N),\n",
    "    )\n",
    "\n",
    "    gemm_int8_tc_kernel[grid](\n",
    "        A_q, B_q, C_i32,\n",
    "        M, N, K1,\n",
    "        a_m, a_k,\n",
    "        b_k, b_n,\n",
    "        c_m, c_n,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "        BLOCK_K=BLOCK_K,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=num_stages,\n",
    "    )\n",
    "\n",
    "    return C_i32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cceb33-ae6a-41c3-af13-4b6c12f62f2c",
   "metadata": {},
   "source": [
    "# title search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c24134f7-47f2-4d16-b951-2fb19db1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_once_gemm_int8_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters=100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    INT8 Triton GEMM vs torch FP16 matmul\n",
    "    \"\"\"\n",
    "    A_q = torch.randint(-128, 127, (M, K), device=device, dtype=torch.int8)\n",
    "    B_q = torch.randint(-128, 127, (K, N), device=device, dtype=torch.int8)\n",
    "\n",
    "    # FP16 baseline\n",
    "    A_f16 = A_q.to(torch.float16)\n",
    "    B_f16 = B_q.to(torch.float16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        C_ref = (A_f16 @ B_f16).float() \n",
    "\n",
    "    def _call_triton():\n",
    "        C_i32 = gemm_int8_tc(\n",
    "            A_q, B_q,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return C_i32\n",
    "        \n",
    "    # triton matmul int8\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # torch matmul FP16\n",
    "    def _call_torch():\n",
    "        return A_f16 @ B_f16\n",
    "    for _ in range(5):\n",
    "        _ = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref2 = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # bandwidth\n",
    "    bytes_moved = A_q.numel() + B_q.numel()      \n",
    "    bytes_moved += C_i32.numel() * 4            \n",
    "    bytes_moved = float(bytes_moved)\n",
    "\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch  = bytes_moved / t_torch  / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M, \"K\": K, \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e230b854-ec14-467d-a3c6-d55a1937a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tune_gemm_int8_tiles_for_shape(\n",
    "    M, K, N,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(1, 2, 4, 8),\n",
    "    stages=(2, 3,4),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    records = []\n",
    "    for BM in blocks_M:\n",
    "        for BN in blocks_N:\n",
    "            for BK in blocks_K:\n",
    "                if (K % 4 != 0) or (BK % 4 != 0):\n",
    "                    print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}: K/BK not multiple of 4\")\n",
    "                    continue\n",
    "\n",
    "                for W in warps:\n",
    "                    for S in stages:\n",
    "                        try:\n",
    "                            rec = bench_once_gemm_int8_vs_torch(\n",
    "                                M, K, N,\n",
    "                                BLOCK_M=BM,\n",
    "                                BLOCK_N=BN,\n",
    "                                BLOCK_K=BK,\n",
    "                                num_warps=W,\n",
    "                                num_stages=S,\n",
    "                                iters=iters,\n",
    "                                device=device,\n",
    "                            )\n",
    "                        except RuntimeError as e:\n",
    "                            print(f\"[SKIP] BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        print(\n",
    "                            f\"BM={BM}, BN={BN}, BK={BK}, W={W}, S={S}: \"\n",
    "                            f\"t_triton={rec['t_triton_ms']:.3f} ms, \"\n",
    "                            f\"speed_vs_torch={rec['speed_vs_torch']:.3f}x, \"\n",
    "                        )\n",
    "                        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No valid tile configs found for this GEMM shape\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c363e02-f7c4-4911-a6da-ed7305f15f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM=32, BN=32, BK=32, W=2, S=2: t_triton=0.239 ms, speed_vs_torch=1.003x, err=inf\n",
      "BM=32, BN=32, BK=32, W=2, S=3: t_triton=0.251 ms, speed_vs_torch=0.910x, err=inf\n",
      "BM=32, BN=32, BK=32, W=4, S=2: t_triton=0.404 ms, speed_vs_torch=0.569x, err=inf\n",
      "BM=32, BN=32, BK=32, W=4, S=3: t_triton=0.408 ms, speed_vs_torch=0.541x, err=inf\n",
      "BM=32, BN=32, BK=32, W=8, S=2: t_triton=0.492 ms, speed_vs_torch=0.443x, err=inf\n",
      "BM=32, BN=32, BK=32, W=8, S=3: t_triton=0.484 ms, speed_vs_torch=0.447x, err=inf\n",
      "BM=32, BN=32, BK=64, W=2, S=2: t_triton=0.218 ms, speed_vs_torch=1.095x, err=inf\n",
      "BM=32, BN=32, BK=64, W=2, S=3: t_triton=0.225 ms, speed_vs_torch=1.012x, err=inf\n",
      "BM=32, BN=32, BK=64, W=4, S=2: t_triton=0.303 ms, speed_vs_torch=0.720x, err=inf\n",
      "BM=32, BN=32, BK=64, W=4, S=3: t_triton=0.343 ms, speed_vs_torch=0.636x, err=inf\n",
      "BM=32, BN=32, BK=64, W=8, S=2: t_triton=0.380 ms, speed_vs_torch=0.605x, err=inf\n",
      "BM=32, BN=32, BK=64, W=8, S=3: t_triton=0.373 ms, speed_vs_torch=0.582x, err=inf\n",
      "BM=32, BN=32, BK=128, W=2, S=2: t_triton=0.211 ms, speed_vs_torch=1.085x, err=inf\n",
      "BM=32, BN=32, BK=128, W=2, S=3: t_triton=0.247 ms, speed_vs_torch=0.911x, err=inf\n",
      "BM=32, BN=32, BK=128, W=4, S=2: t_triton=0.323 ms, speed_vs_torch=0.674x, err=inf\n",
      "BM=32, BN=32, BK=128, W=4, S=3: t_triton=0.336 ms, speed_vs_torch=0.651x, err=inf\n",
      "BM=32, BN=32, BK=128, W=8, S=2: t_triton=0.331 ms, speed_vs_torch=0.679x, err=inf\n",
      "BM=32, BN=32, BK=128, W=8, S=3: t_triton=0.351 ms, speed_vs_torch=0.623x, err=inf\n",
      "BM=32, BN=64, BK=32, W=2, S=2: t_triton=0.184 ms, speed_vs_torch=1.205x, err=inf\n",
      "BM=32, BN=64, BK=32, W=2, S=3: t_triton=0.189 ms, speed_vs_torch=1.166x, err=inf\n",
      "BM=32, BN=64, BK=32, W=4, S=2: t_triton=0.212 ms, speed_vs_torch=1.043x, err=inf\n",
      "BM=32, BN=64, BK=32, W=4, S=3: t_triton=0.219 ms, speed_vs_torch=1.024x, err=inf\n",
      "BM=32, BN=64, BK=32, W=8, S=2: t_triton=0.345 ms, speed_vs_torch=0.635x, err=inf\n",
      "BM=32, BN=64, BK=32, W=8, S=3: t_triton=0.339 ms, speed_vs_torch=0.663x, err=inf\n",
      "BM=32, BN=64, BK=64, W=2, S=2: t_triton=0.176 ms, speed_vs_torch=1.245x, err=inf\n",
      "BM=32, BN=64, BK=64, W=2, S=3: t_triton=0.175 ms, speed_vs_torch=1.277x, err=inf\n",
      "BM=32, BN=64, BK=64, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=1.286x, err=inf\n",
      "BM=32, BN=64, BK=64, W=4, S=3: t_triton=0.186 ms, speed_vs_torch=1.215x, err=inf\n",
      "BM=32, BN=64, BK=64, W=8, S=2: t_triton=0.293 ms, speed_vs_torch=0.746x, err=inf\n",
      "BM=32, BN=64, BK=64, W=8, S=3: t_triton=0.321 ms, speed_vs_torch=0.683x, err=inf\n",
      "BM=32, BN=64, BK=128, W=2, S=2: t_triton=0.173 ms, speed_vs_torch=1.295x, err=inf\n",
      "BM=32, BN=64, BK=128, W=2, S=3: t_triton=0.180 ms, speed_vs_torch=1.233x, err=inf\n",
      "BM=32, BN=64, BK=128, W=4, S=2: t_triton=0.180 ms, speed_vs_torch=1.287x, err=inf\n",
      "BM=32, BN=64, BK=128, W=4, S=3: t_triton=0.191 ms, speed_vs_torch=1.170x, err=inf\n",
      "BM=32, BN=64, BK=128, W=8, S=2: t_triton=0.275 ms, speed_vs_torch=0.806x, err=inf\n",
      "BM=32, BN=64, BK=128, W=8, S=3: t_triton=0.304 ms, speed_vs_torch=0.721x, err=inf\n",
      "BM=32, BN=128, BK=32, W=2, S=2: t_triton=0.163 ms, speed_vs_torch=1.334x, err=inf\n",
      "BM=32, BN=128, BK=32, W=2, S=3: t_triton=0.158 ms, speed_vs_torch=1.390x, err=inf\n",
      "BM=32, BN=128, BK=32, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=1.262x, err=inf\n",
      "BM=32, BN=128, BK=32, W=4, S=3: t_triton=0.174 ms, speed_vs_torch=1.287x, err=inf\n",
      "BM=32, BN=128, BK=32, W=8, S=2: t_triton=0.223 ms, speed_vs_torch=0.973x, err=inf\n",
      "BM=32, BN=128, BK=32, W=8, S=3: t_triton=0.213 ms, speed_vs_torch=1.038x, err=inf\n",
      "BM=32, BN=128, BK=64, W=2, S=2: t_triton=0.157 ms, speed_vs_torch=1.400x, err=inf\n",
      "BM=32, BN=128, BK=64, W=2, S=3: t_triton=0.153 ms, speed_vs_torch=1.445x, err=inf\n",
      "BM=32, BN=128, BK=64, W=4, S=2: t_triton=0.164 ms, speed_vs_torch=1.408x, err=inf\n",
      "BM=32, BN=128, BK=64, W=4, S=3: t_triton=0.161 ms, speed_vs_torch=1.386x, err=inf\n",
      "BM=32, BN=128, BK=64, W=8, S=2: t_triton=0.184 ms, speed_vs_torch=1.214x, err=inf\n",
      "BM=32, BN=128, BK=64, W=8, S=3: t_triton=0.189 ms, speed_vs_torch=1.200x, err=inf\n",
      "BM=32, BN=128, BK=128, W=2, S=2: t_triton=0.166 ms, speed_vs_torch=1.319x, err=inf\n",
      "BM=32, BN=128, BK=128, W=2, S=3: t_triton=0.195 ms, speed_vs_torch=1.125x, err=inf\n",
      "BM=32, BN=128, BK=128, W=4, S=2: t_triton=0.164 ms, speed_vs_torch=1.392x, err=inf\n",
      "BM=32, BN=128, BK=128, W=4, S=3: t_triton=0.168 ms, speed_vs_torch=1.323x, err=inf\n",
      "BM=32, BN=128, BK=128, W=8, S=2: t_triton=0.177 ms, speed_vs_torch=1.243x, err=inf\n",
      "BM=32, BN=128, BK=128, W=8, S=3: t_triton=0.183 ms, speed_vs_torch=1.247x, err=inf\n",
      "BM=64, BN=32, BK=32, W=2, S=2: t_triton=0.193 ms, speed_vs_torch=1.157x, err=inf\n",
      "BM=64, BN=32, BK=32, W=2, S=3: t_triton=0.192 ms, speed_vs_torch=1.173x, err=inf\n",
      "BM=64, BN=32, BK=32, W=4, S=2: t_triton=0.220 ms, speed_vs_torch=1.004x, err=inf\n",
      "BM=64, BN=32, BK=32, W=4, S=3: t_triton=0.212 ms, speed_vs_torch=1.063x, err=inf\n",
      "BM=64, BN=32, BK=32, W=8, S=2: t_triton=0.358 ms, speed_vs_torch=0.628x, err=inf\n",
      "BM=64, BN=32, BK=32, W=8, S=3: t_triton=0.357 ms, speed_vs_torch=0.609x, err=inf\n",
      "BM=64, BN=32, BK=64, W=2, S=2: t_triton=0.177 ms, speed_vs_torch=1.256x, err=inf\n",
      "BM=64, BN=32, BK=64, W=2, S=3: t_triton=0.199 ms, speed_vs_torch=1.111x, err=inf\n",
      "BM=64, BN=32, BK=64, W=4, S=2: t_triton=0.181 ms, speed_vs_torch=1.230x, err=inf\n",
      "BM=64, BN=32, BK=64, W=4, S=3: t_triton=0.206 ms, speed_vs_torch=1.107x, err=inf\n",
      "BM=64, BN=32, BK=64, W=8, S=2: t_triton=0.281 ms, speed_vs_torch=0.779x, err=inf\n",
      "BM=64, BN=32, BK=64, W=8, S=3: t_triton=0.286 ms, speed_vs_torch=0.765x, err=inf\n",
      "BM=64, BN=32, BK=128, W=2, S=2: t_triton=0.201 ms, speed_vs_torch=1.120x, err=inf\n",
      "BM=64, BN=32, BK=128, W=2, S=3: t_triton=0.190 ms, speed_vs_torch=1.170x, err=inf\n",
      "BM=64, BN=32, BK=128, W=4, S=2: t_triton=0.213 ms, speed_vs_torch=1.050x, err=inf\n",
      "BM=64, BN=32, BK=128, W=4, S=3: t_triton=0.230 ms, speed_vs_torch=0.957x, err=inf\n",
      "BM=64, BN=32, BK=128, W=8, S=2: t_triton=0.273 ms, speed_vs_torch=0.807x, err=inf\n",
      "BM=64, BN=32, BK=128, W=8, S=3: t_triton=0.295 ms, speed_vs_torch=0.736x, err=inf\n",
      "BM=64, BN=64, BK=32, W=2, S=2: t_triton=0.122 ms, speed_vs_torch=1.801x, err=inf\n",
      "BM=64, BN=64, BK=32, W=2, S=3: t_triton=0.127 ms, speed_vs_torch=1.774x, err=inf\n",
      "BM=64, BN=64, BK=32, W=4, S=2: t_triton=0.178 ms, speed_vs_torch=1.276x, err=inf\n",
      "BM=64, BN=64, BK=32, W=4, S=3: t_triton=0.168 ms, speed_vs_torch=1.318x, err=inf\n",
      "BM=64, BN=64, BK=32, W=8, S=2: t_triton=0.208 ms, speed_vs_torch=1.054x, err=inf\n",
      "BM=64, BN=64, BK=32, W=8, S=3: t_triton=0.202 ms, speed_vs_torch=1.100x, err=inf\n",
      "BM=64, BN=64, BK=64, W=2, S=2: t_triton=0.114 ms, speed_vs_torch=1.914x, err=inf\n",
      "BM=64, BN=64, BK=64, W=2, S=3: t_triton=0.130 ms, speed_vs_torch=1.754x, err=inf\n",
      "BM=64, BN=64, BK=64, W=4, S=2: t_triton=0.154 ms, speed_vs_torch=1.491x, err=inf\n",
      "BM=64, BN=64, BK=64, W=4, S=3: t_triton=0.159 ms, speed_vs_torch=1.377x, err=inf\n",
      "BM=64, BN=64, BK=64, W=8, S=2: t_triton=0.177 ms, speed_vs_torch=1.230x, err=inf\n",
      "BM=64, BN=64, BK=64, W=8, S=3: t_triton=0.169 ms, speed_vs_torch=1.368x, err=inf\n",
      "BM=64, BN=64, BK=128, W=2, S=2: t_triton=0.129 ms, speed_vs_torch=1.710x, err=inf\n",
      "BM=64, BN=64, BK=128, W=2, S=3: t_triton=0.142 ms, speed_vs_torch=1.554x, err=inf\n",
      "BM=64, BN=64, BK=128, W=4, S=2: t_triton=0.160 ms, speed_vs_torch=1.442x, err=inf\n",
      "BM=64, BN=64, BK=128, W=4, S=3: t_triton=0.162 ms, speed_vs_torch=1.364x, err=inf\n",
      "BM=64, BN=64, BK=128, W=8, S=2: t_triton=0.169 ms, speed_vs_torch=1.290x, err=inf\n",
      "BM=64, BN=64, BK=128, W=8, S=3: t_triton=0.167 ms, speed_vs_torch=1.363x, err=inf\n",
      "BM=64, BN=128, BK=32, W=2, S=2: t_triton=0.126 ms, speed_vs_torch=1.746x, err=inf\n",
      "BM=64, BN=128, BK=32, W=2, S=3: t_triton=0.113 ms, speed_vs_torch=1.956x, err=inf\n",
      "BM=64, BN=128, BK=32, W=4, S=2: t_triton=0.120 ms, speed_vs_torch=1.844x, err=inf\n",
      "BM=64, BN=128, BK=32, W=4, S=3: t_triton=0.119 ms, speed_vs_torch=1.855x, err=inf\n",
      "BM=64, BN=128, BK=32, W=8, S=2: t_triton=0.192 ms, speed_vs_torch=1.141x, err=inf\n",
      "BM=64, BN=128, BK=32, W=8, S=3: t_triton=0.178 ms, speed_vs_torch=1.270x, err=inf\n",
      "BM=64, BN=128, BK=64, W=2, S=2: t_triton=0.119 ms, speed_vs_torch=1.842x, err=inf\n",
      "BM=64, BN=128, BK=64, W=2, S=3: t_triton=0.105 ms, speed_vs_torch=2.085x, err=inf\n",
      "BM=64, BN=128, BK=64, W=4, S=2: t_triton=0.109 ms, speed_vs_torch=2.030x, err=inf\n",
      "BM=64, BN=128, BK=64, W=4, S=3: t_triton=0.116 ms, speed_vs_torch=1.981x, err=inf\n",
      "BM=64, BN=128, BK=64, W=8, S=2: t_triton=0.170 ms, speed_vs_torch=1.284x, err=inf\n",
      "BM=64, BN=128, BK=64, W=8, S=3: t_triton=0.150 ms, speed_vs_torch=1.518x, err=inf\n",
      "BM=64, BN=128, BK=128, W=2, S=2: t_triton=0.125 ms, speed_vs_torch=1.757x, err=inf\n",
      "BM=64, BN=128, BK=128, W=2, S=3: t_triton=0.164 ms, speed_vs_torch=1.335x, err=inf\n",
      "BM=64, BN=128, BK=128, W=4, S=2: t_triton=0.112 ms, speed_vs_torch=1.997x, err=inf\n",
      "BM=64, BN=128, BK=128, W=4, S=3: t_triton=0.115 ms, speed_vs_torch=1.909x, err=inf\n",
      "BM=64, BN=128, BK=128, W=8, S=2: t_triton=0.165 ms, speed_vs_torch=1.331x, err=inf\n",
      "BM=64, BN=128, BK=128, W=8, S=3: t_triton=0.154 ms, speed_vs_torch=1.491x, err=inf\n",
      "BM=128, BN=32, BK=32, W=2, S=2: t_triton=0.133 ms, speed_vs_torch=1.678x, err=inf\n",
      "BM=128, BN=32, BK=32, W=2, S=3: t_triton=0.139 ms, speed_vs_torch=1.634x, err=inf\n",
      "BM=128, BN=32, BK=32, W=4, S=2: t_triton=0.185 ms, speed_vs_torch=1.223x, err=inf\n",
      "BM=128, BN=32, BK=32, W=4, S=3: t_triton=0.183 ms, speed_vs_torch=1.266x, err=inf\n",
      "BM=128, BN=32, BK=32, W=8, S=2: t_triton=0.221 ms, speed_vs_torch=0.998x, err=inf\n",
      "BM=128, BN=32, BK=32, W=8, S=3: t_triton=0.218 ms, speed_vs_torch=1.020x, err=inf\n",
      "BM=128, BN=32, BK=64, W=2, S=2: t_triton=0.125 ms, speed_vs_torch=1.847x, err=inf\n",
      "BM=128, BN=32, BK=64, W=2, S=3: t_triton=0.127 ms, speed_vs_torch=1.794x, err=inf\n",
      "BM=128, BN=32, BK=64, W=4, S=2: t_triton=0.163 ms, speed_vs_torch=1.397x, err=inf\n",
      "BM=128, BN=32, BK=64, W=4, S=3: t_triton=0.174 ms, speed_vs_torch=1.273x, err=inf\n",
      "BM=128, BN=32, BK=64, W=8, S=2: t_triton=0.179 ms, speed_vs_torch=1.224x, err=inf\n",
      "BM=128, BN=32, BK=64, W=8, S=3: t_triton=0.179 ms, speed_vs_torch=1.340x, err=inf\n",
      "BM=128, BN=32, BK=128, W=2, S=2: t_triton=0.140 ms, speed_vs_torch=1.605x, err=inf\n",
      "BM=128, BN=32, BK=128, W=2, S=3: t_triton=0.165 ms, speed_vs_torch=1.338x, err=inf\n",
      "BM=128, BN=32, BK=128, W=4, S=2: t_triton=0.175 ms, speed_vs_torch=1.312x, err=inf\n",
      "BM=128, BN=32, BK=128, W=4, S=3: t_triton=0.185 ms, speed_vs_torch=1.191x, err=inf\n",
      "BM=128, BN=32, BK=128, W=8, S=2: t_triton=0.177 ms, speed_vs_torch=1.256x, err=inf\n",
      "BM=128, BN=32, BK=128, W=8, S=3: t_triton=0.190 ms, speed_vs_torch=1.218x, err=inf\n",
      "BM=128, BN=64, BK=32, W=2, S=2: t_triton=0.138 ms, speed_vs_torch=1.592x, err=inf\n",
      "BM=128, BN=64, BK=32, W=2, S=3: t_triton=0.114 ms, speed_vs_torch=1.938x, err=inf\n",
      "BM=128, BN=64, BK=32, W=4, S=2: t_triton=0.134 ms, speed_vs_torch=1.668x, err=inf\n",
      "BM=128, BN=64, BK=32, W=4, S=3: t_triton=0.124 ms, speed_vs_torch=1.781x, err=inf\n",
      "BM=128, BN=64, BK=32, W=8, S=2: t_triton=0.184 ms, speed_vs_torch=1.189x, err=inf\n",
      "BM=128, BN=64, BK=32, W=8, S=3: t_triton=0.165 ms, speed_vs_torch=1.384x, err=inf\n",
      "BM=128, BN=64, BK=64, W=2, S=2: t_triton=0.127 ms, speed_vs_torch=1.758x, err=inf\n",
      "BM=128, BN=64, BK=64, W=2, S=3: t_triton=0.113 ms, speed_vs_torch=1.938x, err=inf\n",
      "BM=128, BN=64, BK=64, W=4, S=2: t_triton=0.107 ms, speed_vs_torch=2.059x, err=inf\n",
      "BM=128, BN=64, BK=64, W=4, S=3: t_triton=0.119 ms, speed_vs_torch=1.890x, err=inf\n",
      "BM=128, BN=64, BK=64, W=8, S=2: t_triton=0.175 ms, speed_vs_torch=1.243x, err=inf\n",
      "BM=128, BN=64, BK=64, W=8, S=3: t_triton=0.149 ms, speed_vs_torch=1.556x, err=inf\n",
      "BM=128, BN=64, BK=128, W=2, S=2: t_triton=0.198 ms, speed_vs_torch=1.103x, err=inf\n",
      "BM=128, BN=64, BK=128, W=2, S=3: t_triton=0.211 ms, speed_vs_torch=1.027x, err=inf\n",
      "BM=128, BN=64, BK=128, W=4, S=2: t_triton=0.132 ms, speed_vs_torch=1.872x, err=inf\n",
      "BM=128, BN=64, BK=128, W=4, S=3: t_triton=0.134 ms, speed_vs_torch=1.632x, err=inf\n",
      "BM=128, BN=64, BK=128, W=8, S=2: t_triton=0.154 ms, speed_vs_torch=1.429x, err=inf\n",
      "BM=128, BN=64, BK=128, W=8, S=3: t_triton=0.153 ms, speed_vs_torch=1.488x, err=inf\n",
      "BM=128, BN=128, BK=32, W=2, S=2: t_triton=0.395 ms, speed_vs_torch=0.573x, err=inf\n",
      "BM=128, BN=128, BK=32, W=2, S=3: t_triton=0.463 ms, speed_vs_torch=0.478x, err=inf\n",
      "BM=128, BN=128, BK=32, W=4, S=2: t_triton=0.126 ms, speed_vs_torch=1.719x, err=inf\n",
      "BM=128, BN=128, BK=32, W=4, S=3: t_triton=0.117 ms, speed_vs_torch=1.933x, err=inf\n",
      "BM=128, BN=128, BK=32, W=8, S=2: t_triton=0.141 ms, speed_vs_torch=1.588x, err=inf\n",
      "BM=128, BN=128, BK=32, W=8, S=3: t_triton=0.136 ms, speed_vs_torch=1.632x, err=inf\n",
      "BM=128, BN=128, BK=64, W=2, S=2: t_triton=0.518 ms, speed_vs_torch=0.462x, err=inf\n",
      "BM=128, BN=128, BK=64, W=2, S=3: t_triton=0.656 ms, speed_vs_torch=0.343x, err=inf\n",
      "BM=128, BN=128, BK=64, W=4, S=2: t_triton=0.119 ms, speed_vs_torch=1.915x, err=inf\n",
      "BM=128, BN=128, BK=64, W=4, S=3: t_triton=0.102 ms, speed_vs_torch=2.222x, err=inf\n",
      "BM=128, BN=128, BK=64, W=8, S=2: t_triton=0.174 ms, speed_vs_torch=1.294x, err=inf\n",
      "BM=128, BN=128, BK=64, W=8, S=3: t_triton=0.124 ms, speed_vs_torch=1.803x, err=inf\n",
      "BM=128, BN=128, BK=128, W=2, S=2: t_triton=0.597 ms, speed_vs_torch=0.367x, err=inf\n",
      "BM=128, BN=128, BK=128, W=2, S=3: t_triton=0.624 ms, speed_vs_torch=0.351x, err=inf\n",
      "BM=128, BN=128, BK=128, W=4, S=2: t_triton=0.136 ms, speed_vs_torch=1.597x, err=inf\n",
      "BM=128, BN=128, BK=128, W=4, S=3: t_triton=0.181 ms, speed_vs_torch=1.333x, err=inf\n",
      "BM=128, BN=128, BK=128, W=8, S=2: t_triton=0.188 ms, speed_vs_torch=1.318x, err=inf\n",
      "BM=128, BN=128, BK=128, W=8, S=3: t_triton=0.124 ms, speed_vs_torch=1.746x, err=inf\n"
     ]
    }
   ],
   "source": [
    "df_gemm_tiles = tune_gemm_int8_tiles_for_shape(\n",
    "    M=4096, K=1024, N=1024,\n",
    "    blocks_M=(32, 64, 128),\n",
    "    blocks_N=(32, 64, 128),\n",
    "    blocks_K=(32, 64, 128),\n",
    "    warps=(2, 4, 8),\n",
    "    stages=(2, 3),\n",
    "    iters=200,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c29522-a1df-4062-adbc-9edc3a7e4709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch_ms</th>\n",
       "      <th>speed_vs_torch</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_torch_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.102371</td>\n",
       "      <td>0.227427</td>\n",
       "      <td>2.221593</td>\n",
       "      <td>215.100350</td>\n",
       "      <td>96.822552</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.105459</td>\n",
       "      <td>0.219851</td>\n",
       "      <td>2.084713</td>\n",
       "      <td>208.803098</td>\n",
       "      <td>100.159135</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.107344</td>\n",
       "      <td>0.220985</td>\n",
       "      <td>2.058654</td>\n",
       "      <td>205.135243</td>\n",
       "      <td>99.645308</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.108656</td>\n",
       "      <td>0.220532</td>\n",
       "      <td>2.029643</td>\n",
       "      <td>202.659345</td>\n",
       "      <td>99.849741</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.112499</td>\n",
       "      <td>0.224624</td>\n",
       "      <td>1.996683</td>\n",
       "      <td>195.736770</td>\n",
       "      <td>98.030955</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.112915</td>\n",
       "      <td>0.220857</td>\n",
       "      <td>1.955962</td>\n",
       "      <td>195.015222</td>\n",
       "      <td>99.702993</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113490</td>\n",
       "      <td>0.219917</td>\n",
       "      <td>1.937766</td>\n",
       "      <td>194.026760</td>\n",
       "      <td>100.129099</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.114204</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>1.913770</td>\n",
       "      <td>192.813812</td>\n",
       "      <td>100.750794</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.114252</td>\n",
       "      <td>0.221440</td>\n",
       "      <td>1.938172</td>\n",
       "      <td>192.732780</td>\n",
       "      <td>99.440503</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.115337</td>\n",
       "      <td>0.220175</td>\n",
       "      <td>1.908972</td>\n",
       "      <td>190.919420</td>\n",
       "      <td>100.011661</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        M     K     N  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "153  4096  1024  1024      128      128       64          4           3   \n",
       "97   4096  1024  1024       64      128       64          2           3   \n",
       "134  4096  1024  1024      128       64       64          4           2   \n",
       "98   4096  1024  1024       64      128       64          4           2   \n",
       "104  4096  1024  1024       64      128      128          4           2   \n",
       "91   4096  1024  1024       64      128       32          2           3   \n",
       "133  4096  1024  1024      128       64       64          2           3   \n",
       "78   4096  1024  1024       64       64       64          2           2   \n",
       "127  4096  1024  1024      128       64       32          2           3   \n",
       "105  4096  1024  1024       64      128      128          4           3   \n",
       "\n",
       "     t_triton_ms  t_torch_ms  speed_vs_torch  bw_triton_GBs  bw_torch_GBs  \\\n",
       "153     0.102371    0.227427        2.221593     215.100350     96.822552   \n",
       "97      0.105459    0.219851        2.084713     208.803098    100.159135   \n",
       "134     0.107344    0.220985        2.058654     205.135243     99.645308   \n",
       "98      0.108656    0.220532        2.029643     202.659345     99.849741   \n",
       "104     0.112499    0.224624        1.996683     195.736770     98.030955   \n",
       "91      0.112915    0.220857        1.955962     195.015222     99.702993   \n",
       "133     0.113490    0.219917        1.937766     194.026760    100.129099   \n",
       "78      0.114204    0.218560        1.913770     192.813812    100.750794   \n",
       "127     0.114252    0.221440        1.938172     192.732780     99.440503   \n",
       "105     0.115337    0.220175        1.908972     190.919420    100.011661   \n",
       "\n",
       "     max_abs_err  \n",
       "153          inf  \n",
       "97           inf  \n",
       "134          inf  \n",
       "98           inf  \n",
       "104          inf  \n",
       "91           inf  \n",
       "133          inf  \n",
       "78           inf  \n",
       "127          inf  \n",
       "105          inf  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemm_tiles.sort_values(\"t_triton_ms\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb321830-e82a-43ae-96f5-691be9a9ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa596d30-56f6-4fce-ae4e-684416db22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_GEMM_BEST_BLOCK_M = 128\n",
    "INT8_GEMM_BEST_BLOCK_N = 128\n",
    "INT8_GEMM_BEST_BLOCK_K = 64\n",
    "INT8_GEMM_BEST_WARPS   = 4\n",
    "INT8_GEMM_BEST_STAGES  = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ac81a-3b19-4e5b-b47a-fabdbdac525b",
   "metadata": {},
   "source": [
    "# BENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78cf0a0-ba04-4581-99ca-b910ad54c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Ожидаем, что где-то выше уже есть:\n",
    "# from your_module import gemm_int8_tc\n",
    "# (тот, что берёт A_q[int8][M,K], B_q[int8][K,N] и возвращает C_i32[int32][M,N])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench_once_gemm_int8_vs_torch(\n",
    "    M, K, N,\n",
    "    BLOCK_M,\n",
    "    BLOCK_N,\n",
    "    BLOCK_K,\n",
    "    num_warps,\n",
    "    num_stages,\n",
    "    iters: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Бенч для ОДНОГО GEMM-шейпа и ОДНОГО набора тайлов:\n",
    "      - A_q: [M, K] int8\n",
    "      - B_q: [K, N] int8\n",
    "      - Triton: gemm_int8_tc -> C_i32 [M,N] int32\n",
    "      - Torch: (A_q.float() @ B_q.float()).to(int32)\n",
    "      - считаем:\n",
    "          t_triton_ms, t_torch_ms,\n",
    "          speed_vs_torch,\n",
    "          max_abs_err\n",
    "    \"\"\"\n",
    "    # --- проверка кратности для INT8 dot ---\n",
    "    if (K % 4) != 0 or (BLOCK_K % 4) != 0:\n",
    "        raise RuntimeError(f\"K={K} или BLOCK_K={BLOCK_K} не кратны 4 — INT8 dot невалиден\")\n",
    "\n",
    "    # --- данные ---\n",
    "    A_q = torch.randint(-128, 127, (M, K), device=device, dtype=torch.int8)\n",
    "    B_q = torch.randint(-128, 127, (K, N), device=device, dtype=torch.int8)\n",
    "\n",
    "    # референс: fp16 GEMM + cast к int32\n",
    "    A_f = A_q.float()\n",
    "    B_f = B_q.float()\n",
    "\n",
    "    def _call_triton():\n",
    "        C_i32 = gemm_int8_tc(\n",
    "            A_q, B_q,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "            BLOCK_K=BLOCK_K,\n",
    "            num_warps=num_warps,\n",
    "            num_stages=num_stages,\n",
    "        )\n",
    "        return C_i32\n",
    "\n",
    "    def _call_torch():\n",
    "        C_ref_f = A_f @ B_f          # [M,N] fp16, cuBLAS\n",
    "        C_ref_i32 = C_ref_f.to(torch.int32)\n",
    "        return C_ref_i32\n",
    "\n",
    "    # --- прогрев Triton ---\n",
    "    for _ in range(5):\n",
    "        _ = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # измеряем Triton\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_i32 = _call_triton()\n",
    "    torch.cuda.synchronize()\n",
    "    t_triton = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # --- прогрев Torch ---\n",
    "    for _ in range(5):\n",
    "        _ = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # измеряем Torch\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        C_ref_i32 = _call_torch()\n",
    "    torch.cuda.synchronize()\n",
    "    t_torch = (time.perf_counter() - t0) / iters\n",
    "\n",
    "    # --- ошибка ---\n",
    "    max_abs_err = (C_i32 - C_ref_i32).abs().max().item()\n",
    "\n",
    "    # простая оценка \"байт\" (очень грубо): читаем A и B, пишем C\n",
    "    bytes_moved = A_q.numel() + B_q.numel() + C_i32.numel()  # в \"элементах\"\n",
    "    # переводим в байты: int8=1 байт, int32=4 байта\n",
    "    bytes_moved = A_q.numel() * 1 + B_q.numel() * 1 + C_i32.numel() * 4\n",
    "    bw_triton = bytes_moved / t_triton / 1e9\n",
    "    bw_torch = bytes_moved / t_torch / 1e9\n",
    "\n",
    "    return {\n",
    "        \"M\": M,\n",
    "        \"K\": K,\n",
    "        \"N\": N,\n",
    "        \"BLOCK_M\": BLOCK_M,\n",
    "        \"BLOCK_N\": BLOCK_N,\n",
    "        \"BLOCK_K\": BLOCK_K,\n",
    "        \"num_warps\": num_warps,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"t_triton_ms\": t_triton * 1e3,\n",
    "        \"t_torch_ms\": t_torch * 1e3,\n",
    "        \"speed_vs_torch\": t_torch / t_triton,\n",
    "        \"bw_triton_GBs\": bw_triton,\n",
    "        \"bw_torch_GBs\": bw_torch,\n",
    "        \"max_abs_err\": max_abs_err,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_gemm_int8_best_tiles(\n",
    "    tiles_cfg: dict,\n",
    "    Ms=(1024, 2048, 4096),\n",
    "    Ks=(256, 512, 1024),\n",
    "    Ns=(256, 512, 1024),\n",
    "    iters_per_shape: int = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Сравнение gemm_int8_tc vs torch матрицы на сетке (M, K, N)\n",
    "    при фиксированных лучших тайлах tiles_cfg.\n",
    "\n",
    "    tiles_cfg: dict с ключами:\n",
    "      - \"BLOCK_M\"\n",
    "      - \"BLOCK_N\"\n",
    "      - \"BLOCK_K\"\n",
    "      - \"num_warps\"\n",
    "      - \"num_stages\"\n",
    "    \"\"\"\n",
    "    BM = tiles_cfg[\"BLOCK_M\"]\n",
    "    BN = tiles_cfg[\"BLOCK_N\"]\n",
    "    BK = tiles_cfg[\"BLOCK_K\"]\n",
    "    NW = tiles_cfg[\"num_warps\"]\n",
    "    NS = tiles_cfg[\"num_stages\"]\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for M in Ms:\n",
    "        for K in Ks:\n",
    "            for N in Ns:\n",
    "                print(f\"=== GEMM SHAPE: M={M}, K={K}, N={N} ===\")\n",
    "\n",
    "                # Проверяем кратность для INT8 dot\n",
    "                if (K % 4) != 0 or (BK % 4) != 0:\n",
    "                    print(f\"[SKIP] M={M}, K={K}, N={N}: K/BLOCK_K not multiple of 4\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    rec = bench_once_gemm_int8_vs_torch(\n",
    "                        M, K, N,\n",
    "                        BLOCK_M=BM,\n",
    "                        BLOCK_N=BN,\n",
    "                        BLOCK_K=BK,\n",
    "                        num_warps=NW,\n",
    "                        num_stages=NS,\n",
    "                        iters=iters_per_shape,\n",
    "                        device=device,\n",
    "                    )\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"[SKIP] M={M}, K={K}, N={N}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if rec[\"max_abs_err\"] != 0:\n",
    "                    print(f\"[WRONG] M={M}, K={K}, N={N}, err={rec['max_abs_err']}\")\n",
    "                    continue\n",
    "\n",
    "                records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        print(\"[WARN] no successful GEMM records\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58d9d9d-5ab5-4988-8bcc-905e4829f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GEMM SHAPE: M=512, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=512, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=512, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=512, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=512, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=512, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=512, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=512, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=512, K=1024, N=1024 ===\n",
      "=== GEMM SHAPE: M=1024, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=1024, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=1024, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=1024, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=1024, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=1024, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=1024, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=1024, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=1024, K=1024, N=1024 ===\n",
      "=== GEMM SHAPE: M=2048, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=2048, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=2048, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=2048, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=2048, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=2048, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=2048, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=2048, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=2048, K=1024, N=1024 ===\n",
      "=== GEMM SHAPE: M=4096, K=256, N=256 ===\n",
      "=== GEMM SHAPE: M=4096, K=256, N=512 ===\n",
      "=== GEMM SHAPE: M=4096, K=256, N=1024 ===\n",
      "=== GEMM SHAPE: M=4096, K=512, N=256 ===\n",
      "=== GEMM SHAPE: M=4096, K=512, N=512 ===\n",
      "=== GEMM SHAPE: M=4096, K=512, N=1024 ===\n",
      "=== GEMM SHAPE: M=4096, K=1024, N=256 ===\n",
      "=== GEMM SHAPE: M=4096, K=1024, N=512 ===\n",
      "=== GEMM SHAPE: M=4096, K=1024, N=1024 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>BLOCK_M</th>\n",
       "      <th>BLOCK_N</th>\n",
       "      <th>BLOCK_K</th>\n",
       "      <th>num_warps</th>\n",
       "      <th>num_stages</th>\n",
       "      <th>t_triton_ms</th>\n",
       "      <th>t_torch_ms</th>\n",
       "      <th>speed_vs_torch</th>\n",
       "      <th>bw_triton_GBs</th>\n",
       "      <th>bw_torch_GBs</th>\n",
       "      <th>max_abs_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.060889</td>\n",
       "      <td>0.364229</td>\n",
       "      <td>5.981854</td>\n",
       "      <td>189.432118</td>\n",
       "      <td>31.667794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113535</td>\n",
       "      <td>0.664686</td>\n",
       "      <td>5.854477</td>\n",
       "      <td>193.950395</td>\n",
       "      <td>33.128560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040263</td>\n",
       "      <td>0.232254</td>\n",
       "      <td>5.768443</td>\n",
       "      <td>449.245510</td>\n",
       "      <td>77.879862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>0.342030</td>\n",
       "      <td>5.742029</td>\n",
       "      <td>220.044549</td>\n",
       "      <td>38.321744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.204391</td>\n",
       "      <td>5.580816</td>\n",
       "      <td>271.994050</td>\n",
       "      <td>48.737327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071848</td>\n",
       "      <td>0.372878</td>\n",
       "      <td>5.189799</td>\n",
       "      <td>269.994717</td>\n",
       "      <td>52.024117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046445</td>\n",
       "      <td>0.230206</td>\n",
       "      <td>4.956555</td>\n",
       "      <td>135.460993</td>\n",
       "      <td>27.329667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045830</td>\n",
       "      <td>0.224259</td>\n",
       "      <td>4.893245</td>\n",
       "      <td>188.755678</td>\n",
       "      <td>38.574744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045775</td>\n",
       "      <td>0.203730</td>\n",
       "      <td>4.450675</td>\n",
       "      <td>234.797943</td>\n",
       "      <td>52.755586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.058133</td>\n",
       "      <td>0.248542</td>\n",
       "      <td>4.275406</td>\n",
       "      <td>117.243935</td>\n",
       "      <td>27.422875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>0.145036</td>\n",
       "      <td>3.943190</td>\n",
       "      <td>149.668854</td>\n",
       "      <td>37.956285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>0.127764</td>\n",
       "      <td>3.712996</td>\n",
       "      <td>266.639698</td>\n",
       "      <td>71.812550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042685</td>\n",
       "      <td>0.158031</td>\n",
       "      <td>3.702224</td>\n",
       "      <td>150.461656</td>\n",
       "      <td>40.640886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.033742</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>3.581086</td>\n",
       "      <td>132.074132</td>\n",
       "      <td>36.881032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.051731</td>\n",
       "      <td>0.154078</td>\n",
       "      <td>2.978472</td>\n",
       "      <td>184.963389</td>\n",
       "      <td>62.100095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.129231</td>\n",
       "      <td>2.773855</td>\n",
       "      <td>78.774574</td>\n",
       "      <td>28.398953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054154</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>2.254669</td>\n",
       "      <td>96.813734</td>\n",
       "      <td>42.939216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034469</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>2.189505</td>\n",
       "      <td>140.694761</td>\n",
       "      <td>64.258711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.098944</td>\n",
       "      <td>2.153211</td>\n",
       "      <td>79.866921</td>\n",
       "      <td>37.092007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042350</td>\n",
       "      <td>0.079822</td>\n",
       "      <td>1.884822</td>\n",
       "      <td>77.374153</td>\n",
       "      <td>41.051175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       M     K     N  BLOCK_M  BLOCK_N  BLOCK_K  num_warps  num_stages  \\\n",
       "26  2048  1024  1024       64      128       64          4           3   \n",
       "35  4096  1024  1024       64      128       64          4           3   \n",
       "29  4096   256  1024       64      128       64          4           3   \n",
       "34  4096  1024   512       64      128       64          4           3   \n",
       "23  2048   512  1024       64      128       64          4           3   \n",
       "32  4096   512  1024       64      128       64          4           3   \n",
       "17  1024  1024  1024       64      128       64          4           3   \n",
       "33  4096  1024   256       64      128       64          4           3   \n",
       "31  4096   512   512       64      128       64          4           3   \n",
       "25  2048  1024   512       64      128       64          4           3   \n",
       "22  2048   512   512       64      128       64          4           3   \n",
       "20  2048   256  1024       64      128       64          4           3   \n",
       "30  4096   512   256       64      128       64          4           3   \n",
       "24  2048  1024   256       64      128       64          4           3   \n",
       "28  4096   256   512       64      128       64          4           3   \n",
       "8    512  1024  1024       64      128       64          4           3   \n",
       "14  1024   512  1024       64      128       64          4           3   \n",
       "19  2048   256   512       64      128       64          4           3   \n",
       "16  1024  1024   512       64      128       64          4           3   \n",
       "21  2048   512   256       64      128       64          4           3   \n",
       "\n",
       "    t_triton_ms  t_torch_ms  speed_vs_torch  bw_triton_GBs  bw_torch_GBs  \\\n",
       "26     0.060889    0.364229        5.981854     189.432118     31.667794   \n",
       "35     0.113535    0.664686        5.854477     193.950395     33.128560   \n",
       "29     0.040263    0.232254        5.768443     449.245510     77.879862   \n",
       "34     0.059566    0.342030        5.742029     220.044549     38.321744   \n",
       "23     0.036624    0.204391        5.580816     271.994050     48.737327   \n",
       "32     0.071848    0.372878        5.189799     269.994717     52.024117   \n",
       "17     0.046445    0.230206        4.956555     135.460993     27.329667   \n",
       "33     0.045830    0.224259        4.893245     188.755678     38.574744   \n",
       "31     0.045775    0.203730        4.450675     234.797943     52.755586   \n",
       "25     0.058133    0.248542        4.275406     117.243935     27.422875   \n",
       "22     0.036781    0.145036        3.943190     149.668854     37.956285   \n",
       "20     0.034410    0.127764        3.712996     266.639698     71.812550   \n",
       "30     0.042685    0.158031        3.702224     150.461656     40.640886   \n",
       "24     0.033742    0.120833        3.581086     132.074132     36.881032   \n",
       "28     0.051731    0.154078        2.978472     184.963389     62.100095   \n",
       "8      0.046589    0.129231        2.773855      78.774574     28.398953   \n",
       "14     0.054154    0.122100        2.254669      96.813734     42.939216   \n",
       "19     0.034469    0.075471        2.189505     140.694761     64.258711   \n",
       "16     0.045952    0.098944        2.153211      79.866921     37.092007   \n",
       "21     0.042350    0.079822        1.884822      77.374153     41.051175   \n",
       "\n",
       "    max_abs_err  \n",
       "26            0  \n",
       "35            0  \n",
       "29            0  \n",
       "34            0  \n",
       "23            0  \n",
       "32            0  \n",
       "17            0  \n",
       "33            0  \n",
       "31            0  \n",
       "25            0  \n",
       "22            0  \n",
       "20            0  \n",
       "30            0  \n",
       "24            0  \n",
       "28            0  \n",
       "8             0  \n",
       "14            0  \n",
       "19            0  \n",
       "16            0  \n",
       "21            0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gemm_tiles = {\n",
    "    \"BLOCK_M\": 64,\n",
    "    \"BLOCK_N\": 128,\n",
    "    \"BLOCK_K\": 64,\n",
    "    \"num_warps\": 4,\n",
    "    \"num_stages\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df_gemm = benchmark_gemm_int8_best_tiles(\n",
    "    best_gemm_tiles,\n",
    "    Ms=(512, 1024, 2048, 4096),\n",
    "    Ks=(256, 512, 1024),\n",
    "    Ns=(256, 512, 1024),\n",
    "    iters_per_shape=50,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "df_gemm.sort_values(\"speed_vs_torch\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc783a-dfe5-4306-9f8b-b4e9eee2cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588e49e-84e6-4dd0-b9a0-73010622b059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de16cb-e4b7-4598-9f28-5bfd9c6045b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942eb61-f0a3-4b67-bd66-e82b70f809bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
