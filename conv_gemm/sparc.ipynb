{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569de371-f473-4728-b47c-f7356e67d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Sparse-by-K (structured) for Conv2d -> im2col -> GEMM\n",
    "import torch, torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try: torch.set_float32_matmul_precision(\"high\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f2a21a-e8af-4c7e-a153-d5311a3f43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype  = torch.float16 \n",
    "\n",
    "# ----- ПАРАМЕТРЫ -----\n",
    "N, C, H, W   = 4, 64, 128, 128     \n",
    "OUT, KH, KW  = 128, 3, 3          \n",
    "SH, SW       = 1, 1\n",
    "PH, PW       = 1, 1\n",
    "DH, DW       = 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b618d38-06c2-4c1a-8630-ea3e70d79ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sparsity = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2a3d93-c054-426a-943f-8f3bd1235411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (4, 64, 128, 128), Weights: (128, 64, 3, 3), Bias: (128,)\n",
      "Output: (N, OUT, H_OUT, W_OUT) = (4, 128, 128, 128)\n",
      "K = C*KH*KW = 576, L = H_OUT*W_OUT = 16384\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator(device=device).manual_seed(0)\n",
    "x = torch.randn(N, C, H, W, device=device, dtype=dtype, generator=g)\n",
    "w = torch.randn(OUT, C, KH, KW, device=device, dtype=dtype, generator=g)\n",
    "b = torch.randn(OUT, device=device, dtype=dtype, generator=g)\n",
    "\n",
    "# ----- ВЫХОДНЫЕ РАЗМЕРЫ -----\n",
    "H_OUT = (H + 2*PH - DH*(KH-1) - 1)//SH + 1\n",
    "W_OUT = (W + 2*PW - DW*(KW-1) - 1)//SW + 1\n",
    "L     = H_OUT * W_OUT\n",
    "K     = C * KH * KW\n",
    "print(f\"Input:  {tuple(x.shape)}, Weights: {tuple(w.shape)}, Bias: {tuple(b.shape)}\")\n",
    "print(f\"Output: (N, OUT, H_OUT, W_OUT) = ({N}, {OUT}, {H_OUT}, {W_OUT})\")\n",
    "print(f\"K = C*KH*KW = {K}, L = H_OUT*W_OUT = {L}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3e3391-71c5-4b7e-8b73-cb4ce1d20fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== БАЗА: cuDNN Conv2d =====================\n",
    "for _ in range(5):\n",
    "    y_ref = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "e0s, e0e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "e0s.record()\n",
    "for _ in range(20):\n",
    "    y_ref = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "e0e.record(); torch.cuda.synchronize()\n",
    "t_conv_ms = e0s.elapsed_time(e0e) / 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82dc9360-5c6c-4dea-b0ed-293e22e29b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203620/1919742281.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H_OUT': 128, 'W_OUT': 128, 'L': 16384, 'K': 576, 'OUT': 128, 'CONV2D_TOTAL_ms': 0.429752311706543, 'DENSE_FP16_TOTAL_ms': 2.4207154846191408, 'PRUNED_K_ratio': 0.5, 'PRUNED_K_TOTAL_ms': 2.400809020996094, 'PRUNED_K_keep': 288, 'INT8_EMU_TOTAL_ms': 6.909661865234375, 'INT8_EMU_max_abs_err': 2.109375, 'INT8_EMU_mse': 0.14916929602622986, 'DENSE_GEMM_GFLOP_s_est': 3992.074441379639}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203620/1919742281.py:165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _out_hw(H, W, KH, KW, SH, SW, PH, PW, DH, DW):\n",
    "    H_OUT = (H + 2*PH - DH*(KH-1) - 1)//SH + 1\n",
    "    W_OUT = (W + 2*PW - DW*(KW-1) - 1)//SW + 1\n",
    "    return H_OUT, W_OUT\n",
    "\n",
    "@torch.inference_mode()\n",
    "def benchmark_total(x, w, b,\n",
    "                    KH, KW, SH, SW, PH, PW, DH, DW,\n",
    "                    iters_total=50,\n",
    "                    prune_topk_ratio=None,   # например 0.5 оставит 50% K-столбцов\n",
    "                    int8_emulation=False,    # True — эмулировать INT8 (квантизация, dequant, GEMM FP16)\n",
    "                    seed=0):\n",
    "    assert x.is_cuda and w.is_cuda and b.is_cuda\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # ---- формы\n",
    "    N, C_in, H, W = x.shape\n",
    "    OUT = w.shape[0]\n",
    "    K = C_in * KH * KW\n",
    "    H_OUT, W_OUT = _out_hw(H, W, KH, KW, SH, SW, PH, PW, DH, DW)\n",
    "    L = H_OUT * W_OUT\n",
    "\n",
    "    # ---- baseline: цельная conv2d (TOTAL)\n",
    "    for _ in range(3):\n",
    "        _ = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "    torch.cuda.synchronize()\n",
    "    ecs, ece = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "    ecs.record()\n",
    "    for _ in range(max(10, iters_total//2)):\n",
    "        _ = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "    ece.record(); torch.cuda.synchronize()\n",
    "    t_conv2d_total_ms = ecs.elapsed_time(ece) / max(10, iters_total//2)\n",
    "\n",
    "    results = {\n",
    "        \"H_OUT\": H_OUT, \"W_OUT\": W_OUT, \"L\": L, \"K\": K, \"OUT\": OUT,\n",
    "        \"CONV2D_TOTAL_ms\": t_conv2d_total_ms\n",
    "    }\n",
    "\n",
    "    # ---- 1) DENSE FP16 TOTAL: unfold + (prep) + GEMM + bias + fold   (всё внутри одного цикла)\n",
    "    for _ in range(3):\n",
    "        X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))     # (N,K,L)\n",
    "        A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()                             # (N*L, K)\n",
    "        B2d = w.view(OUT, -1).transpose(0, 1).contiguous()                                  # (K, OUT)\n",
    "        Y2d = A2d @ B2d\n",
    "        Y2d_bias = Y2d + b.view(1, OUT)\n",
    "        Y_col = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "        _ = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    e1s, e1e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "    e1s.record()\n",
    "    for _ in range(iters_total):\n",
    "        # unfold\n",
    "        X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))     # (N,K,L)\n",
    "        # prep матриц для GEMM\n",
    "        A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()                             # (N*L, K)\n",
    "        B2d = w.view(OUT, -1).transpose(0, 1).contiguous()                                  # (K, OUT)\n",
    "        # GEMM\n",
    "        Y2d = A2d @ B2d                                                                     # (N*L, OUT)\n",
    "        # bias + fold\n",
    "        Y2d_bias = Y2d + b.view(1, OUT)\n",
    "        Y_col = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()                       # (N,OUT,L)\n",
    "        _ = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)                                    # (N,OUT,H_OUT,W_OUT)\n",
    "    e1e.record(); torch.cuda.synchronize()\n",
    "    results[\"DENSE_FP16_TOTAL_ms\"] = e1s.elapsed_time(e1e) / iters_total\n",
    "\n",
    "    # ---- 2) PRUNED-K TOTAL: считаем «спецификацию» (маску top-|K|), режем столбцы, тот же путь\n",
    "    if prune_topk_ratio is not None and 0 < prune_topk_ratio < 1.0:\n",
    "        K_keep = max(1, int(round(K * prune_topk_ratio)))\n",
    "\n",
    "        # прогрев + вычисление важности по L2 (по столбцам K)\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            W2d = w.view(OUT, -1).transpose(0, 1).contiguous()                              # (K, OUT)\n",
    "            # важность по столбцу K как L2 по OUT\n",
    "            importance = (W2d.float().pow(2).sum(dim=1))                                    # (K,)\n",
    "            keep_idx = torch.topk(importance, k=K_keep, dim=0).indices.sort()[0]            # отсортируем индексы\n",
    "\n",
    "        # прогрев полного цикла\n",
    "        for _ in range(3):\n",
    "            X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW)) # (N,K,L)\n",
    "            A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()                         # (N*L,K)\n",
    "            A2d_k = A2d.index_select(dim=1, index=keep_idx)                                 # (N*L, K_keep)\n",
    "            B2d_k = W2d.index_select(dim=0, index=keep_idx).contiguous()                    # (K_keep, OUT)\n",
    "            Y2d = A2d_k @ B2d_k\n",
    "            Y2d_bias = Y2d + b.view(1, OUT)\n",
    "            Y_col = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "            _ = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        e2s, e2e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "        e2s.record()\n",
    "        for _ in range(iters_total):\n",
    "            # unfold\n",
    "            X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW)) # (N,K,L)\n",
    "            # calc importance + topk (включаем в TOTAL по твоему требованию)\n",
    "            W2d = w.view(OUT, -1).transpose(0, 1).contiguous()                              # (K,OUT)\n",
    "            importance = (W2d.float().pow(2).sum(dim=1))\n",
    "            keep_idx = torch.topk(importance, k=K_keep, dim=0).indices.sort()[0]\n",
    "            # подготовка A,B под срез\n",
    "            A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()\n",
    "            A2d_k = A2d.index_select(dim=1, index=keep_idx)                                 # (N*L, K_keep)\n",
    "            B2d_k = W2d.index_select(dim=0, index=keep_idx).contiguous()                    # (K_keep, OUT)\n",
    "            # GEMM\n",
    "            Y2d = A2d_k @ B2d_k\n",
    "            # bias + fold\n",
    "            Y2d_bias = Y2d + b.view(1, OUT)\n",
    "            Y_col = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "            _ = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "        e2e.record(); torch.cuda.synchronize()\n",
    "\n",
    "        results[\"PRUNED_K_ratio\"] = prune_topk_ratio\n",
    "        results[\"PRUNED_K_TOTAL_ms\"] = e2s.elapsed_time(e2e) / iters_total\n",
    "        results[\"PRUNED_K_keep\"] = K_keep\n",
    "\n",
    "    # ---- 3) INT8 EMULATION TOTAL: квант A/B → dequant scale → GEMM в FP16 → bias → fold\n",
    "    # ВНИМАНИЕ: это корректность INT8, НО не скорость настоящего INT8 на CUDA.\n",
    "    if int8_emulation:\n",
    "        def quant_per_tensor(xf):\n",
    "            # симметричная PTQ: scale = max(|x|)/127 (per-tensor)\n",
    "            maxv = xf.abs().amax()\n",
    "            scale = (maxv / 127.0).clamp(min=1e-12)\n",
    "            qi = torch.clamp((xf / scale).round(), -128, 127).to(torch.int8)\n",
    "            return qi, scale\n",
    "\n",
    "        for _ in range(3):\n",
    "            # unfold\n",
    "            X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW)) # (N,K,L)\n",
    "            A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()                         # (N*L, K)\n",
    "            B2d = w.view(OUT, -1).transpose(0, 1).contiguous()                              # (K, OUT)\n",
    "            # квант\n",
    "            Ai, sA = quant_per_tensor(A2d.float())\n",
    "            Bi, sB = quant_per_tensor(B2d.float())\n",
    "            # dequant в FP16 для GEMM (эмуляция)\n",
    "            A_deq = (Ai.float() * sA).half().contiguous()\n",
    "            B_deq = (Bi.float() * sB).half().contiguous()\n",
    "            Y2d = A_deq @ B_deq\n",
    "            Y2d_bias = Y2d + b.view(1, OUT)\n",
    "            Y_col = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "            _ = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        e3s, e3e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "        e3s.record()\n",
    "        for _ in range(iters_total):\n",
    "            X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))\n",
    "            A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()\n",
    "            B2d = w.view(OUT, -1).transpose(0, 1).contiguous()\n",
    "            Ai, sA = quant_per_tensor(A2d.float())\n",
    "            Bi, sB = quant_per_tensor(B2d.float())\n",
    "            A_deq = (Ai.float() * sA).half().contiguous()\n",
    "            B_deq = (Bi.float() * sB).half().contiguous()\n",
    "            Y2d = A_deq @ B_deq\n",
    "            Y2d_bias = Y2d + b.view(1, OUT)\n",
    "            Y_col = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "            _ = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "        e3e.record(); torch.cuda.synchronize()\n",
    "\n",
    "        results[\"INT8_EMU_TOTAL_ms\"] = e3s.elapsed_time(e3e) / iters_total\n",
    "\n",
    "        # посчитаем ошибку относительно dense\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            # эталон (один прогон)\n",
    "            Xc = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))\n",
    "            A2d = Xc.transpose(1, 2).reshape(-1, K).contiguous()\n",
    "            B2d = w.view(OUT, -1).transpose(0, 1).contiguous()\n",
    "            Y_ref = A2d @ B2d\n",
    "            Y_ref = (Y_ref + b.view(1, OUT)).view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "            y_ref = F.fold(Y_ref, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "            # int8 эмуляция (один прогон)\n",
    "            Ai, sA = quant_per_tensor(A2d.float())\n",
    "            Bi, sB = quant_per_tensor(B2d.float())\n",
    "            A_deq = (Ai.float() * sA).half().contiguous()\n",
    "            B_deq = (Bi.float() * sB).half().contiguous()\n",
    "            Y_q = A_deq @ B_deq\n",
    "            Y_q = (Y_q + b.view(1, OUT)).view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "            y_q = F.fold(Y_q, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "        err = (y_q.float() - y_ref.float())\n",
    "        results[\"INT8_EMU_max_abs_err\"] = err.abs().amax().item()\n",
    "        results[\"INT8_EMU_mse\"] = (err.pow(2).mean().item())\n",
    "\n",
    "    # ---- GFLOP/s для текущего GEMM dense (информативно)\n",
    "    flops_dense = 2.0 * (N*L) * K * OUT\n",
    "    results[\"DENSE_GEMM_GFLOP_s_est\"] = (flops_dense / 1e9) / (results[\"DENSE_FP16_TOTAL_ms\"] / 1e3)\n",
    "\n",
    "    return results\n",
    "\n",
    "res = benchmark_total(x, w, b, KH,KW, SH,SW, PH,PW, DH,DW,\n",
    "                      iters_total=50,\n",
    "                      prune_topk_ratio=0.5,   # или None\n",
    "                      int8_emulation=True)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50807a64-8282-490b-9b66-ec9cc837b252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
