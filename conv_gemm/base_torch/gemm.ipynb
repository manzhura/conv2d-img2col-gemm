{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88f3c44-c542-4cea-81d5-2c62832cf19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2eef13-6d61-4bfd-baee-95ac22ce6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def cuda_timer(name=\"\"):\n",
    "    \"\"\"Контекстный менеджер для честного замера GPU-времени.\"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"{name:<20s} {dt*1e3:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7630aea1-25b5-4cf3-be94-abfee9ebeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_hw_out(h: int, w: int,\n",
    "                   kH: int, kW: int,\n",
    "                   sH: int, sW: int,\n",
    "                   pH: int, pW: int,\n",
    "                   dH: int, dW: int) -> Tuple[int, int]:\n",
    "    H_out = (h + 2*pH - dH*(kH-1) - 1)//sH + 1\n",
    "    W_out = (w + 2*pW - dW*(kW-1) - 1)//sW + 1\n",
    "    return H_out, W_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f3b74c-53f2-465f-9dc6-224f72b77632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_unfold(x: torch.Tensor,\n",
    "                  kernel_size: Tuple[int, int],\n",
    "                  stride: Tuple[int, int],\n",
    "                  padding: Tuple[int, int],\n",
    "                  dilation: Tuple[int, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [N, C, H, W]\n",
    "    return col: [N, C*kH*kW, H_out*W_out]\n",
    "    \"\"\"\n",
    "    kH, kW = kernel_size\n",
    "    sH, sW = stride\n",
    "    pH, pW = padding\n",
    "    dH, dW = dilation\n",
    "    col = F.unfold(x, kernel_size=(kH, kW), dilation=(dH, dW),\n",
    "                   padding=(pH, pW), stride=(sH, sW))\n",
    "    return col  # [N, K, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180052d7-7a7f-4dd9-a550-ceb53dd485db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemm_weight_col(weight: torch.Tensor,\n",
    "                    col: torch.Tensor,\n",
    "                    bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    weight: [C_out, C_in, kH, kW]\n",
    "    col:    [N, C_in*kH*kW, L]\n",
    "    return: [N, C_out, L]\n",
    "    \"\"\"\n",
    "    N, K, L = col.shape\n",
    "    C_out, C_in, kH, kW = weight.shape\n",
    "    assert K == C_in * kH * kW, \"K несовместим с весами\"\n",
    "    Wm = weight.view(C_out, K)                           # [C_out, K]\n",
    "    y = torch.bmm(Wm.unsqueeze(0).expand(N, -1, -1),     # [N, C_out, K]\n",
    "                  col)                                   # [N, K, L] -> [N, C_out, L]\n",
    "    if bias is not None:\n",
    "        y = y + bias.view(1, -1, 1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b09686c-2945-4258-9d99-c67d74ceffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im_fold(y: torch.Tensor,\n",
    "                out_channels: int,\n",
    "                H_out: int,\n",
    "                W_out: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    y: [N, C_out, L]\n",
    "    return: [N, C_out, H_out, W_out]\n",
    "    \"\"\"\n",
    "    N, C_out, L = y.shape\n",
    "    assert C_out == out_channels\n",
    "    assert L == H_out * W_out\n",
    "    return y.view(N, C_out, H_out, W_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095d314f-6bba-4ff9-aa63-1757baea4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_2tuple(x):\n",
    "    return (x, x) if isinstance(x, int) else tuple(x)\n",
    "class Gem2ColConv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        # --- НОРМАЛИЗАЦИЯ ТОЛЬКО ОДИН РАЗ ---\n",
    "        kH, kW = _to_2tuple(kernel_size)\n",
    "        sH, sW = _to_2tuple(stride)\n",
    "        pH, pW = _to_2tuple(padding)\n",
    "        dH, dW = _to_2tuple(dilation)\n",
    "\n",
    "        # (опционально, но полезно) привести к int\n",
    "        kH, kW = int(kH), int(kW)\n",
    "        sH, sW = int(sH), int(sW)\n",
    "        pH, pW = int(pH), int(pW)\n",
    "        dH, dW = int(dH), int(dW)\n",
    "\n",
    "        self.in_c  = in_channels\n",
    "        self.out_c = out_channels\n",
    "        self.kH, self.kW = kH, kW\n",
    "        self.sH, self.sW = sH, sW\n",
    "        self.pH, self.pW = pH, pW\n",
    "        self.dH, self.dW = dH, dW\n",
    "\n",
    "        # ВАЖНО: здесь все четыре — именно int\n",
    "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, self.kH, self.kW))\n",
    "        self.bias   = nn.Parameter(torch.empty(out_channels)) if bias else None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in = self.in_c * self.kH * self.kW\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, +bound)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _hw_out(self, h: int, w: int) -> Tuple[int, int]:\n",
    "        return compute_hw_out(h, w, self.kH, self.kW, self.sH, self.sW, self.pH, self.pW, self.dH, self.dW)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1) im2col\n",
    "        col = im2col_unfold(\n",
    "            x,\n",
    "            kernel_size=(self.kH, self.kW),\n",
    "            stride=(self.sH, self.sW),\n",
    "            padding=(self.pH, self.pW),\n",
    "            dilation=(self.dH, self.dW),\n",
    "        )  # [N, K, L]\n",
    "\n",
    "        # 2) GEMM\n",
    "        y = gemm_weight_col(self.weight, col, self.bias)  # [N, C_out, L]\n",
    "\n",
    "        # 3) col2im (просто reshape)\n",
    "        N, _, H, W = x.shape\n",
    "        H_out, W_out = self._hw_out(H, W)\n",
    "        out = col2im_fold(y, self.out_c, H_out, W_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62068b8-3ee8-464c-ab27-4381060af8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manzhura/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m _ \u001b[38;5;241m=\u001b[39m conv_ref(x)\n\u001b[1;32m     43\u001b[0m _ \u001b[38;5;241m=\u001b[39m conv_im2col(x)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# замер\u001b[39;00m\n\u001b[1;32m     47\u001b[0m measure_time(\u001b[38;5;28;01mlambda\u001b[39;00m: conv_ref(x), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.Conv2d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:1083\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msynchronize\u001b[39m(device: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \n\u001b[1;32m   1078\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:412\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    411\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 412\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    416\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "@contextmanager\n",
    "def cuda_timer():\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "    yield dt  # можно вернуть dt, но здесь будем считать вручную\n",
    "\n",
    "\n",
    "def measure_time(fn, n_iter=10000, name=\"\"):\n",
    "    torch.cuda.synchronize()\n",
    "    times = []\n",
    "    for _ in range(n_iter):\n",
    "        t0 = time.perf_counter()\n",
    "        fn()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append((time.perf_counter() - t0) * 1e3)\n",
    "    mean_t = statistics.mean(times)\n",
    "    std_t = statistics.stdev(times)\n",
    "    print(f\"{name:<20s} mean={mean_t:.3f} ms ± {std_t:.3f}\")\n",
    "    return mean_t\n",
    "\n",
    "\n",
    "# ==== тест ====\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x = torch.randn(8, 16, 32, 32, device=device, dtype=torch.float16)\n",
    "\n",
    "conv_ref = nn.Conv2d(16, 24, kernel_size=3, stride=2, padding=1, bias=True).to(device).to(torch.float16)\n",
    "conv_im2col = Gem2ColConv2d(16, 24, kernel_size=3, stride=2, padding=1, bias=True).to(device).to(torch.float16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv_im2col.weight.copy_(conv_ref.weight)\n",
    "    if conv_ref.bias is not None:\n",
    "        conv_im2col.bias.copy_(conv_ref.bias)\n",
    "\n",
    "# прогрев\n",
    "_ = conv_ref(x)\n",
    "_ = conv_im2col(x)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# замер\n",
    "measure_time(lambda: conv_ref(x), name=\"nn.Conv2d\")\n",
    "measure_time(lambda: conv_im2col(x), name=\"Gem2ColConv2d\")\n",
    "\n",
    "# сравнение результата\n",
    "y0 = conv_ref(x)\n",
    "y1 = conv_im2col(x)\n",
    "print(\"Shapes:\", y0.shape, y1.shape)\n",
    "print(\"mean|diff|:\", (y0 - y1).abs().mean().item())\n",
    "print(\"max|diff| :\", (y0 - y1).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9436768-d442-4b28-b9fb-cadb0690104e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
