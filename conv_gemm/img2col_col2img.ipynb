{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a636a509-5582-4917-b37e-1e7ce346f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859b1047-5ce2-4a5b-aec3-93d85d721753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _out_hw(H, W, KH, KW, SH, SW, PH, PW, DH, DW):\n",
    "    H_OUT = (H + 2*PH - DH*(KH-1) - 1)//SH + 1\n",
    "    W_OUT = (W + 2*PW - DW*(KW-1) - 1)//SW + 1\n",
    "    return H_OUT, W_OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4c331-3aaf-4250-8f6a-496be7468443",
   "metadata": {},
   "source": [
    "# col2img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a14c01-1fe5-40e0-baf8-38233d5062fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F.conv2d    : 0.087 ms | ~1,635.0 GFLOP/s\n",
      "im2col+GEMM : 0.537 ms | ~263.8 GFLOP/s\n",
      "Speedup (i2c vs conv2d): 0.161×\n",
      "Max |diff|: 1.562e-02 (dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.float16  # можно torch.float32 с TF32\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# размеры\n",
    "N, C, H, W   = 2, 3, 512, 640\n",
    "OUT, KH, KW  = 16, 3, 3\n",
    "SH, SW       = 2, 2\n",
    "PH, PW       = 1, 1\n",
    "DH, DW       = 1, 1\n",
    "\n",
    "x = torch.randn(N, C, H, W, device=device, dtype=dtype)\n",
    "w = torch.randn(OUT, C, KH, KW, device=device, dtype=dtype)\n",
    "b = torch.randn(OUT, device=device, dtype=dtype)\n",
    "\n",
    "H_OUT = (H + 2*PH - DH*(KH-1) - 1)//SH + 1\n",
    "W_OUT = (W + 2*PW - DW*(KW-1) - 1)//SW + 1\n",
    "L     = H_OUT * W_OUT\n",
    "K     = C * KH * KW\n",
    "\n",
    "# --- эталон conv2d ---\n",
    "for _ in range(5):\n",
    "    y_ref = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "torch.cuda.synchronize()\n",
    "s0,e0 = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "s0.record()\n",
    "for _ in range(20):\n",
    "    y_ref = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "e0.record(); torch.cuda.synchronize()\n",
    "t_ref_ms = s0.elapsed_time(e0)/20.0\n",
    "\n",
    "# --- im2col + ОДИН большой GEMM + fold ---\n",
    "for _ in range(5):\n",
    "    X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))  # [N, K, L]\n",
    "    # A2d: (N*L, K)\n",
    "    A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()\n",
    "    # B2d: (K, OUT)\n",
    "    B2d = w.view(OUT, -1).transpose(0, 1).contiguous()\n",
    "    Y2d = A2d @ B2d                                       # (N*L, OUT) — ОДИН GEMM!\n",
    "    Y2d = Y2d + b.view(1, OUT)\n",
    "    Y_col = Y2d.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "    y_i2c = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "s1,e1 = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "s1.record()\n",
    "for _ in range(20):\n",
    "    X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))  # [N,K,L]\n",
    "    A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()    # (N*L, K)\n",
    "    B2d = w.view(OUT, -1).transpose(0, 1).contiguous()         # (K, OUT)\n",
    "    Y2d = A2d @ B2d\n",
    "    Y2d = Y2d + b.view(1, OUT)\n",
    "    Y_col = Y2d.view(N, L, OUT).transpose(1, 2).contiguous()   # [N, OUT, L]\n",
    "    y_i2c = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "e1.record(); torch.cuda.synchronize()\n",
    "t_i2c_ms = s1.elapsed_time(e1)/20.0\n",
    "\n",
    "# проверка и метрики\n",
    "torch.testing.assert_close(y_i2c, y_ref, rtol=1e-2, atol=1e-2)\n",
    "max_abs = (y_i2c - y_ref).abs().max().item()\n",
    "flops = 2.0 * N * OUT * H_OUT * W_OUT * K\n",
    "gflops_ref = flops/(t_ref_ms*1e6)\n",
    "gflops_i2c = flops/(t_i2c_ms*1e6)\n",
    "\n",
    "print(f\"F.conv2d    : {t_ref_ms:.3f} ms | ~{gflops_ref:,.1f} GFLOP/s\")\n",
    "print(f\"im2col+GEMM : {t_i2c_ms:.3f} ms | ~{gflops_i2c:,.1f} GFLOP/s\")\n",
    "print(f\"Speedup (i2c vs conv2d): {t_ref_ms/t_i2c_ms:.3f}×\")\n",
    "print(f\"Max |diff|: {max_abs:.3e} ({dtype=})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a02e-de6d-456f-aea3-8f4771ebef21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938288e-4f16-4169-820e-b46d025dd978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22bec0b1-6fc8-439a-8157-a88c288cc827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (4, 64, 128, 128), Weights: (128, 64, 3, 3), Bias: (128,)\n",
      "Output: (N, OUT, H_OUT, W_OUT) = (4, 128, 128, 128)\n",
      "K = C*KH*KW = 576, L = H_OUT*W_OUT = 16384\n",
      "✅ Dense im2col+GEMM совпал с F.conv2d (fp16, rtol=1e-2, atol=1e-2).\n",
      "\n",
      "Structured K-pruning: target_sparsity=0.50 -> keep 288/576 (−288)\n",
      "\n",
      "=== Timing (ms) ===\n",
      "F.conv2d (cuDNN):         0.478 ms\n",
      "unfold (im2col):          0.379 ms\n",
      "GEMM dense (N*L,K)x(K,O): 0.410 ms  | ~23,572.9 GFLOP/s\n",
      "GEMM pruned (K_keep):     0.134 ms  | ~35,932.0 GFLOP/s\n",
      "Speedup GEMM (pruned vs dense): 3.05×\n",
      "\n",
      "=== Quality vs F.conv2d ===\n",
      "Max |diff| after K-prune:  9.162e+01 (dtype=torch.float16)\n",
      "MSE after K-prune:         2.580e+02\n"
     ]
    }
   ],
   "source": [
    "# %% Sparse-by-K (structured) for Conv2d -> im2col -> GEMM\n",
    "import torch, torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try: torch.set_float32_matmul_precision(\"high\")\n",
    "except: pass\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.float16  # оставь fp16 для скорости (можно fp32)\n",
    "\n",
    "# ----- ПАРАМЕТРЫ -----\n",
    "N, C, H, W   = 4, 64, 128, 128     # batch, in_channels, H, W\n",
    "OUT, KH, KW  = 128, 3, 3           # out_channels, kernel\n",
    "SH, SW       = 1, 1\n",
    "PH, PW       = 1, 1\n",
    "DH, DW       = 1, 1\n",
    "\n",
    "# целевая структурная разреженность по K (0.0..0.9). Пример: 0.5 = выкинем 50% K-строк\n",
    "target_sparsity = 0.5\n",
    "\n",
    "# ----- ДАННЫЕ -----\n",
    "g = torch.Generator(device=device).manual_seed(0)\n",
    "x = torch.randn(N, C, H, W, device=device, dtype=dtype, generator=g)\n",
    "w = torch.randn(OUT, C, KH, KW, device=device, dtype=dtype, generator=g)\n",
    "b = torch.randn(OUT, device=device, dtype=dtype, generator=g)\n",
    "\n",
    "# ----- ВЫХОДНЫЕ РАЗМЕРЫ -----\n",
    "H_OUT = (H + 2*PH - DH*(KH-1) - 1)//SH + 1\n",
    "W_OUT = (W + 2*PW - DW*(KW-1) - 1)//SW + 1\n",
    "L     = H_OUT * W_OUT\n",
    "K     = C * KH * KW\n",
    "\n",
    "print(f\"Input:  {tuple(x.shape)}, Weights: {tuple(w.shape)}, Bias: {tuple(b.shape)}\")\n",
    "print(f\"Output: (N, OUT, H_OUT, W_OUT) = ({N}, {OUT}, {H_OUT}, {W_OUT})\")\n",
    "print(f\"K = C*KH*KW = {K}, L = H_OUT*W_OUT = {L}\")\n",
    "\n",
    "# ===================== БАЗА: cuDNN Conv2d =====================\n",
    "for _ in range(5):\n",
    "    y_ref = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "e0s, e0e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "e0s.record()\n",
    "for _ in range(20):\n",
    "    y_ref = F.conv2d(x, w, b, stride=(SH,SW), padding=(PH,PW), dilation=(DH,DW))\n",
    "e0e.record(); torch.cuda.synchronize()\n",
    "t_conv_ms = e0s.elapsed_time(e0e) / 20.0\n",
    "\n",
    "# ===================== БАЗА: im2col + 1 GEMM =====================\n",
    "# 1) im2col\n",
    "for _ in range(3):\n",
    "    X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))  # [N, K, L]\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "e1s, e1e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "e1s.record()\n",
    "for _ in range(20):\n",
    "    X_col = F.unfold(x, (KH,KW), dilation=(DH,DW), padding=(PH,PW), stride=(SH,SW))\n",
    "e1e.record(); torch.cuda.synchronize()\n",
    "t_unfold_ms = e1s.elapsed_time(e1e) / 20.0\n",
    "\n",
    "# 2) A = (N*L, K)\n",
    "A2d = X_col.transpose(1, 2).reshape(-1, K).contiguous()\n",
    "# 3) B = (K, OUT)\n",
    "B2d = w.view(OUT, -1).transpose(0, 1).contiguous()\n",
    "\n",
    "# прогрев GEMM\n",
    "for _ in range(3):\n",
    "    Y2d = A2d @ B2d\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "e2s, e2e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "e2s.record()\n",
    "for _ in range(50):\n",
    "    Y2d = A2d @ B2d\n",
    "e2e.record(); torch.cuda.synchronize()\n",
    "t_gemm_dense_ms = e2s.elapsed_time(e2e) / 50.0\n",
    "\n",
    "# 4) bias + fold\n",
    "Y2d_bias = Y2d + b.view(1, OUT)\n",
    "Y_col    = Y2d_bias.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "y_i2c    = F.fold(Y_col, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "# сверка корректности с conv2d (без спарсификации — должны совпасть плотно)\n",
    "try:\n",
    "    torch.testing.assert_close(y_i2c, y_ref, rtol=1e-2, atol=1e-2)\n",
    "    print(\"✅ Dense im2col+GEMM совпал с F.conv2d (fp16, rtol=1e-2, atol=1e-2).\")\n",
    "except AssertionError as e:\n",
    "    print(\"⚠️ Dense путь отличается от F.conv2d больше допуска:\", str(e).splitlines()[0])\n",
    "\n",
    "# ===================== СПАРСИФИКАЦИЯ ПО K (СТРУКТУРНАЯ) =====================\n",
    "# Идея: B2d имеет форму (K, OUT). Обнулим \"неважные\" строки (общий вклад по OUT мал),\n",
    "# затем УДАЛИМ эти строки из B2d и соответствующие столбцы из A2d → получим меньший GEMM: (N*L, K_keep) @ (K_keep, OUT)\n",
    "\n",
    "# 1) оценка важности строк B2d: возьмем L2-норму по оси OUT\n",
    "row_importance = B2d.float().pow(2).sum(dim=1)  # [K], считаем в fp32 ради стабильности\n",
    "# 2) отберём верхние (1 - sparsity) долю строк\n",
    "k_keep = max(1, int((1.0 - target_sparsity) * K))\n",
    "keep_idx = torch.topk(row_importance, k_keep, largest=True, sorted=False).indices\n",
    "keep_idx, _ = torch.sort(keep_idx)  # стабильный порядок\n",
    "k_pruned = K - k_keep\n",
    "\n",
    "print(f\"\\nStructured K-pruning: target_sparsity={target_sparsity:.2f} -> keep {k_keep}/{K} (−{k_pruned})\")\n",
    "\n",
    "# 3) сжимаем A и B\n",
    "A2d_sp = A2d.index_select(1, keep_idx)      # (N*L, K_keep)\n",
    "B2d_sp = B2d.index_select(0, keep_idx)      # (K_keep, OUT)\n",
    "\n",
    "# 4) GEMM на суженных матрицах\n",
    "for _ in range(3):\n",
    "    Y2d_sp = A2d_sp @ B2d_sp\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "e3s, e3e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "e3s.record()\n",
    "for _ in range(50):\n",
    "    Y2d_sp = A2d_sp @ B2d_sp\n",
    "e3e.record(); torch.cuda.synchronize()\n",
    "t_gemm_sparse_ms = e3s.elapsed_time(e3e) / 50.0\n",
    "\n",
    "# 5) bias + сборка обратно\n",
    "Y2d_sp = Y2d_sp + b.view(1, OUT)\n",
    "Y_col_sp = Y2d_sp.view(N, L, OUT).transpose(1, 2).contiguous()\n",
    "y_i2c_sp = F.fold(Y_col_sp, (H_OUT, W_OUT), kernel_size=1)\n",
    "\n",
    "# ОЦЕНКА РАЗЛИЧИЙ (после прунинга ожидается отличаться!)\n",
    "diff_max = (y_i2c_sp - y_ref).abs().max().item()\n",
    "mse      = torch.mean((y_i2c_sp - y_ref)**2).item()\n",
    "\n",
    "# ===================== СВОДКА ВРЕМЕНИ И GFLOP/s =====================\n",
    "# Теоретические FLOPs одного GEMM: 2 * (N*L) * K_eff * OUT\n",
    "def gflops(nl, kk, oo, ms):\n",
    "    return (2.0 * nl * kk * oo) / (ms * 1e6)\n",
    "\n",
    "NL = N * L\n",
    "gflops_dense = gflops(NL, K,      OUT, t_gemm_dense_ms)\n",
    "gflops_sp    = gflops(NL, k_keep, OUT, t_gemm_sparse_ms)\n",
    "\n",
    "print(\"\\n=== Timing (ms) ===\")\n",
    "print(f\"F.conv2d (cuDNN):         {t_conv_ms:.3f} ms\")\n",
    "print(f\"unfold (im2col):          {t_unfold_ms:.3f} ms\")\n",
    "print(f\"GEMM dense (N*L,K)x(K,O): {t_gemm_dense_ms:.3f} ms  | ~{gflops_dense:,.1f} GFLOP/s\")\n",
    "print(f\"GEMM pruned (K_keep):     {t_gemm_sparse_ms:.3f} ms  | ~{gflops_sp:,.1f} GFLOP/s\")\n",
    "print(f\"Speedup GEMM (pruned vs dense): {t_gemm_dense_ms / t_gemm_sparse_ms:.2f}×\")\n",
    "\n",
    "print(\"\\n=== Quality vs F.conv2d ===\")\n",
    "print(f\"Max |diff| after K-prune:  {diff_max:.3e} (dtype={dtype})\")\n",
    "print(f\"MSE after K-prune:         {mse:.3e}\")\n",
    "\n",
    "# Пояснение:\n",
    "# - ускорение приходит за счёт уменьшения K (структурный prune), поэтому GEMM становится меньше.\n",
    "# - это *реальный* ускор (без спец. sparse-ядра), работает на стандартном torch.matmul.\n",
    "# - качество будет зависеть от заданной target_sparsity. Подбери порог/маску из обучения (L1/L2/OBS и т.д.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "123fd6ea-40cb-42d2-88d9-a4645c775950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implicit_conv_triton.py\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "\n",
    "# ---------- Triton kernel ----------\n",
    "# Простая версия: один блок обрабатывает несколько выходных точек по пространству и по выходным каналам.\n",
    "# Ограничение: kernel заточен под kernel_size=3, stride=1, padding=1 (можно расширить).\n",
    "@triton.jit\n",
    "def conv3x3_fused_kernel(\n",
    "    # pointers\n",
    "    x_ptr, w_ptr, b_ptr, y_ptr,\n",
    "    # sizes\n",
    "    N, C, H, W, OUT, H_OUT, W_OUT,\n",
    "    # strides (in elements, не байты)\n",
    "    stride_xn, stride_xc, stride_xh, stride_xw,\n",
    "    stride_wn, stride_wc, stride_wh, stride_ww,\n",
    "    stride_yn, stride_yc, stride_yh, stride_yw,\n",
    "    # tile sizes\n",
    "    OUT_BLOCK, OUT_PIXELS, C_block\n",
    "):\n",
    "    pid = tl.program_id(0)   # индекс блока по output (блок покрывает OUT_BLOCK выходных каналов и OUT_PIXELS позиций)\n",
    "    # block mapping: каждое pid обрабатывает (out_c_base, pixel_base)\n",
    "    out_c_base = (pid // ((H_OUT*W_OUT + OUT_PIXELS - 1) // OUT_PIXELS)) * OUT_BLOCK\n",
    "    pix_block_id = pid % ((H_OUT*W_OUT + OUT_PIXELS - 1) // OUT_PIXELS)\n",
    "    pix_base = pix_block_id * OUT_PIXELS\n",
    "\n",
    "    # массив выходных каналов и позиций в блоке\n",
    "    out_cs = out_c_base + tl.arange(0, OUT_BLOCK)\n",
    "    pixs = pix_base + tl.arange(0, OUT_PIXELS)\n",
    "\n",
    "    # маски\n",
    "    mask_outc = out_cs < OUT\n",
    "    mask_pix = pixs < (H_OUT*W_OUT)\n",
    "\n",
    "    # координаты пикселей (h,w) из индекса pix (0..L-1)\n",
    "    h_idx = (pixs // W_OUT)\n",
    "    w_idx = (pixs % W_OUT)\n",
    "\n",
    "    # адреса для output: y[n, out_c, h, w]\n",
    "    # Мы не параллелим по N здесь - можно добавить внешний цикл по n\n",
    "    n = 0  # prototype for batch=1 only; расширяй по необходимости\n",
    "\n",
    "    # инициализация аккумуляторов\n",
    "    acc = tl.zeros((OUT_BLOCK, OUT_PIXELS), dtype=tl.float32)\n",
    "\n",
    "    # проходим по input channels блоками C_block\n",
    "    for c_off in range(0, C, C_block):\n",
    "        # загрузим C_block входных каналов для всех пикселей\n",
    "        cc = c_off + tl.arange(0, C_block)\n",
    "        mask_c = cc < C\n",
    "\n",
    "        # для каждого kk в C_block и для каждой позиции читаем 3x3 патч\n",
    "        # читаем элементы с учётом padding=1, stride=1, dilation=1\n",
    "        for kh in range(-1, 2):\n",
    "            for kw in range(-1, 2):\n",
    "                # входные координаты\n",
    "                ih = h_idx + kh\n",
    "                iw = w_idx + kw\n",
    "\n",
    "                # mask по границам\n",
    "                valid_hw = (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)\n",
    "\n",
    "                # строим адреса x[n, cc, ih, iw] (векторизованные)\n",
    "                # адрес = base + n*stride_xn + cc*stride_xc + ih*stride_xh + iw*stride_xw\n",
    "                x_off = n * stride_xn + cc[:, None] * stride_xc + ih[None, :] * stride_xh + iw[None, :] * stride_xw\n",
    "                # приводим к указателю\n",
    "                x_vals = tl.load(x_ptr + x_off, mask=(mask_c[:, None] & valid_hw[None, :]), other=0.0).to(tl.float32)  # shape (C_block, OUT_PIXELS)\n",
    "\n",
    "                # веса: w[out_c, cc, kh+1, kw+1]\n",
    "                kc = cc[:, None]  # shape (C_block, 1)\n",
    "                oc = out_cs[:, None]  # shape (OUT_BLOCK, 1)\n",
    "                w_off = oc[:, None, None] * stride_wn + kc[None, :, None] * stride_wc + (kh+1) * stride_wh + (kw+1) * stride_ww\n",
    "                w_vals = tl.load(w_ptr + w_off, mask=(mask_outc[:, None, None] & mask_c[None, :, None]), other=0.0).to(tl.float32)  # (OUT_BLOCK, C_block, 1)\n",
    "\n",
    "                # умножение и накопление: acc[out_c, pix] += sum_c ( w[out_c,c] * x[c,pix] )\n",
    "                # преобразуем: w_vals (OUT_BLOCK, C_block) , x_vals (C_block, OUT_PIXELS)\n",
    "                # Сделаем мат-муль частично\n",
    "                # Приведём w_vals к (OUT_BLOCK, C_block) и умножим на x_vals (C_block, OUT_PIXELS)\n",
    "                # Сделаем явный вклад:\n",
    "                # Note: это не оптимально, но корректно для прототипа\n",
    "                for oc_i in range(OUT_BLOCK):\n",
    "                    w_row = w_vals[oc_i, :, 0]  # (C_block,)\n",
    "                    # умножение по C_block: (C_block,) @ (C_block, OUT_PIXELS) -> (OUT_PIXELS,)\n",
    "                    dot = tl.dot(w_row, x_vals)\n",
    "                    acc = tl.where(mask_outc[oc_i], acc, acc)  # no-op to keep shape; accumulation below\n",
    "                    # аккумулируем в соответствующую строку\n",
    "                    acc = acc + tl.where(mask_outc[oc_i, None], tl.reshape(dot, (1, OUT_PIXELS)).to(tl.float32), tl.zeros((OUT_BLOCK, OUT_PIXELS), dtype=tl.float32))\n",
    "\n",
    "    # После накопления добавляем bias и записываем\n",
    "    # bias: b[out_c]\n",
    "    b_vals = tl.load(b_ptr + out_cs, mask=mask_outc, other=0.0).to(tl.float32)  # (OUT_BLOCK,)\n",
    "    # добавим bias и записываем y[n, out_c, h, w]\n",
    "    for i_oc in range(OUT_BLOCK):\n",
    "        if not mask_outc[i_oc]:\n",
    "            continue\n",
    "        y_vals = acc[i_oc] + b_vals[i_oc]\n",
    "        # адреса для записи y\n",
    "        y_off = n * stride_yn + out_cs[i_oc] * stride_yc + h_idx * stride_yh + w_idx * stride_yw\n",
    "        tl.store(y_ptr + y_off, y_vals.to(tl.float32), mask=mask_pix)\n",
    "\n",
    "# ---------- Python harness ----------\n",
    "def run_triton_conv3x3(x, w, b):\n",
    "    # x: (N,C,H,W), w: (OUT,C,3,3), b:(OUT,)\n",
    "    assert x.is_cuda and w.is_cuda and b.is_cuda\n",
    "    N, C, H, W = x.shape\n",
    "    OUT, Cw, KH, KW = w.shape\n",
    "    assert KH == 3 and KW == 3\n",
    "    H_OUT = H\n",
    "    W_OUT = W\n",
    "\n",
    "    # strides (in elements)\n",
    "    # Compute contiguous strides\n",
    "    stride_xn = x.stride(0)\n",
    "    stride_xc = x.stride(1)\n",
    "    stride_xh = x.stride(2)\n",
    "    stride_xw = x.stride(3)\n",
    "    stride_wn = w.stride(0)\n",
    "    stride_wc = w.stride(1)\n",
    "    stride_wh = w.stride(2)\n",
    "    stride_ww = w.stride(3)\n",
    "    y = torch.zeros((N, OUT, H_OUT, W_OUT), device=x.device, dtype=x.dtype)\n",
    "    stride_yn = y.stride(0)\n",
    "    stride_yc = y.stride(1)\n",
    "    stride_yh = y.stride(2)\n",
    "    stride_yw = y.stride(3)\n",
    "\n",
    "    # параметры блокировки (настрой)\n",
    "    OUT_BLOCK = 8      # сколько выходных каналов в блоке\n",
    "    OUT_PIXELS = 16    # сколько пространственных позиций в блоке\n",
    "    C_block = 16       # блок по входным каналам\n",
    "\n",
    "    # grid size: количество блоков = ceil(OUT/OUT_BLOCK) * ceil(L/OUT_PIXELS)\n",
    "    grid_x = (math.ceil(OUT / OUT_BLOCK) * math.ceil((H_OUT * W_OUT) / OUT_PIXELS),)\n",
    "\n",
    "    # вызов kernel (для простоты: batch size = 1 поддержан в ядре; для N>1 - можно обойти циклом по n)\n",
    "    conv3x3_fused_kernel[grid_x](\n",
    "        x, w, b, y,\n",
    "        N, C, H, W, OUT, H_OUT, W_OUT,\n",
    "        stride_xn, stride_xc, stride_xh, stride_xw,\n",
    "        stride_wn, stride_wc, stride_wh, stride_ww,\n",
    "        stride_yn, stride_yc, stride_yh, stride_yw,\n",
    "        OUT_BLOCK, OUT_PIXELS, C_block\n",
    "    )\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f497723-23ba-46ef-a4cc-47775928d67c",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 20:26:\n    stride_yn, stride_yc, stride_yh, stride_yw,\n    # tile sizes\n    OUT_BLOCK, OUT_PIXELS, C_block\n):\n    pid = tl.program_id(0)   # индекс блока по output (блок покрывает OUT_BLOCK выходных каналов и OUT_PIXELS позиций)\n    # block mapping: каждое pid обрабатывает (out_c_base, pixel_base)\n    out_c_base = (pid // ((H_OUT*W_OUT + OUT_PIXELS - 1) // OUT_PIXELS)) * OUT_BLOCK\n    pix_block_id = pid % ((H_OUT*W_OUT + OUT_PIXELS - 1) // OUT_PIXELS)\n    pix_base = pix_block_id * OUT_PIXELS\n\n    # массив выходных каналов и позиций в блоке\n    out_cs = out_c_base + tl.arange(0, OUT_BLOCK)\n                          ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/language/core.py:42\u001b[0m, in \u001b[0;36mbuiltin.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you forget to add @triton.jit ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`_semantic` argument must be provided outside of JIT functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/language/core.py:1654\u001b[0m, in \u001b[0;36marange\u001b[0;34m(start, end, _semantic)\u001b[0m\n\u001b[1;32m   1653\u001b[0m end \u001b[38;5;241m=\u001b[39m _unwrap_if_constexpr(end)\n\u001b[0;32m-> 1654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_semantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/language/semantic.py:574\u001b[0m, in \u001b[0;36mTritonSemantic.arange\u001b[0;34m(self, start, end, ret_ty)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(end, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marange\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms arguments must be of type tl.constexpr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    575\u001b[0m is_start_int64 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(start \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: arange's arguments must be of type tl.constexpr",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m w_small \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mcontiguous()               \u001b[38;5;66;03m# (OUT,C,3,3)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m b_small \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m----> 7\u001b[0m y_tr \u001b[38;5;241m=\u001b[39m \u001b[43mrun_triton_conv3x3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_small\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# сравним с F.conv2d\u001b[39;00m\n\u001b[1;32m      9\u001b[0m y_ref \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(x_small, w_small, b_small, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 138\u001b[0m, in \u001b[0;36mrun_triton_conv3x3\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m    135\u001b[0m grid_x \u001b[38;5;241m=\u001b[39m (math\u001b[38;5;241m.\u001b[39mceil(OUT \u001b[38;5;241m/\u001b[39m OUT_BLOCK) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mceil((H_OUT \u001b[38;5;241m*\u001b[39m W_OUT) \u001b[38;5;241m/\u001b[39m OUT_PIXELS),)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# вызов kernel (для простоты: batch size = 1 поддержан в ядре; для N>1 - можно обойти циклом по n)\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[43mconv3x3_fused_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid_x\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_OUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_OUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_xn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_xc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_xh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_xw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_wn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_wc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_wh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_ww\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_yn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_yc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_yh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_yw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOUT_BLOCK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_PIXELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_block\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/runtime/jit.py:390\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/runtime/jit.py:594\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    593\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constexprs, attrs)\n\u001b[0;32m--> 594\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m kernel_cache[key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_hook(knobs\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mjit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n\u001b[1;32m    597\u001b[0m                 warmup)\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/compiler/compiler.py:339\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m module_map \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_module_map()\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    341\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/compiler/compiler.py:83\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, module_map, context)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, module_map, context):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ast_to_ttir\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 20:26:\n    stride_yn, stride_yc, stride_yh, stride_yw,\n    # tile sizes\n    OUT_BLOCK, OUT_PIXELS, C_block\n):\n    pid = tl.program_id(0)   # индекс блока по output (блок покрывает OUT_BLOCK выходных каналов и OUT_PIXELS позиций)\n    # block mapping: каждое pid обрабатывает (out_c_base, pixel_base)\n    out_c_base = (pid // ((H_OUT*W_OUT + OUT_PIXELS - 1) // OUT_PIXELS)) * OUT_BLOCK\n    pix_block_id = pid % ((H_OUT*W_OUT + OUT_PIXELS - 1) // OUT_PIXELS)\n    pix_base = pix_block_id * OUT_PIXELS\n\n    # массив выходных каналов и позиций в блоке\n    out_cs = out_c_base + tl.arange(0, OUT_BLOCK)\n                          ^"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# подготовим данные (пример для N=1)\n",
    "x_small = x[:1].contiguous()          # (1,C,H,W)\n",
    "w_small = w.contiguous()               # (OUT,C,3,3)\n",
    "b_small = b.contiguous()\n",
    "y_tr = run_triton_conv3x3(x_small, w_small, b_small)\n",
    "# сравним с F.conv2d\n",
    "y_ref = F.conv2d(x_small, w_small, b_small, padding=1)\n",
    "torch.testing.assert_close(y_tr, y_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8a463-b5f1-4a5f-b6cc-60900e54c600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
