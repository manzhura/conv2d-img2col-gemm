{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a636a509-5582-4917-b37e-1e7ce346f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859b1047-5ce2-4a5b-aec3-93d85d721753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _out_hw(H, W, KH, KW, SH, SW, PH, PW, DH, DW):\n",
    "    H_OUT = (H + 2*PH - DH*(KH-1) - 1)//SH + 1\n",
    "    W_OUT = (W + 2*PW - DW*(KW-1) - 1)//SW + 1\n",
    "    return H_OUT, W_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a14c01-1fe5-40e0-baf8-38233d5062fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def _im2col_kernel_2d(\n",
    "    x_ptr, cols_ptr,\n",
    "    N, C, H, W,\n",
    "    KH, KW, SH, SW, PH, PW, DH, DW,\n",
    "    H_OUT, W_OUT, K_TOTAL,\n",
    "    sxn, sxc, sxh, sxw,\n",
    "    scn, sck, scl,\n",
    "    BLOCK_X: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    L = H_OUT * W_OUT\n",
    "\n",
    "    # индекс батча    \n",
    "    n = pid // L\n",
    "    # индекс окна/пикселя  \n",
    "    l = pid %  L\n",
    "\n",
    "    # oh/ow окна и его базовые координаты во входе\n",
    "    oh = l // W_OUT\n",
    "    ow = l %  W_OUT\n",
    "    ih0 = oh * SH - PH\n",
    "    iw0 = ow * SW - PW\n",
    "\n",
    "    # Общие константы по этому pid\n",
    "    base_xn = n * sxn\n",
    "    base_cols_nl = n * scn + l * scl\n",
    "\n",
    "    # Основной цикл по K с шагом BLOCK_X\n",
    "    k0 = 0\n",
    "    while k0 < K_TOTAL:\n",
    "        k = k0 + tl.arange(0, BLOCK_X)                      # [BLOCK_X]\n",
    "        k_mask = k < K_TOTAL\n",
    "\n",
    "        # Разложение k -> (c, kh, kw)\n",
    "        r  = k %  (KH * KW)\n",
    "        c  = k // (KH * KW)\n",
    "        kh = r // KW\n",
    "        kw = r %  KW\n",
    "\n",
    "        # Абсолютные координаты пикселя окна во входе\n",
    "        ih = ih0 + kh * DH\n",
    "        iw = iw0 + kw * DW\n",
    "\n",
    "        # Маска \"внутри\" входного тензора\n",
    "        inb = (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W) & k_mask\n",
    "\n",
    "        # Адреса: x[n, c, ih, iw]\n",
    "        x_off = base_xn + c * sxc + ih * sxh + iw * sxw\n",
    "\n",
    "        # Маскированная загрузка (без tl.where — он лишний)\n",
    "        vals = tl.load(x_ptr + x_off, mask=inb, other=0)\n",
    "\n",
    "        # Адреса для cols[n, k, l]\n",
    "        cols_off = base_cols_nl + k * sck\n",
    "\n",
    "        # Запись с маской\n",
    "        tl.store(cols_ptr + cols_off, vals, mask=k_mask)\n",
    "\n",
    "        k0 += BLOCK_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba78fc6-c9f3-4d79-af14-f25b060a82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_triton(x, kernel_size, stride=1, padding=0, dilation=1, block_k=256):\n",
    "    assert x.is_cuda\n",
    "    if isinstance(kernel_size, int): kernel_size = (kernel_size, kernel_size)\n",
    "    if isinstance(stride, int): stride = (stride, stride)\n",
    "    if isinstance(padding, int): padding = (padding, padding)\n",
    "    if isinstance(dilation, int): dilation = (dilation, dilation)\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    KH, KW = kernel_size\n",
    "    SH, SW = stride\n",
    "    PH, PW = padding\n",
    "    DH, DW = dilation\n",
    "    # Выходные  H / W  размеры тензора\n",
    "    H_OUT, W_OUT = _out_hw(H, W, KH, KW, SH, SW, PH, PW, DH, DW)\n",
    "    # длина выхожного массива\n",
    "    L = H_OUT * W_OUT\n",
    "    #  длинны кернела \n",
    "    K_TOTAL = C * KH * KW\n",
    "    # пустышка выходного тензора\n",
    "    cols = torch.empty((N, K_TOTAL, L), device=x.device, dtype=torch.float32)\n",
    "    # страйд по входному массиву\n",
    "    sxn, sxc, sxh, sxw = x.stride()\n",
    "    # страйд по выходному массиву\n",
    "    scn, sck, scl = cols.stride()\n",
    "    # одномерныя сетка\n",
    "    grid = (N * L,)\n",
    "    _im2col_kernel_2d[grid](\n",
    "        x, cols,\n",
    "        N, C, H, W,\n",
    "        KH, KW, SH, SW, PH, PW, DH, DW,\n",
    "        H_OUT, W_OUT, K_TOTAL,\n",
    "        sxn, sxc, sxh, sxw,\n",
    "        scn, sck, scl,\n",
    "        BLOCK_X=block_k,\n",
    "        num_warps=4, num_stages=2,\n",
    "    )\n",
    "    return cols, (H_OUT, W_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54be6997-659c-4c0f-8388-d01941366411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# батч / каналы/ ширина аarray / высота аarray\n",
    "N, C, H, W = 1, 3, 2000, 2000\n",
    "# H / W свёртки\n",
    "KH, KW = 3, 3\n",
    "# страйд свертки\n",
    "SH, SW = 1, 1\n",
    "#  паддинг свертки\n",
    "PH, PW = 1, 1\n",
    "# дилатация свертки\n",
    "DH, DW = 1, 1\n",
    "#  размер блока НПУ\n",
    "block_k = 256\n",
    "\n",
    "# входной тензор N, C, H, W\n",
    "x = torch.arange(N*C*H*W, dtype=torch.float32, device='cuda').reshape(N, C, H, W)\n",
    "\n",
    "# базовый unfold\n",
    "cols_ref = F.unfold(\n",
    "    x, kernel_size=(KH, KW),\n",
    "    dilation=(DH, DW),\n",
    "    padding=(PH, PW),\n",
    "    stride=(SH, SW)\n",
    ")\n",
    "\n",
    "# im2col_triton\n",
    "cols_tr, (H_out, W_out) = im2col_triton(\n",
    "    x,\n",
    "    kernel_size=(KH, KW),\n",
    "    stride=(SH, SW),\n",
    "    padding=(PH, PW),\n",
    "    dilation=(DH, DW),\n",
    "    block_k=block_k\n",
    ")\n",
    "\n",
    "# # проверим совпадение\n",
    "# print(\"cols_ref shape:\", cols_ref.shape)\n",
    "# print(\"cols_tr   shape:\", cols_tr.shape)\n",
    "\n",
    "# max_abs = (cols_tr - cols_ref).abs().max().item()\n",
    "# print(\"Макс. разница:\", max_abs)\n",
    "# print(\"H_out, W_out =\", H_out, W_out)\n",
    "\n",
    "# # убедимся визуально\n",
    "# print(\"\\nПервый столбец эталон:\")\n",
    "# print(cols_ref[0, :, 0])\n",
    "# print(\"\\nПервый столбец Triton:\")\n",
    "# print(cols_tr[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2417ed6c-3011-455b-b1ea-b3405890e975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма cols_ref: torch.Size([1, 27, 4000000])\n",
      "Один патч 3x3  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 2.0000e+03, 2.0010e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 4.0000e+06, 4.0000e+06, 0.0000e+00, 4.0020e+06, 4.0020e+06,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0000e+06, 8.0000e+06,\n",
      "        0.0000e+00, 8.0020e+06, 8.0020e+06], device='cuda:0')\n",
      "Первый элемент всех патчей:  tensor([      0.,       0.,       0.,  ..., 3997996., 3997997., 3997998.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Форма cols_ref:\", cols_ref.shape)\n",
    "print(\"Один патч 3x3 \", cols_ref[0, :, 0])\n",
    "print(\"Первый элемент всех патчей: \",cols_ref[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9d0c7b-a8ac-4cf6-8efb-8cf32961569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _col2im_kernel(\n",
    "    dx_ptr,\n",
    "    dcols_ptr,\n",
    "    N, C, H, W,\n",
    "    KH, KW, SH, SW, PH, PW, DH, DW,\n",
    "    H_OUT, W_OUT, K_TOTAL,\n",
    "    sxn, sxc, sxh, sxw,\n",
    "    scn, sck, scl,\n",
    "    BLOCK_X: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    L = H_OUT * W_OUT\n",
    "    n = pid // L\n",
    "    l = pid %  L\n",
    "\n",
    "    oh = l // W_OUT\n",
    "    ow = l %  W_OUT\n",
    "    ih0 = oh * SH - PH\n",
    "    iw0 = ow * SW - PW\n",
    "\n",
    "    base_dx_n     = n * sxn\n",
    "    base_dcols_nl = n * scn + l * scl\n",
    "\n",
    "    k0 = 0\n",
    "    while k0 < K_TOTAL:\n",
    "        k      = k0 + tl.arange(0, BLOCK_X)\n",
    "        k_mask = k < K_TOTAL\n",
    "\n",
    "        r  = k %  (KH * KW)\n",
    "        c  = k // (KH * KW)\n",
    "        kh = r // KW\n",
    "        kw = r %  KW\n",
    "\n",
    "        ih = ih0 + kh * DH\n",
    "        iw = iw0 + kw * DW\n",
    "\n",
    "        inb = (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W) & k_mask\n",
    "\n",
    "        dval = tl.load(dcols_ptr + (base_dcols_nl + k * sck), mask=k_mask, other=0)\n",
    "        tl.atomic_add(dx_ptr + (base_dx_n + c * sxc + ih * sxh + iw * sxw), dval, mask=inb)\n",
    "\n",
    "        k0 += BLOCK_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e1af3f-2e48-430f-8147-48901495e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im_triton(\n",
    "    dcols: torch.Tensor,         # (N, K_TOTAL, L)\n",
    "    out_shape,                   # (N, C, H, W)\n",
    "    kernel_size, stride=1, padding=0, dilation=1,\n",
    "    block_x: int = 256\n",
    "):\n",
    "\n",
    "    N, C, H, W = out_shape\n",
    "    KH, KW = kernel_size\n",
    "    SH, SW = stride\n",
    "    PH, PW = padding\n",
    "    DH, DW = dilation\n",
    "\n",
    "    # Восстанавливаем H_OUT, W_OUT и L = H_OUT*W_OUT из размера dcols\n",
    "    # dcols: (N, K_TOTAL, L), где K_TOTAL = C*KH*KW\n",
    "    K_TOTAL = C * KH * KW\n",
    "    assert dcols.shape[1] == K_TOTAL\n",
    "    L = dcols.shape[2]\n",
    "\n",
    "    # Если нужно: можно вычислить H_OUT,W_OUT через формулу, но часто у тебя уже есть (например из im2col).\n",
    "    # Здесь восстановим через прямую формулу:\n",
    "    H_OUT, W_OUT = _out_hw(H, W, KH, KW, SH, SW, PH, PW, DH, DW)\n",
    "    assert H_OUT * W_OUT == L, \"L не совпадает с H_OUT*W_OUT — проверь параметры.\"\n",
    "\n",
    "    # Выходной градиент по входу (или просто собранное изображение) — аккумулируем в fp32\n",
    "    dx = torch.zeros((N, C, H, W), device=dcols.device, dtype=torch.float32)\n",
    "\n",
    "    sxn, sxc, sxh, sxw = dx.stride()\n",
    "    scn, sck, scl = dcols.stride()\n",
    "\n",
    "    grid = (N * L,)\n",
    "    _col2im_kernel[grid](\n",
    "        dx, dcols,\n",
    "        N, C, H, W,\n",
    "        KH, KW, SH, SW, PH, PW, DH, DW,\n",
    "        H_OUT, W_OUT, K_TOTAL,\n",
    "        sxn, sxc, sxh, sxw,\n",
    "        scn, sck, scl,\n",
    "        BLOCK_X=block_x,\n",
    "        num_warps=4, num_stages=2,\n",
    "    )\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d91bc52-c33c-4272-9188-e7294a2f558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[im2col] max |diff| = 0\n"
     ]
    }
   ],
   "source": [
    "N, C, H, W = 1, 3, 2000, 2000\n",
    "KH, KW = 3, 3\n",
    "SH, SW = 1, 1\n",
    "PH, PW = 1, 1\n",
    "DH, DW = 1, 1\n",
    "block_k = 256\n",
    "device = \"cuda\"\n",
    "\n",
    "# ---- данные ----\n",
    "x = torch.arange(N*C*H*W, dtype=torch.float32, device=device).reshape(N, C, H, W)\n",
    "\n",
    "\n",
    "# ---- 1) эталонный unfold ----\n",
    "cols_ref = F.unfold(\n",
    "    x, kernel_size=(KH, KW),\n",
    "    dilation=(DH, DW),\n",
    "    padding=(PH, PW),\n",
    "    stride=(SH, SW)\n",
    ")  # (N, C*KH*KW, L)\n",
    "\n",
    "# ---- 2) твой im2col_triton ----\n",
    "cols_tr, (H_out, W_out) = im2col_triton(\n",
    "    x,\n",
    "    kernel_size=(KH, KW),\n",
    "    stride=(SH, SW),\n",
    "    padding=(PH, PW),\n",
    "    dilation=(DH, DW),\n",
    "    block_k=block_k\n",
    ") \n",
    "\n",
    "x_sum_ref = F.fold(\n",
    "    cols_ref, output_size=(H, W),\n",
    "    kernel_size=(KH, KW),\n",
    "    dilation=(DH, DW),\n",
    "    padding=(PH, PW),\n",
    "    stride=(SH, SW)\n",
    ")  # (N, C, H, W)\n",
    "\n",
    "# твой col2im (должен быть определён: col2im_triton)\n",
    "x_sum_tr = col2im_triton(\n",
    "    cols_tr, out_shape=(N, C, H, W),\n",
    "    kernel_size=(KH, KW),\n",
    "    stride=(SH, SW),\n",
    "    padding=(PH, PW),\n",
    "    dilation=(DH, DW),\n",
    "    block_x=256\n",
    ").to(x_sum_ref.dtype)\n",
    "max_abs = (cols_tr - cols_ref).abs().max().item()\n",
    "print(f\"[im2col] max |diff| = {max_abs:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d33a46-7453-4c1a-8276-25f2c23204b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64,  \"BLOCK_K\": 64}, num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 128, \"BLOCK_K\": 64}, num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32}, num_warps=8, num_stages=3),\n",
    "    ],\n",
    "    key=[\"M\", \"N\", \"K\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _gemm_kernel(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    lda, ldb, ldc,                       # row-major: lda=K, ldb=N, ldc=N\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "    BLOCK_K: tl.constexpr,\n",
    "    OUT_FP16: tl.constexpr,              # хранить C в fp16 или fp32\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "\n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)   # [BM]\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)   # [BN]\n",
    "    mask_m = offs_m < M\n",
    "    mask_n = offs_n < N\n",
    "\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "\n",
    "    k0 = 0\n",
    "    while k0 < K:\n",
    "        offs_k = k0 + tl.arange(0, BLOCK_K)            # [BK]\n",
    "        mask_k = offs_k < K\n",
    "\n",
    "        a = tl.load(a_ptr + (offs_m[:, None] * lda + offs_k[None, :]),\n",
    "                    mask=mask_m[:, None] & mask_k[None, :], other=0)\n",
    "        b = tl.load(b_ptr + (offs_k[:, None] * ldb + offs_n[None, :]),\n",
    "                    mask=mask_k[:, None] & mask_n[None, :], other=0)\n",
    "\n",
    "        acc += tl.dot(a, b, out_dtype=tl.float32)\n",
    "        k0 += BLOCK_K\n",
    "\n",
    "    c = acc.to(tl.float16) if OUT_FP16 else acc\n",
    "    tl.store(c_ptr + (offs_m[:, None] * ldc + offs_n[None, :]),\n",
    "             c, mask=mask_m[:, None] & mask_n[None, :])\n",
    "\n",
    "\n",
    "def gemm_triton(A: torch.Tensor, B: torch.Tensor, out_dtype=torch.float16) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A: (M, K), B: (K, N)  -> C: (M, N)\n",
    "    - Accumulation fp32, запись в fp16 или fp32.\n",
    "    - Требует CUDA и contiguous row-major.\n",
    "    \"\"\"\n",
    "    assert A.is_cuda and B.is_cuda\n",
    "    assert A.dim() == 2 and B.dim() == 2\n",
    "    M, K = A.shape\n",
    "    Kb, N = B.shape\n",
    "    assert K == Kb\n",
    "\n",
    "    A = A.contiguous()\n",
    "    B = B.contiguous()\n",
    "\n",
    "    C = torch.empty((M, N), device=A.device,\n",
    "                    dtype=(torch.float16 if out_dtype == torch.float16 else torch.float32))\n",
    "\n",
    "    lda = A.stride(0)  # row-major: lda == K\n",
    "    ldb = B.stride(0)  # row-major: ldb == N\n",
    "    ldc = C.stride(0)  # row-major: ldc == N\n",
    "\n",
    "    grid = (triton.cdiv(M, 128), triton.cdiv(N, 128))\n",
    "    _gemm_kernel[grid](\n",
    "        A, B, C,\n",
    "        M, N, K,\n",
    "        lda, ldb, ldc,\n",
    "        OUT_FP16=(out_dtype == torch.float16),\n",
    "    )\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c3858-591a-42c9-99b2-3b04abf5f397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
