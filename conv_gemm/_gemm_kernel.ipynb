{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a598c201-64c8-4bce-84a7-6e08e02e7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c7d9ee-9a43-4b2f-8b85-3224cb9bcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64,  \"BLOCK_K\": 64}, num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 128, \"BLOCK_K\": 64}, num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32}, num_warps=8, num_stages=3),\n",
    "    ],\n",
    "    key=[\"M\", \"N\", \"K\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _gemm_kernel(\n",
    "    a_ptr, b_ptr, bias_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    lda, ldb, ldc,                              # leading dimensions (row-major: lda = K, ldb = N, ldc = N)\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "    BLOCK_K: tl.constexpr,\n",
    "    ADD_BIAS: tl.constexpr,\n",
    "    OUT_FP16: tl.constexpr,\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "\n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)      # [BM]\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)      # [BN]\n",
    "    mask_m = offs_m < M\n",
    "    mask_n = offs_n < N\n",
    "\n",
    "    # fp32 аккумулятор для численной стабильности\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "\n",
    "    # петля по K\n",
    "    k0 = 0\n",
    "    while k0 < K:\n",
    "        offs_k = k0 + tl.arange(0, BLOCK_K)               # [BK]\n",
    "        mask_k = offs_k < K\n",
    "\n",
    "        a_ptrs = a_ptr + (offs_m[:, None] * lda + offs_k[None, :])   # (BM, BK)\n",
    "        b_ptrs = b_ptr + (offs_k[:, None] * ldb + offs_n[None, :])   # (BK, BN)\n",
    "\n",
    "        a = tl.load(a_ptrs, mask=mask_m[:, None] & mask_k[None, :], other=0)\n",
    "        b = tl.load(b_ptrs, mask=mask_k[:, None] & mask_n[None, :], other=0)\n",
    "\n",
    "        # fp32 accumulate\n",
    "        acc += tl.dot(a, b, out_dtype=tl.float32)\n",
    "\n",
    "        k0 += BLOCK_K\n",
    "\n",
    "    # эпилог: добавляем bias по столбцам (OUT-ось)\n",
    "    if ADD_BIAS:\n",
    "        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0).to(tl.float32)  # [BN]\n",
    "        acc = acc + bias[None, :]\n",
    "\n",
    "    # запись\n",
    "    c_ptrs = c_ptr + (offs_m[:, None] * ldc + offs_n[None, :])      # (BM, BN)\n",
    "    if OUT_FP16:\n",
    "        tl.store(c_ptrs, acc.to(tl.float16), mask=mask_m[:, None] & mask_n[None, :])\n",
    "    else:\n",
    "        tl.store(c_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc057c94-e869-46f2-b584-b37204a578e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_gemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b835854-fd0a-4820-9947-4f2131163e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward_im2col_gemm_retongue(\n",
    "    x: torch.Tensor,              # (N,C,H,W), CUDA\n",
    "    weight: torch.Tensor,         # (OUT,C,KH,KW), CUDA\n",
    "    bias: torch.Tensor | None,    # (OUT,) or None\n",
    "    *,\n",
    "    stride=1, padding=0, dilation=1,\n",
    "    encodeRetongue=None,\n",
    "    out_dtype=torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Прямой проход Conv2d через им2кол + GEMM на Triton, с вызовом encodeRetongue.\n",
    "    Возвращает Y формы (N, OUT, H_OUT, W_OUT).\n",
    "    Требует твою функцию im2col_triton(x, kernel_size, stride, padding, dilation) -> (cols, (H_OUT, W_OUT)).\n",
    "    \"\"\"\n",
    "    assert x.is_cuda and weight.is_cuda\n",
    "    if isinstance(stride, int):   stride   = (stride, stride)\n",
    "    if isinstance(padding, int):  padding  = (padding, padding)\n",
    "    if isinstance(dilation, int): dilation = (dilation, dilation)\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    OUT, Cw, KH, KW = weight.shape\n",
    "    assert Cw == C\n",
    "\n",
    "    # 1) im2col (используем твой уже реализованный im2col_triton)\n",
    "    cols, (H_OUT, W_OUT) = im2col_triton(x, (KH, KW), stride=stride, padding=padding, dilation=dilation)\n",
    "    # cols: (N, K_TOTAL, L)\n",
    "    L = H_OUT * W_OUT\n",
    "    K_TOTAL = C * KH * KW\n",
    "\n",
    "    # 2) превратим в (M, K) и (K, N)\n",
    "    A = cols.transpose(1, 2).reshape(N * L, K_TOTAL).contiguous()               # (M=N*L, K_TOTAL)\n",
    "    B = weight.reshape(OUT, K_TOTAL).transpose(0, 1).contiguous()               # (K_TOTAL, OUT)\n",
    "\n",
    "    # 3) GEMM (с encodeRetongue и bias в эпилоге)\n",
    "    y2d = matmul_triton_retongue(A, B, bias, encodeRetongue=encodeRetongue, out_dtype=out_dtype)  # (N*L, OUT)\n",
    "\n",
    "    # 4) вернуть форму Conv2d выхода\n",
    "    y = y2d.view(N, L, OUT).transpose(1, 2).reshape(N, OUT, H_OUT, W_OUT)\n",
    "    return y\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Пример быстрой проверки корректности (не обязательно)\n",
    "# =========================\n",
    "def _quick_check():\n",
    "    torch.manual_seed(0)\n",
    "    device = \"cuda\"\n",
    "    N, C, H, W = 2, 3, 16, 16\n",
    "    OUT, KH, KW = 5, 3, 3\n",
    "    stride, padding, dilation = (2, 1), (1, 1), (1, 1)\n",
    "\n",
    "    x = torch.randn(N, C, H, W, device=device, dtype=torch.float16)\n",
    "    w = torch.randn(OUT, C, KH, KW, device=device, dtype=torch.float16)\n",
    "    b = torch.randn(OUT, device=device, dtype=torch.float32)\n",
    "\n",
    "    def encodeRetongue_fn(t):\n",
    "        # пример: тождественная функция (подставь свою)\n",
    "        return t\n",
    "\n",
    "    y_ref = F.conv2d(x.to(torch.float32), w.to(torch.float32), b.to(torch.float32),\n",
    "                     stride=stride, padding=padding, dilation=dilation, groups=1).to(torch.float16)\n",
    "\n",
    "    y = conv2d_forward_im2col_gemm_retongue(\n",
    "        x, w, b, stride=stride, padding=padding, dilation=dilation,\n",
    "        encodeRetongue=encodeRetongue_fn, out_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    torch.testing.assert_close(y, y_ref, rtol=1e-2, atol=1e-2)\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b41380-a148-45ed-ad87-cb55a99f712d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
