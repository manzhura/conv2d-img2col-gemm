{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0ecb3a-de19-4186-9744-69814f7542fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "\n",
    "# === импорт твоего слоя INT8 ===\n",
    "from conv_gemm.layers.triton_conv2d_int8 import TritonConv2dINT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bd367b-d6f4-46b4-a406-2341f122136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342041ae-9273-4fb7-b579-77c15c550685",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "Cin = 3\n",
    "Cout = 8\n",
    "H = W = 32\n",
    "K = 3\n",
    "\n",
    "x = torch.randn(N, Cin, H, W, device=device, dtype=torch.float32)\n",
    "conv_ref = nn.Conv2d(Cin, Cout, K, padding=K//2, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341c7b13-dcd1-4358-b2a7-76687612b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int8 = TritonConv2dINT8(\n",
    "    Cin, Cout, K,\n",
    "    padding=K//2,\n",
    "    precision_mode=\"int8_infer\"   # чистый INT8 инференс\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852b1b9-e876-42a0-a68a-e4407e44191a",
   "metadata": {},
   "source": [
    "# base forvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df3addc-6ff2-4941-9ab8-44b9f7a51f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv_int8.weight.copy_(conv_ref.weight)\n",
    "    if conv_ref.bias is not None:\n",
    "        conv_int8.bias.copy_(conv_ref.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f02ca4-d6b5-4227-8e57-1758ba2b1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_ref.shape: torch.Size([1, 8, 32, 32])\n",
      "y_int8.shape: torch.Size([1, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# FP32 reference\n",
    "y_ref = conv_ref(x)\n",
    "\n",
    "# INT8 Triton\n",
    "y_int8 = conv_int8(x)\n",
    "\n",
    "print(\"y_ref.shape:\", y_ref.shape)\n",
    "print(\"y_int8.shape:\", y_int8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f118a1-a4d5-4998-8f7f-95997c487610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ACCURACY CHECK ===\n",
      "max error: 2.6488137245178223\n",
      "mean error: 0.44135573506355286\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#                   ОЦЕНКА ТОЧНОСТИ\n",
    "# ============================================================\n",
    "\n",
    "err = (y_ref - y_int8).abs()\n",
    "print(\"\\n=== ACCURACY CHECK ===\")\n",
    "print(\"max error:\", err.max().item())\n",
    "print(\"mean error:\", err.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd949324-9f28-4431-9fc2-803eecc67d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SPEED (ms) ===\n",
      "PyTorch FP32 Conv2D:   0.045 ms\n",
      "Triton INT8 Conv2D:    0.545 ms\n",
      "Speedup: 0.083x\n"
     ]
    }
   ],
   "source": [
    "def bench(fn, iters=200):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) * 1000 / iters  # ms\n",
    "\n",
    "t_ref = bench(lambda: conv_ref(x))\n",
    "t_int8 = bench(lambda: conv_int8(x))\n",
    "\n",
    "print(\"\\n=== SPEED (ms) ===\")\n",
    "print(f\"PyTorch FP32 Conv2D:   {t_ref:.3f} ms\")\n",
    "print(f\"Triton INT8 Conv2D:    {t_int8:.3f} ms\")\n",
    "print(f\"Speedup: {t_ref / t_int8:.3f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62baa804-d677-4a0b-a371-7d9a64895ead",
   "metadata": {},
   "source": [
    "# BENCH FORVARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669834a1-8a75-4310-88cc-366c47333dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "from conv_gemm.layers.triton_conv2d_int8 import TritonConv2dINT8  # путь подгони под себя\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "def bench_ms(fn, iters=50):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - t0) * 1000.0 / iters\n",
    "\n",
    "\n",
    "def run_int8_conv_bench(\n",
    "    image_sizes=(32, 64, 112, 224, 512),\n",
    "    batch_sizes=(1, 2, 3, 4),\n",
    "    channels=((1, 1), (1, 3), (3, 8), (8, 16), (16, 32)),\n",
    "    kernels=(1, 3, 5, 7, 9, 11),\n",
    "    iters=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    channels: кортеж пар (Cin, Cout),\n",
    "      например: ((1,1), (1,3), (3,8), (8,16), (16,32))\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for H in image_sizes:\n",
    "        W = H\n",
    "        for N in batch_sizes:\n",
    "            for (Cin, Cout) in channels:\n",
    "                for K in kernels:\n",
    "                    # Нормальная свёртка не умеет kernel > spatial\n",
    "                    if K > H or K > W:\n",
    "                        continue\n",
    "\n",
    "                    print(f\"[bench] img={H} N={N} Cin={Cin} Cout={Cout} K={K}\")\n",
    "\n",
    "                    # Создаём вход и две свёртки\n",
    "                    x = torch.randn(N, Cin, H, W, device=device, dtype=torch.float32)\n",
    "\n",
    "                    conv_ref = nn.Conv2d(\n",
    "                        Cin, Cout, kernel_size=K,\n",
    "                        stride=1,\n",
    "                        padding=K // 2,\n",
    "                        bias=True,\n",
    "                    ).to(device)\n",
    "\n",
    "                    conv_int8 = TritonConv2dINT8(\n",
    "                        in_channels=Cin,\n",
    "                        out_channels=Cout,\n",
    "                        kernel_size=K,\n",
    "                        stride=1,\n",
    "                        padding=K // 2,\n",
    "                        dilation=1,\n",
    "                        bias=True,\n",
    "                        precision_mode=\"int8_infer\",   # чистый инференс\n",
    "                        use_weight_shadow=False,\n",
    "                    ).to(device)\n",
    "\n",
    "                    # Копируем веса/биасы, чтобы честно сравнивать\n",
    "                    with torch.no_grad():\n",
    "                        conv_int8.weight.copy_(conv_ref.weight)\n",
    "                        if conv_ref.bias is not None:\n",
    "                            conv_int8.bias.copy_(conv_ref.bias)\n",
    "\n",
    "                    # FP32 reference\n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            y_ref = conv_ref(x)\n",
    "                    except Exception as e:\n",
    "                        print(\"  [SKIP] FP32 conv failed:\", e)\n",
    "                        rows.append([H, N, Cin, Cout, K,\n",
    "                                     None, None, None, None, None, str(e)])\n",
    "                        continue\n",
    "\n",
    "                    # INT8 forward (один прогон для ошибки)\n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            y_int8 = conv_int8(x)\n",
    "                    except Exception as e:\n",
    "                        print(\"  [SKIP] INT8 conv failed:\", e)\n",
    "                        rows.append([H, N, Cin, Cout, K,\n",
    "                                     None, None, None, None, None, f\"int8_fail: {e}\"])\n",
    "                        continue\n",
    "\n",
    "                    # Ошибки\n",
    "                    err = (y_ref - y_int8).abs()\n",
    "                    err_max = err.max().item()\n",
    "                    err_mean = err.mean().item()\n",
    "\n",
    "                    # Бенч\n",
    "                    try:\n",
    "                        t_ref = bench_ms(lambda: conv_ref(x), iters=iters)\n",
    "                        t_int8 = bench_ms(lambda: conv_int8(x), iters=iters)\n",
    "                        speedup = t_ref / t_int8 if t_int8 > 0 else None\n",
    "                    except Exception as e:\n",
    "                        print(\"  [WARN] bench failed:\", e)\n",
    "                        t_ref = t_int8 = speedup = None\n",
    "\n",
    "                    rows.append([\n",
    "                        H, N, Cin, Cout, K,\n",
    "                        t_ref, t_int8, speedup,\n",
    "                        err_max, err_mean,\n",
    "                        None,\n",
    "                    ])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"img\", \"N\", \"Cin\", \"Cout\", \"K\",\n",
    "            \"t_ref_ms\", \"t_int8_ms\", \"speedup\",\n",
    "            \"err_max\", \"err_mean\", \"note\",\n",
    "        ],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eaf3a2b-ec9f-48c7-bc75-e31163967ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bench] img=32 N=1 Cin=1 Cout=1 K=1\n",
      "[bench] img=32 N=1 Cin=1 Cout=1 K=3\n",
      "[bench] img=32 N=1 Cin=1 Cout=1 K=5\n",
      "[bench] img=32 N=1 Cin=1 Cout=1 K=7\n",
      "[bench] img=32 N=1 Cin=1 Cout=1 K=9\n",
      "[bench] img=32 N=1 Cin=1 Cout=1 K=11\n",
      "[bench] img=32 N=1 Cin=1 Cout=3 K=1\n",
      "[bench] img=32 N=1 Cin=1 Cout=3 K=3\n",
      "[bench] img=32 N=1 Cin=1 Cout=3 K=5\n",
      "[bench] img=32 N=1 Cin=1 Cout=3 K=7\n",
      "[bench] img=32 N=1 Cin=1 Cout=3 K=9\n",
      "[bench] img=32 N=1 Cin=1 Cout=3 K=11\n",
      "[bench] img=32 N=1 Cin=3 Cout=8 K=1\n",
      "[bench] img=32 N=1 Cin=3 Cout=8 K=3\n",
      "[bench] img=32 N=1 Cin=3 Cout=8 K=5\n",
      "[bench] img=32 N=1 Cin=3 Cout=8 K=7\n",
      "[bench] img=32 N=1 Cin=3 Cout=8 K=9\n",
      "[bench] img=32 N=1 Cin=3 Cout=8 K=11\n",
      "[bench] img=32 N=1 Cin=8 Cout=16 K=1\n",
      "[bench] img=32 N=1 Cin=8 Cout=16 K=3\n",
      "[bench] img=32 N=1 Cin=8 Cout=16 K=5\n",
      "[bench] img=32 N=1 Cin=8 Cout=16 K=7\n",
      "[bench] img=32 N=1 Cin=8 Cout=16 K=9\n",
      "[bench] img=32 N=1 Cin=8 Cout=16 K=11\n",
      "[bench] img=32 N=1 Cin=16 Cout=32 K=1\n",
      "[bench] img=32 N=1 Cin=16 Cout=32 K=3\n",
      "[bench] img=32 N=1 Cin=16 Cout=32 K=5\n",
      "[bench] img=32 N=1 Cin=16 Cout=32 K=7\n",
      "[bench] img=32 N=1 Cin=16 Cout=32 K=9\n",
      "[bench] img=32 N=1 Cin=16 Cout=32 K=11\n",
      "[bench] img=32 N=1 Cin=32 Cout=64 K=1\n",
      "[bench] img=32 N=1 Cin=32 Cout=64 K=3\n",
      "[bench] img=32 N=1 Cin=32 Cout=64 K=5\n",
      "[bench] img=32 N=1 Cin=32 Cout=64 K=7\n",
      "[bench] img=32 N=1 Cin=32 Cout=64 K=9\n",
      "[bench] img=32 N=1 Cin=32 Cout=64 K=11\n",
      "[bench] img=32 N=2 Cin=1 Cout=1 K=1\n",
      "[bench] img=32 N=2 Cin=1 Cout=1 K=3\n",
      "[bench] img=32 N=2 Cin=1 Cout=1 K=5\n",
      "[bench] img=32 N=2 Cin=1 Cout=1 K=7\n",
      "[bench] img=32 N=2 Cin=1 Cout=1 K=9\n",
      "[bench] img=32 N=2 Cin=1 Cout=1 K=11\n",
      "[bench] img=32 N=2 Cin=1 Cout=3 K=1\n",
      "[bench] img=32 N=2 Cin=1 Cout=3 K=3\n",
      "[bench] img=32 N=2 Cin=1 Cout=3 K=5\n",
      "[bench] img=32 N=2 Cin=1 Cout=3 K=7\n",
      "[bench] img=32 N=2 Cin=1 Cout=3 K=9\n",
      "[bench] img=32 N=2 Cin=1 Cout=3 K=11\n",
      "[bench] img=32 N=2 Cin=3 Cout=8 K=1\n",
      "[bench] img=32 N=2 Cin=3 Cout=8 K=3\n",
      "[bench] img=32 N=2 Cin=3 Cout=8 K=5\n",
      "[bench] img=32 N=2 Cin=3 Cout=8 K=7\n",
      "[bench] img=32 N=2 Cin=3 Cout=8 K=9\n",
      "[bench] img=32 N=2 Cin=3 Cout=8 K=11\n",
      "[bench] img=32 N=2 Cin=8 Cout=16 K=1\n",
      "[bench] img=32 N=2 Cin=8 Cout=16 K=3\n",
      "[bench] img=32 N=2 Cin=8 Cout=16 K=5\n",
      "[bench] img=32 N=2 Cin=8 Cout=16 K=7\n",
      "[bench] img=32 N=2 Cin=8 Cout=16 K=9\n",
      "[bench] img=32 N=2 Cin=8 Cout=16 K=11\n",
      "[bench] img=32 N=2 Cin=16 Cout=32 K=1\n",
      "[bench] img=32 N=2 Cin=16 Cout=32 K=3\n",
      "[bench] img=32 N=2 Cin=16 Cout=32 K=5\n",
      "[bench] img=32 N=2 Cin=16 Cout=32 K=7\n",
      "[bench] img=32 N=2 Cin=16 Cout=32 K=9\n",
      "[bench] img=32 N=2 Cin=16 Cout=32 K=11\n",
      "[bench] img=32 N=2 Cin=32 Cout=64 K=1\n",
      "[bench] img=32 N=2 Cin=32 Cout=64 K=3\n",
      "[bench] img=32 N=2 Cin=32 Cout=64 K=5\n",
      "[bench] img=32 N=2 Cin=32 Cout=64 K=7\n",
      "[bench] img=32 N=2 Cin=32 Cout=64 K=9\n",
      "[bench] img=32 N=2 Cin=32 Cout=64 K=11\n",
      "[bench] img=32 N=4 Cin=1 Cout=1 K=1\n",
      "[bench] img=32 N=4 Cin=1 Cout=1 K=3\n",
      "[bench] img=32 N=4 Cin=1 Cout=1 K=5\n",
      "[bench] img=32 N=4 Cin=1 Cout=1 K=7\n",
      "[bench] img=32 N=4 Cin=1 Cout=1 K=9\n",
      "[bench] img=32 N=4 Cin=1 Cout=1 K=11\n",
      "[bench] img=32 N=4 Cin=1 Cout=3 K=1\n",
      "[bench] img=32 N=4 Cin=1 Cout=3 K=3\n",
      "[bench] img=32 N=4 Cin=1 Cout=3 K=5\n",
      "[bench] img=32 N=4 Cin=1 Cout=3 K=7\n",
      "[bench] img=32 N=4 Cin=1 Cout=3 K=9\n",
      "[bench] img=32 N=4 Cin=1 Cout=3 K=11\n",
      "[bench] img=32 N=4 Cin=3 Cout=8 K=1\n",
      "[bench] img=32 N=4 Cin=3 Cout=8 K=3\n",
      "[bench] img=32 N=4 Cin=3 Cout=8 K=5\n",
      "[bench] img=32 N=4 Cin=3 Cout=8 K=7\n",
      "[bench] img=32 N=4 Cin=3 Cout=8 K=9\n",
      "[bench] img=32 N=4 Cin=3 Cout=8 K=11\n",
      "[bench] img=32 N=4 Cin=8 Cout=16 K=1\n",
      "[bench] img=32 N=4 Cin=8 Cout=16 K=3\n",
      "[bench] img=32 N=4 Cin=8 Cout=16 K=5\n",
      "[bench] img=32 N=4 Cin=8 Cout=16 K=7\n",
      "[bench] img=32 N=4 Cin=8 Cout=16 K=9\n",
      "[bench] img=32 N=4 Cin=8 Cout=16 K=11\n",
      "[bench] img=32 N=4 Cin=16 Cout=32 K=1\n",
      "[bench] img=32 N=4 Cin=16 Cout=32 K=3\n",
      "[bench] img=32 N=4 Cin=16 Cout=32 K=5\n",
      "[bench] img=32 N=4 Cin=16 Cout=32 K=7\n",
      "[bench] img=32 N=4 Cin=16 Cout=32 K=9\n",
      "[bench] img=32 N=4 Cin=16 Cout=32 K=11\n",
      "[bench] img=32 N=4 Cin=32 Cout=64 K=1\n",
      "[bench] img=32 N=4 Cin=32 Cout=64 K=3\n",
      "[bench] img=32 N=4 Cin=32 Cout=64 K=5\n",
      "[bench] img=32 N=4 Cin=32 Cout=64 K=7\n",
      "[bench] img=32 N=4 Cin=32 Cout=64 K=9\n",
      "[bench] img=32 N=4 Cin=32 Cout=64 K=11\n",
      "[bench] img=32 N=8 Cin=1 Cout=1 K=1\n",
      "[bench] img=32 N=8 Cin=1 Cout=1 K=3\n",
      "[bench] img=32 N=8 Cin=1 Cout=1 K=5\n",
      "[bench] img=32 N=8 Cin=1 Cout=1 K=7\n",
      "[bench] img=32 N=8 Cin=1 Cout=1 K=9\n",
      "[bench] img=32 N=8 Cin=1 Cout=1 K=11\n",
      "[bench] img=32 N=8 Cin=1 Cout=3 K=1\n",
      "[bench] img=32 N=8 Cin=1 Cout=3 K=3\n",
      "[bench] img=32 N=8 Cin=1 Cout=3 K=5\n",
      "[bench] img=32 N=8 Cin=1 Cout=3 K=7\n",
      "[bench] img=32 N=8 Cin=1 Cout=3 K=9\n",
      "[bench] img=32 N=8 Cin=1 Cout=3 K=11\n",
      "[bench] img=32 N=8 Cin=3 Cout=8 K=1\n",
      "[bench] img=32 N=8 Cin=3 Cout=8 K=3\n",
      "[bench] img=32 N=8 Cin=3 Cout=8 K=5\n",
      "[bench] img=32 N=8 Cin=3 Cout=8 K=7\n",
      "[bench] img=32 N=8 Cin=3 Cout=8 K=9\n",
      "[bench] img=32 N=8 Cin=3 Cout=8 K=11\n",
      "[bench] img=32 N=8 Cin=8 Cout=16 K=1\n",
      "[bench] img=32 N=8 Cin=8 Cout=16 K=3\n",
      "[bench] img=32 N=8 Cin=8 Cout=16 K=5\n",
      "[bench] img=32 N=8 Cin=8 Cout=16 K=7\n",
      "[bench] img=32 N=8 Cin=8 Cout=16 K=9\n",
      "[bench] img=32 N=8 Cin=8 Cout=16 K=11\n",
      "[bench] img=32 N=8 Cin=16 Cout=32 K=1\n",
      "[bench] img=32 N=8 Cin=16 Cout=32 K=3\n",
      "[bench] img=32 N=8 Cin=16 Cout=32 K=5\n",
      "[bench] img=32 N=8 Cin=16 Cout=32 K=7\n",
      "[bench] img=32 N=8 Cin=16 Cout=32 K=9\n",
      "[bench] img=32 N=8 Cin=16 Cout=32 K=11\n",
      "[bench] img=32 N=8 Cin=32 Cout=64 K=1\n",
      "[bench] img=32 N=8 Cin=32 Cout=64 K=3\n",
      "[bench] img=32 N=8 Cin=32 Cout=64 K=5\n",
      "[bench] img=32 N=8 Cin=32 Cout=64 K=7\n",
      "[bench] img=32 N=8 Cin=32 Cout=64 K=9\n",
      "[bench] img=32 N=8 Cin=32 Cout=64 K=11\n",
      "[bench] img=64 N=1 Cin=1 Cout=1 K=1\n",
      "[bench] img=64 N=1 Cin=1 Cout=1 K=3\n",
      "[bench] img=64 N=1 Cin=1 Cout=1 K=5\n",
      "[bench] img=64 N=1 Cin=1 Cout=1 K=7\n",
      "[bench] img=64 N=1 Cin=1 Cout=1 K=9\n",
      "[bench] img=64 N=1 Cin=1 Cout=1 K=11\n",
      "[bench] img=64 N=1 Cin=1 Cout=3 K=1\n",
      "[bench] img=64 N=1 Cin=1 Cout=3 K=3\n",
      "[bench] img=64 N=1 Cin=1 Cout=3 K=5\n",
      "[bench] img=64 N=1 Cin=1 Cout=3 K=7\n",
      "[bench] img=64 N=1 Cin=1 Cout=3 K=9\n",
      "[bench] img=64 N=1 Cin=1 Cout=3 K=11\n",
      "[bench] img=64 N=1 Cin=3 Cout=8 K=1\n",
      "[bench] img=64 N=1 Cin=3 Cout=8 K=3\n",
      "[bench] img=64 N=1 Cin=3 Cout=8 K=5\n",
      "[bench] img=64 N=1 Cin=3 Cout=8 K=7\n",
      "[bench] img=64 N=1 Cin=3 Cout=8 K=9\n",
      "[bench] img=64 N=1 Cin=3 Cout=8 K=11\n",
      "[bench] img=64 N=1 Cin=8 Cout=16 K=1\n",
      "[bench] img=64 N=1 Cin=8 Cout=16 K=3\n",
      "[bench] img=64 N=1 Cin=8 Cout=16 K=5\n",
      "[bench] img=64 N=1 Cin=8 Cout=16 K=7\n",
      "[bench] img=64 N=1 Cin=8 Cout=16 K=9\n",
      "[bench] img=64 N=1 Cin=8 Cout=16 K=11\n",
      "[bench] img=64 N=1 Cin=16 Cout=32 K=1\n",
      "[bench] img=64 N=1 Cin=16 Cout=32 K=3\n",
      "[bench] img=64 N=1 Cin=16 Cout=32 K=5\n",
      "[bench] img=64 N=1 Cin=16 Cout=32 K=7\n",
      "[bench] img=64 N=1 Cin=16 Cout=32 K=9\n",
      "[bench] img=64 N=1 Cin=16 Cout=32 K=11\n",
      "[bench] img=64 N=1 Cin=32 Cout=64 K=1\n",
      "[bench] img=64 N=1 Cin=32 Cout=64 K=3\n",
      "[bench] img=64 N=1 Cin=32 Cout=64 K=5\n",
      "[bench] img=64 N=1 Cin=32 Cout=64 K=7\n",
      "[bench] img=64 N=1 Cin=32 Cout=64 K=9\n",
      "[bench] img=64 N=1 Cin=32 Cout=64 K=11\n",
      "[bench] img=64 N=2 Cin=1 Cout=1 K=1\n",
      "[bench] img=64 N=2 Cin=1 Cout=1 K=3\n",
      "[bench] img=64 N=2 Cin=1 Cout=1 K=5\n",
      "[bench] img=64 N=2 Cin=1 Cout=1 K=7\n",
      "[bench] img=64 N=2 Cin=1 Cout=1 K=9\n",
      "[bench] img=64 N=2 Cin=1 Cout=1 K=11\n",
      "[bench] img=64 N=2 Cin=1 Cout=3 K=1\n",
      "[bench] img=64 N=2 Cin=1 Cout=3 K=3\n",
      "[bench] img=64 N=2 Cin=1 Cout=3 K=5\n",
      "[bench] img=64 N=2 Cin=1 Cout=3 K=7\n",
      "[bench] img=64 N=2 Cin=1 Cout=3 K=9\n",
      "[bench] img=64 N=2 Cin=1 Cout=3 K=11\n",
      "[bench] img=64 N=2 Cin=3 Cout=8 K=1\n",
      "[bench] img=64 N=2 Cin=3 Cout=8 K=3\n",
      "[bench] img=64 N=2 Cin=3 Cout=8 K=5\n",
      "[bench] img=64 N=2 Cin=3 Cout=8 K=7\n",
      "[bench] img=64 N=2 Cin=3 Cout=8 K=9\n",
      "[bench] img=64 N=2 Cin=3 Cout=8 K=11\n",
      "[bench] img=64 N=2 Cin=8 Cout=16 K=1\n",
      "[bench] img=64 N=2 Cin=8 Cout=16 K=3\n",
      "[bench] img=64 N=2 Cin=8 Cout=16 K=5\n",
      "[bench] img=64 N=2 Cin=8 Cout=16 K=7\n",
      "[bench] img=64 N=2 Cin=8 Cout=16 K=9\n",
      "[bench] img=64 N=2 Cin=8 Cout=16 K=11\n",
      "[bench] img=64 N=2 Cin=16 Cout=32 K=1\n",
      "[bench] img=64 N=2 Cin=16 Cout=32 K=3\n",
      "[bench] img=64 N=2 Cin=16 Cout=32 K=5\n",
      "[bench] img=64 N=2 Cin=16 Cout=32 K=7\n",
      "[bench] img=64 N=2 Cin=16 Cout=32 K=9\n",
      "[bench] img=64 N=2 Cin=16 Cout=32 K=11\n",
      "[bench] img=64 N=2 Cin=32 Cout=64 K=1\n",
      "[bench] img=64 N=2 Cin=32 Cout=64 K=3\n",
      "[bench] img=64 N=2 Cin=32 Cout=64 K=5\n",
      "[bench] img=64 N=2 Cin=32 Cout=64 K=7\n",
      "[bench] img=64 N=2 Cin=32 Cout=64 K=9\n",
      "[bench] img=64 N=2 Cin=32 Cout=64 K=11\n",
      "[bench] img=64 N=4 Cin=1 Cout=1 K=1\n",
      "[bench] img=64 N=4 Cin=1 Cout=1 K=3\n",
      "[bench] img=64 N=4 Cin=1 Cout=1 K=5\n",
      "[bench] img=64 N=4 Cin=1 Cout=1 K=7\n",
      "[bench] img=64 N=4 Cin=1 Cout=1 K=9\n",
      "[bench] img=64 N=4 Cin=1 Cout=1 K=11\n",
      "[bench] img=64 N=4 Cin=1 Cout=3 K=1\n",
      "[bench] img=64 N=4 Cin=1 Cout=3 K=3\n",
      "[bench] img=64 N=4 Cin=1 Cout=3 K=5\n",
      "[bench] img=64 N=4 Cin=1 Cout=3 K=7\n",
      "[bench] img=64 N=4 Cin=1 Cout=3 K=9\n",
      "[bench] img=64 N=4 Cin=1 Cout=3 K=11\n",
      "[bench] img=64 N=4 Cin=3 Cout=8 K=1\n",
      "[bench] img=64 N=4 Cin=3 Cout=8 K=3\n",
      "[bench] img=64 N=4 Cin=3 Cout=8 K=5\n",
      "[bench] img=64 N=4 Cin=3 Cout=8 K=7\n",
      "[bench] img=64 N=4 Cin=3 Cout=8 K=9\n",
      "[bench] img=64 N=4 Cin=3 Cout=8 K=11\n",
      "[bench] img=64 N=4 Cin=8 Cout=16 K=1\n",
      "[bench] img=64 N=4 Cin=8 Cout=16 K=3\n",
      "[bench] img=64 N=4 Cin=8 Cout=16 K=5\n",
      "[bench] img=64 N=4 Cin=8 Cout=16 K=7\n",
      "[bench] img=64 N=4 Cin=8 Cout=16 K=9\n",
      "[bench] img=64 N=4 Cin=8 Cout=16 K=11\n",
      "[bench] img=64 N=4 Cin=16 Cout=32 K=1\n",
      "[bench] img=64 N=4 Cin=16 Cout=32 K=3\n",
      "[bench] img=64 N=4 Cin=16 Cout=32 K=5\n",
      "[bench] img=64 N=4 Cin=16 Cout=32 K=7\n",
      "[bench] img=64 N=4 Cin=16 Cout=32 K=9\n",
      "[bench] img=64 N=4 Cin=16 Cout=32 K=11\n",
      "[bench] img=64 N=4 Cin=32 Cout=64 K=1\n",
      "[bench] img=64 N=4 Cin=32 Cout=64 K=3\n",
      "[bench] img=64 N=4 Cin=32 Cout=64 K=5\n",
      "[bench] img=64 N=4 Cin=32 Cout=64 K=7\n",
      "[bench] img=64 N=4 Cin=32 Cout=64 K=9\n",
      "[bench] img=64 N=4 Cin=32 Cout=64 K=11\n",
      "[bench] img=64 N=8 Cin=1 Cout=1 K=1\n",
      "[bench] img=64 N=8 Cin=1 Cout=1 K=3\n",
      "[bench] img=64 N=8 Cin=1 Cout=1 K=5\n",
      "[bench] img=64 N=8 Cin=1 Cout=1 K=7\n",
      "[bench] img=64 N=8 Cin=1 Cout=1 K=9\n",
      "[bench] img=64 N=8 Cin=1 Cout=1 K=11\n",
      "[bench] img=64 N=8 Cin=1 Cout=3 K=1\n",
      "[bench] img=64 N=8 Cin=1 Cout=3 K=3\n",
      "[bench] img=64 N=8 Cin=1 Cout=3 K=5\n",
      "[bench] img=64 N=8 Cin=1 Cout=3 K=7\n",
      "[bench] img=64 N=8 Cin=1 Cout=3 K=9\n",
      "[bench] img=64 N=8 Cin=1 Cout=3 K=11\n",
      "[bench] img=64 N=8 Cin=3 Cout=8 K=1\n",
      "[bench] img=64 N=8 Cin=3 Cout=8 K=3\n",
      "[bench] img=64 N=8 Cin=3 Cout=8 K=5\n",
      "[bench] img=64 N=8 Cin=3 Cout=8 K=7\n",
      "[bench] img=64 N=8 Cin=3 Cout=8 K=9\n",
      "[bench] img=64 N=8 Cin=3 Cout=8 K=11\n",
      "[bench] img=64 N=8 Cin=8 Cout=16 K=1\n",
      "[bench] img=64 N=8 Cin=8 Cout=16 K=3\n",
      "[bench] img=64 N=8 Cin=8 Cout=16 K=5\n",
      "[bench] img=64 N=8 Cin=8 Cout=16 K=7\n",
      "[bench] img=64 N=8 Cin=8 Cout=16 K=9\n",
      "[bench] img=64 N=8 Cin=8 Cout=16 K=11\n",
      "[bench] img=64 N=8 Cin=16 Cout=32 K=1\n",
      "[bench] img=64 N=8 Cin=16 Cout=32 K=3\n",
      "[bench] img=64 N=8 Cin=16 Cout=32 K=5\n",
      "[bench] img=64 N=8 Cin=16 Cout=32 K=7\n",
      "[bench] img=64 N=8 Cin=16 Cout=32 K=9\n",
      "[bench] img=64 N=8 Cin=16 Cout=32 K=11\n",
      "[bench] img=64 N=8 Cin=32 Cout=64 K=1\n",
      "[bench] img=64 N=8 Cin=32 Cout=64 K=3\n",
      "[bench] img=64 N=8 Cin=32 Cout=64 K=5\n",
      "[bench] img=64 N=8 Cin=32 Cout=64 K=7\n",
      "[bench] img=64 N=8 Cin=32 Cout=64 K=9\n",
      "[bench] img=64 N=8 Cin=32 Cout=64 K=11\n",
      "[bench] img=112 N=1 Cin=1 Cout=1 K=1\n",
      "[bench] img=112 N=1 Cin=1 Cout=1 K=3\n",
      "[bench] img=112 N=1 Cin=1 Cout=1 K=5\n",
      "[bench] img=112 N=1 Cin=1 Cout=1 K=7\n",
      "[bench] img=112 N=1 Cin=1 Cout=1 K=9\n",
      "[bench] img=112 N=1 Cin=1 Cout=1 K=11\n",
      "[bench] img=112 N=1 Cin=1 Cout=3 K=1\n",
      "[bench] img=112 N=1 Cin=1 Cout=3 K=3\n",
      "[bench] img=112 N=1 Cin=1 Cout=3 K=5\n",
      "[bench] img=112 N=1 Cin=1 Cout=3 K=7\n",
      "[bench] img=112 N=1 Cin=1 Cout=3 K=9\n",
      "[bench] img=112 N=1 Cin=1 Cout=3 K=11\n",
      "[bench] img=112 N=1 Cin=3 Cout=8 K=1\n",
      "[bench] img=112 N=1 Cin=3 Cout=8 K=3\n",
      "[bench] img=112 N=1 Cin=3 Cout=8 K=5\n",
      "[bench] img=112 N=1 Cin=3 Cout=8 K=7\n",
      "[bench] img=112 N=1 Cin=3 Cout=8 K=9\n",
      "[bench] img=112 N=1 Cin=3 Cout=8 K=11\n",
      "[bench] img=112 N=1 Cin=8 Cout=16 K=1\n",
      "[bench] img=112 N=1 Cin=8 Cout=16 K=3\n",
      "[bench] img=112 N=1 Cin=8 Cout=16 K=5\n",
      "[bench] img=112 N=1 Cin=8 Cout=16 K=7\n",
      "[bench] img=112 N=1 Cin=8 Cout=16 K=9\n",
      "[bench] img=112 N=1 Cin=8 Cout=16 K=11\n",
      "[bench] img=112 N=1 Cin=16 Cout=32 K=1\n",
      "[bench] img=112 N=1 Cin=16 Cout=32 K=3\n",
      "[bench] img=112 N=1 Cin=16 Cout=32 K=5\n",
      "[bench] img=112 N=1 Cin=16 Cout=32 K=7\n",
      "[bench] img=112 N=1 Cin=16 Cout=32 K=9\n",
      "[bench] img=112 N=1 Cin=16 Cout=32 K=11\n",
      "[bench] img=112 N=1 Cin=32 Cout=64 K=1\n",
      "[bench] img=112 N=1 Cin=32 Cout=64 K=3\n",
      "[bench] img=112 N=1 Cin=32 Cout=64 K=5\n",
      "[bench] img=112 N=1 Cin=32 Cout=64 K=7\n",
      "[bench] img=112 N=1 Cin=32 Cout=64 K=9\n",
      "[bench] img=112 N=1 Cin=32 Cout=64 K=11\n",
      "[bench] img=112 N=2 Cin=1 Cout=1 K=1\n",
      "[bench] img=112 N=2 Cin=1 Cout=1 K=3\n",
      "[bench] img=112 N=2 Cin=1 Cout=1 K=5\n",
      "[bench] img=112 N=2 Cin=1 Cout=1 K=7\n",
      "[bench] img=112 N=2 Cin=1 Cout=1 K=9\n",
      "[bench] img=112 N=2 Cin=1 Cout=1 K=11\n",
      "[bench] img=112 N=2 Cin=1 Cout=3 K=1\n",
      "[bench] img=112 N=2 Cin=1 Cout=3 K=3\n",
      "[bench] img=112 N=2 Cin=1 Cout=3 K=5\n",
      "[bench] img=112 N=2 Cin=1 Cout=3 K=7\n",
      "[bench] img=112 N=2 Cin=1 Cout=3 K=9\n",
      "[bench] img=112 N=2 Cin=1 Cout=3 K=11\n",
      "[bench] img=112 N=2 Cin=3 Cout=8 K=1\n",
      "[bench] img=112 N=2 Cin=3 Cout=8 K=3\n",
      "[bench] img=112 N=2 Cin=3 Cout=8 K=5\n",
      "[bench] img=112 N=2 Cin=3 Cout=8 K=7\n",
      "[bench] img=112 N=2 Cin=3 Cout=8 K=9\n",
      "[bench] img=112 N=2 Cin=3 Cout=8 K=11\n",
      "[bench] img=112 N=2 Cin=8 Cout=16 K=1\n",
      "[bench] img=112 N=2 Cin=8 Cout=16 K=3\n",
      "[bench] img=112 N=2 Cin=8 Cout=16 K=5\n",
      "[bench] img=112 N=2 Cin=8 Cout=16 K=7\n",
      "[bench] img=112 N=2 Cin=8 Cout=16 K=9\n",
      "[bench] img=112 N=2 Cin=8 Cout=16 K=11\n",
      "[bench] img=112 N=2 Cin=16 Cout=32 K=1\n",
      "[bench] img=112 N=2 Cin=16 Cout=32 K=3\n",
      "[bench] img=112 N=2 Cin=16 Cout=32 K=5\n",
      "[bench] img=112 N=2 Cin=16 Cout=32 K=7\n",
      "[bench] img=112 N=2 Cin=16 Cout=32 K=9\n",
      "[bench] img=112 N=2 Cin=16 Cout=32 K=11\n",
      "[bench] img=112 N=2 Cin=32 Cout=64 K=1\n",
      "[bench] img=112 N=2 Cin=32 Cout=64 K=3\n",
      "[bench] img=112 N=2 Cin=32 Cout=64 K=5\n",
      "[bench] img=112 N=2 Cin=32 Cout=64 K=7\n",
      "[bench] img=112 N=2 Cin=32 Cout=64 K=9\n",
      "[bench] img=112 N=2 Cin=32 Cout=64 K=11\n",
      "[bench] img=112 N=4 Cin=1 Cout=1 K=1\n",
      "[bench] img=112 N=4 Cin=1 Cout=1 K=3\n",
      "[bench] img=112 N=4 Cin=1 Cout=1 K=5\n",
      "[bench] img=112 N=4 Cin=1 Cout=1 K=7\n",
      "[bench] img=112 N=4 Cin=1 Cout=1 K=9\n",
      "[bench] img=112 N=4 Cin=1 Cout=1 K=11\n",
      "[bench] img=112 N=4 Cin=1 Cout=3 K=1\n",
      "[bench] img=112 N=4 Cin=1 Cout=3 K=3\n",
      "[bench] img=112 N=4 Cin=1 Cout=3 K=5\n",
      "[bench] img=112 N=4 Cin=1 Cout=3 K=7\n",
      "[bench] img=112 N=4 Cin=1 Cout=3 K=9\n",
      "[bench] img=112 N=4 Cin=1 Cout=3 K=11\n",
      "[bench] img=112 N=4 Cin=3 Cout=8 K=1\n",
      "[bench] img=112 N=4 Cin=3 Cout=8 K=3\n",
      "[bench] img=112 N=4 Cin=3 Cout=8 K=5\n",
      "[bench] img=112 N=4 Cin=3 Cout=8 K=7\n",
      "[bench] img=112 N=4 Cin=3 Cout=8 K=9\n",
      "[bench] img=112 N=4 Cin=3 Cout=8 K=11\n",
      "[bench] img=112 N=4 Cin=8 Cout=16 K=1\n",
      "[bench] img=112 N=4 Cin=8 Cout=16 K=3\n",
      "[bench] img=112 N=4 Cin=8 Cout=16 K=5\n",
      "[bench] img=112 N=4 Cin=8 Cout=16 K=7\n",
      "[bench] img=112 N=4 Cin=8 Cout=16 K=9\n",
      "[bench] img=112 N=4 Cin=8 Cout=16 K=11\n",
      "[bench] img=112 N=4 Cin=16 Cout=32 K=1\n",
      "[bench] img=112 N=4 Cin=16 Cout=32 K=3\n",
      "[bench] img=112 N=4 Cin=16 Cout=32 K=5\n",
      "[bench] img=112 N=4 Cin=16 Cout=32 K=7\n",
      "[bench] img=112 N=4 Cin=16 Cout=32 K=9\n",
      "[bench] img=112 N=4 Cin=16 Cout=32 K=11\n",
      "[bench] img=112 N=4 Cin=32 Cout=64 K=1\n",
      "[bench] img=112 N=4 Cin=32 Cout=64 K=3\n",
      "[bench] img=112 N=4 Cin=32 Cout=64 K=5\n",
      "[bench] img=112 N=4 Cin=32 Cout=64 K=7\n",
      "[bench] img=112 N=4 Cin=32 Cout=64 K=9\n",
      "[bench] img=112 N=4 Cin=32 Cout=64 K=11\n",
      "[bench] img=112 N=8 Cin=1 Cout=1 K=1\n",
      "[bench] img=112 N=8 Cin=1 Cout=1 K=3\n",
      "[bench] img=112 N=8 Cin=1 Cout=1 K=5\n",
      "[bench] img=112 N=8 Cin=1 Cout=1 K=7\n",
      "[bench] img=112 N=8 Cin=1 Cout=1 K=9\n",
      "[bench] img=112 N=8 Cin=1 Cout=1 K=11\n",
      "[bench] img=112 N=8 Cin=1 Cout=3 K=1\n",
      "[bench] img=112 N=8 Cin=1 Cout=3 K=3\n",
      "[bench] img=112 N=8 Cin=1 Cout=3 K=5\n",
      "[bench] img=112 N=8 Cin=1 Cout=3 K=7\n",
      "[bench] img=112 N=8 Cin=1 Cout=3 K=9\n",
      "[bench] img=112 N=8 Cin=1 Cout=3 K=11\n",
      "[bench] img=112 N=8 Cin=3 Cout=8 K=1\n",
      "[bench] img=112 N=8 Cin=3 Cout=8 K=3\n",
      "[bench] img=112 N=8 Cin=3 Cout=8 K=5\n",
      "[bench] img=112 N=8 Cin=3 Cout=8 K=7\n",
      "[bench] img=112 N=8 Cin=3 Cout=8 K=9\n",
      "[bench] img=112 N=8 Cin=3 Cout=8 K=11\n",
      "[bench] img=112 N=8 Cin=8 Cout=16 K=1\n",
      "[bench] img=112 N=8 Cin=8 Cout=16 K=3\n",
      "[bench] img=112 N=8 Cin=8 Cout=16 K=5\n",
      "[bench] img=112 N=8 Cin=8 Cout=16 K=7\n",
      "[bench] img=112 N=8 Cin=8 Cout=16 K=9\n",
      "[bench] img=112 N=8 Cin=8 Cout=16 K=11\n",
      "[bench] img=112 N=8 Cin=16 Cout=32 K=1\n",
      "[bench] img=112 N=8 Cin=16 Cout=32 K=3\n",
      "[bench] img=112 N=8 Cin=16 Cout=32 K=5\n",
      "[bench] img=112 N=8 Cin=16 Cout=32 K=7\n",
      "[bench] img=112 N=8 Cin=16 Cout=32 K=9\n",
      "[bench] img=112 N=8 Cin=16 Cout=32 K=11\n",
      "[bench] img=112 N=8 Cin=32 Cout=64 K=1\n",
      "[bench] img=112 N=8 Cin=32 Cout=64 K=3\n",
      "[bench] img=112 N=8 Cin=32 Cout=64 K=5\n",
      "[bench] img=112 N=8 Cin=32 Cout=64 K=7\n",
      "[bench] img=112 N=8 Cin=32 Cout=64 K=9\n",
      "[bench] img=112 N=8 Cin=32 Cout=64 K=11\n",
      "[bench] img=224 N=1 Cin=1 Cout=1 K=1\n",
      "[bench] img=224 N=1 Cin=1 Cout=1 K=3\n",
      "[bench] img=224 N=1 Cin=1 Cout=1 K=5\n",
      "[bench] img=224 N=1 Cin=1 Cout=1 K=7\n",
      "[bench] img=224 N=1 Cin=1 Cout=1 K=9\n",
      "[bench] img=224 N=1 Cin=1 Cout=1 K=11\n",
      "[bench] img=224 N=1 Cin=1 Cout=3 K=1\n",
      "[bench] img=224 N=1 Cin=1 Cout=3 K=3\n",
      "[bench] img=224 N=1 Cin=1 Cout=3 K=5\n",
      "[bench] img=224 N=1 Cin=1 Cout=3 K=7\n",
      "[bench] img=224 N=1 Cin=1 Cout=3 K=9\n",
      "[bench] img=224 N=1 Cin=1 Cout=3 K=11\n",
      "[bench] img=224 N=1 Cin=3 Cout=8 K=1\n",
      "[bench] img=224 N=1 Cin=3 Cout=8 K=3\n",
      "[bench] img=224 N=1 Cin=3 Cout=8 K=5\n",
      "[bench] img=224 N=1 Cin=3 Cout=8 K=7\n",
      "[bench] img=224 N=1 Cin=3 Cout=8 K=9\n",
      "[bench] img=224 N=1 Cin=3 Cout=8 K=11\n",
      "[bench] img=224 N=1 Cin=8 Cout=16 K=1\n",
      "[bench] img=224 N=1 Cin=8 Cout=16 K=3\n",
      "[bench] img=224 N=1 Cin=8 Cout=16 K=5\n",
      "[bench] img=224 N=1 Cin=8 Cout=16 K=7\n",
      "[bench] img=224 N=1 Cin=8 Cout=16 K=9\n",
      "[bench] img=224 N=1 Cin=8 Cout=16 K=11\n",
      "[bench] img=224 N=1 Cin=16 Cout=32 K=1\n",
      "[bench] img=224 N=1 Cin=16 Cout=32 K=3\n",
      "[bench] img=224 N=1 Cin=16 Cout=32 K=5\n",
      "[bench] img=224 N=1 Cin=16 Cout=32 K=7\n",
      "[bench] img=224 N=1 Cin=16 Cout=32 K=9\n",
      "[bench] img=224 N=1 Cin=16 Cout=32 K=11\n",
      "[bench] img=224 N=1 Cin=32 Cout=64 K=1\n",
      "[bench] img=224 N=1 Cin=32 Cout=64 K=3\n",
      "[bench] img=224 N=1 Cin=32 Cout=64 K=5\n",
      "[bench] img=224 N=1 Cin=32 Cout=64 K=7\n",
      "[bench] img=224 N=1 Cin=32 Cout=64 K=9\n",
      "[bench] img=224 N=1 Cin=32 Cout=64 K=11\n",
      "[bench] img=224 N=2 Cin=1 Cout=1 K=1\n",
      "[bench] img=224 N=2 Cin=1 Cout=1 K=3\n",
      "[bench] img=224 N=2 Cin=1 Cout=1 K=5\n",
      "[bench] img=224 N=2 Cin=1 Cout=1 K=7\n",
      "[bench] img=224 N=2 Cin=1 Cout=1 K=9\n",
      "[bench] img=224 N=2 Cin=1 Cout=1 K=11\n",
      "[bench] img=224 N=2 Cin=1 Cout=3 K=1\n",
      "[bench] img=224 N=2 Cin=1 Cout=3 K=3\n",
      "[bench] img=224 N=2 Cin=1 Cout=3 K=5\n",
      "[bench] img=224 N=2 Cin=1 Cout=3 K=7\n",
      "[bench] img=224 N=2 Cin=1 Cout=3 K=9\n",
      "[bench] img=224 N=2 Cin=1 Cout=3 K=11\n",
      "[bench] img=224 N=2 Cin=3 Cout=8 K=1\n",
      "[bench] img=224 N=2 Cin=3 Cout=8 K=3\n",
      "[bench] img=224 N=2 Cin=3 Cout=8 K=5\n",
      "[bench] img=224 N=2 Cin=3 Cout=8 K=7\n",
      "[bench] img=224 N=2 Cin=3 Cout=8 K=9\n",
      "[bench] img=224 N=2 Cin=3 Cout=8 K=11\n",
      "[bench] img=224 N=2 Cin=8 Cout=16 K=1\n",
      "[bench] img=224 N=2 Cin=8 Cout=16 K=3\n",
      "[bench] img=224 N=2 Cin=8 Cout=16 K=5\n",
      "[bench] img=224 N=2 Cin=8 Cout=16 K=7\n",
      "[bench] img=224 N=2 Cin=8 Cout=16 K=9\n",
      "[bench] img=224 N=2 Cin=8 Cout=16 K=11\n",
      "[bench] img=224 N=2 Cin=16 Cout=32 K=1\n",
      "[bench] img=224 N=2 Cin=16 Cout=32 K=3\n",
      "[bench] img=224 N=2 Cin=16 Cout=32 K=5\n",
      "[bench] img=224 N=2 Cin=16 Cout=32 K=7\n",
      "[bench] img=224 N=2 Cin=16 Cout=32 K=9\n",
      "[bench] img=224 N=2 Cin=16 Cout=32 K=11\n",
      "[bench] img=224 N=2 Cin=32 Cout=64 K=1\n",
      "[bench] img=224 N=2 Cin=32 Cout=64 K=3\n",
      "[bench] img=224 N=2 Cin=32 Cout=64 K=5\n",
      "[bench] img=224 N=2 Cin=32 Cout=64 K=7\n",
      "[bench] img=224 N=2 Cin=32 Cout=64 K=9\n",
      "[bench] img=224 N=2 Cin=32 Cout=64 K=11\n",
      "[bench] img=224 N=4 Cin=1 Cout=1 K=1\n",
      "[bench] img=224 N=4 Cin=1 Cout=1 K=3\n",
      "[bench] img=224 N=4 Cin=1 Cout=1 K=5\n",
      "[bench] img=224 N=4 Cin=1 Cout=1 K=7\n",
      "[bench] img=224 N=4 Cin=1 Cout=1 K=9\n",
      "[bench] img=224 N=4 Cin=1 Cout=1 K=11\n",
      "[bench] img=224 N=4 Cin=1 Cout=3 K=1\n",
      "[bench] img=224 N=4 Cin=1 Cout=3 K=3\n",
      "[bench] img=224 N=4 Cin=1 Cout=3 K=5\n",
      "[bench] img=224 N=4 Cin=1 Cout=3 K=7\n",
      "[bench] img=224 N=4 Cin=1 Cout=3 K=9\n",
      "[bench] img=224 N=4 Cin=1 Cout=3 K=11\n",
      "[bench] img=224 N=4 Cin=3 Cout=8 K=1\n",
      "[bench] img=224 N=4 Cin=3 Cout=8 K=3\n",
      "[bench] img=224 N=4 Cin=3 Cout=8 K=5\n",
      "[bench] img=224 N=4 Cin=3 Cout=8 K=7\n",
      "[bench] img=224 N=4 Cin=3 Cout=8 K=9\n",
      "[bench] img=224 N=4 Cin=3 Cout=8 K=11\n",
      "[bench] img=224 N=4 Cin=8 Cout=16 K=1\n",
      "[bench] img=224 N=4 Cin=8 Cout=16 K=3\n",
      "[bench] img=224 N=4 Cin=8 Cout=16 K=5\n",
      "[bench] img=224 N=4 Cin=8 Cout=16 K=7\n",
      "[bench] img=224 N=4 Cin=8 Cout=16 K=9\n",
      "[bench] img=224 N=4 Cin=8 Cout=16 K=11\n",
      "[bench] img=224 N=4 Cin=16 Cout=32 K=1\n",
      "[bench] img=224 N=4 Cin=16 Cout=32 K=3\n",
      "[bench] img=224 N=4 Cin=16 Cout=32 K=5\n",
      "[bench] img=224 N=4 Cin=16 Cout=32 K=7\n",
      "[bench] img=224 N=4 Cin=16 Cout=32 K=9\n",
      "[bench] img=224 N=4 Cin=16 Cout=32 K=11\n",
      "[bench] img=224 N=4 Cin=32 Cout=64 K=1\n",
      "[bench] img=224 N=4 Cin=32 Cout=64 K=3\n",
      "[bench] img=224 N=4 Cin=32 Cout=64 K=5\n",
      "[bench] img=224 N=4 Cin=32 Cout=64 K=7\n",
      "[bench] img=224 N=4 Cin=32 Cout=64 K=9\n",
      "[bench] img=224 N=4 Cin=32 Cout=64 K=11\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 742.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 563.81 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 580.00 MiB memory in use. Of the allocated memory 179.19 MiB is allocated by PyTorch, and 210.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[bench] img=224 N=8 Cin=1 Cout=1 K=1\n",
      "[bench] img=224 N=8 Cin=1 Cout=1 K=3\n",
      "[bench] img=224 N=8 Cin=1 Cout=1 K=5\n",
      "[bench] img=224 N=8 Cin=1 Cout=1 K=7\n",
      "[bench] img=224 N=8 Cin=1 Cout=1 K=9\n",
      "[bench] img=224 N=8 Cin=1 Cout=1 K=11\n",
      "[bench] img=224 N=8 Cin=1 Cout=3 K=1\n",
      "[bench] img=224 N=8 Cin=1 Cout=3 K=3\n",
      "[bench] img=224 N=8 Cin=1 Cout=3 K=5\n",
      "[bench] img=224 N=8 Cin=1 Cout=3 K=7\n",
      "[bench] img=224 N=8 Cin=1 Cout=3 K=9\n",
      "[bench] img=224 N=8 Cin=1 Cout=3 K=11\n",
      "[bench] img=224 N=8 Cin=3 Cout=8 K=1\n",
      "[bench] img=224 N=8 Cin=3 Cout=8 K=3\n",
      "[bench] img=224 N=8 Cin=3 Cout=8 K=5\n",
      "[bench] img=224 N=8 Cin=3 Cout=8 K=7\n",
      "[bench] img=224 N=8 Cin=3 Cout=8 K=9\n",
      "[bench] img=224 N=8 Cin=3 Cout=8 K=11\n",
      "[bench] img=224 N=8 Cin=8 Cout=16 K=1\n",
      "[bench] img=224 N=8 Cin=8 Cout=16 K=3\n",
      "[bench] img=224 N=8 Cin=8 Cout=16 K=5\n",
      "[bench] img=224 N=8 Cin=8 Cout=16 K=7\n",
      "[bench] img=224 N=8 Cin=8 Cout=16 K=9\n",
      "[bench] img=224 N=8 Cin=8 Cout=16 K=11\n",
      "[bench] img=224 N=8 Cin=16 Cout=32 K=1\n",
      "[bench] img=224 N=8 Cin=16 Cout=32 K=3\n",
      "[bench] img=224 N=8 Cin=16 Cout=32 K=5\n",
      "[bench] img=224 N=8 Cin=16 Cout=32 K=7\n",
      "[bench] img=224 N=8 Cin=16 Cout=32 K=9\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 69.69 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 950.00 MiB memory in use. Of the allocated memory 178.01 MiB is allocated by PyTorch, and 581.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[bench] img=224 N=8 Cin=16 Cout=32 K=11\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 742.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 69.69 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 950.00 MiB memory in use. Of the allocated memory 178.12 MiB is allocated by PyTorch, and 581.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[bench] img=224 N=8 Cin=32 Cout=64 K=1\n",
      "[bench] img=224 N=8 Cin=32 Cout=64 K=3\n",
      "[bench] img=224 N=8 Cin=32 Cout=64 K=5\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 262.06 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 950.00 MiB memory in use. Of the allocated memory 355.69 MiB is allocated by PyTorch, and 404.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[bench] img=224 N=8 Cin=32 Cout=64 K=7\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 602.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 634.06 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 578.00 MiB memory in use. Of the allocated memory 355.97 MiB is allocated by PyTorch, and 32.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[bench] img=224 N=8 Cin=32 Cout=64 K=9\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 994.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 486.06 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 726.00 MiB memory in use. Of the allocated memory 357.34 MiB is allocated by PyTorch, and 178.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[bench] img=224 N=8 Cin=32 Cout=64 K=11\n",
      "  [SKIP] INT8 conv failed: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 7.68 GiB of which 632.06 MiB is free. Process 20376 has 4.78 GiB memory in use. Process 151744 has 432.00 MiB memory in use. Process 180859 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 580.00 MiB memory in use. Of the allocated memory 356.81 MiB is allocated by PyTorch, and 33.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "channels_cfg = (\n",
    "    (1, 1),\n",
    "    (1, 3),\n",
    "    (3, 8),\n",
    "    (8, 16),\n",
    "    (16, 32),\n",
    "    (32, 64),\n",
    ")\n",
    "\n",
    "df = run_int8_conv_bench(\n",
    "    image_sizes=(32, 64, 112, 224),\n",
    "    batch_sizes=(1, 2, 4, 8),\n",
    "    channels=channels_cfg,\n",
    "    kernels=(1, 3, 5, 7, 9, 11),\n",
    "    iters=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5044f6fe-935a-4c82-ab63-b9fc24834f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>N</th>\n",
       "      <th>Cin</th>\n",
       "      <th>Cout</th>\n",
       "      <th>K</th>\n",
       "      <th>t_ref_ms</th>\n",
       "      <th>t_int8_ms</th>\n",
       "      <th>speedup</th>\n",
       "      <th>err_max</th>\n",
       "      <th>err_mean</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.694116</td>\n",
       "      <td>0.592287</td>\n",
       "      <td>1.171924</td>\n",
       "      <td>2.656394</td>\n",
       "      <td>0.452488</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.509389</td>\n",
       "      <td>0.974933</td>\n",
       "      <td>0.522486</td>\n",
       "      <td>3.487287</td>\n",
       "      <td>0.447657</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>224</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1.052221</td>\n",
       "      <td>4.354588</td>\n",
       "      <td>0.241635</td>\n",
       "      <td>2.957939</td>\n",
       "      <td>0.452319</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>224</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2.083238</td>\n",
       "      <td>8.775520</td>\n",
       "      <td>0.237392</td>\n",
       "      <td>2.881968</td>\n",
       "      <td>0.449797</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.165160</td>\n",
       "      <td>0.733606</td>\n",
       "      <td>0.225135</td>\n",
       "      <td>2.454377</td>\n",
       "      <td>0.416937</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.134540</td>\n",
       "      <td>0.608317</td>\n",
       "      <td>0.221167</td>\n",
       "      <td>2.725905</td>\n",
       "      <td>0.447466</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>224</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1.343330</td>\n",
       "      <td>6.215819</td>\n",
       "      <td>0.216115</td>\n",
       "      <td>3.069810</td>\n",
       "      <td>0.451914</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.118732</td>\n",
       "      <td>0.554172</td>\n",
       "      <td>0.214252</td>\n",
       "      <td>2.567429</td>\n",
       "      <td>0.419619</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>224</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.514722</td>\n",
       "      <td>2.420974</td>\n",
       "      <td>0.212609</td>\n",
       "      <td>2.999856</td>\n",
       "      <td>0.459054</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>224</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293859</td>\n",
       "      <td>1.394796</td>\n",
       "      <td>0.210682</td>\n",
       "      <td>3.578571</td>\n",
       "      <td>0.439170</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.114902</td>\n",
       "      <td>0.555968</td>\n",
       "      <td>0.206670</td>\n",
       "      <td>2.680249</td>\n",
       "      <td>0.424279</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.118589</td>\n",
       "      <td>0.585222</td>\n",
       "      <td>0.202640</td>\n",
       "      <td>2.505896</td>\n",
       "      <td>0.453768</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.251714</td>\n",
       "      <td>1.252532</td>\n",
       "      <td>0.200964</td>\n",
       "      <td>2.892299</td>\n",
       "      <td>0.443487</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>112</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.322771</td>\n",
       "      <td>1.635051</td>\n",
       "      <td>0.197407</td>\n",
       "      <td>3.040356</td>\n",
       "      <td>0.445423</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>112</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.486914</td>\n",
       "      <td>2.536631</td>\n",
       "      <td>0.191953</td>\n",
       "      <td>2.713216</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>224</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.330313</td>\n",
       "      <td>1.733748</td>\n",
       "      <td>0.190520</td>\n",
       "      <td>2.882931</td>\n",
       "      <td>0.456075</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.095733</td>\n",
       "      <td>0.503429</td>\n",
       "      <td>0.190162</td>\n",
       "      <td>2.900233</td>\n",
       "      <td>0.450494</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>224</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.647902</td>\n",
       "      <td>3.421617</td>\n",
       "      <td>0.189356</td>\n",
       "      <td>3.092120</td>\n",
       "      <td>0.459110</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.120791</td>\n",
       "      <td>0.638922</td>\n",
       "      <td>0.189054</td>\n",
       "      <td>2.872022</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097815</td>\n",
       "      <td>0.521016</td>\n",
       "      <td>0.187739</td>\n",
       "      <td>2.591799</td>\n",
       "      <td>0.444313</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.475208</td>\n",
       "      <td>2.578060</td>\n",
       "      <td>0.184328</td>\n",
       "      <td>2.897853</td>\n",
       "      <td>0.454640</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.386079</td>\n",
       "      <td>2.135181</td>\n",
       "      <td>0.180818</td>\n",
       "      <td>2.823719</td>\n",
       "      <td>0.450728</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.164191</td>\n",
       "      <td>0.913302</td>\n",
       "      <td>0.179777</td>\n",
       "      <td>2.610404</td>\n",
       "      <td>0.433493</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.146866</td>\n",
       "      <td>0.818006</td>\n",
       "      <td>0.179541</td>\n",
       "      <td>2.696876</td>\n",
       "      <td>0.425162</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.167505</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>0.179184</td>\n",
       "      <td>2.706684</td>\n",
       "      <td>0.457375</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.168053</td>\n",
       "      <td>0.948842</td>\n",
       "      <td>0.177114</td>\n",
       "      <td>2.792412</td>\n",
       "      <td>0.459720</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.474644</td>\n",
       "      <td>2.695425</td>\n",
       "      <td>0.176092</td>\n",
       "      <td>3.069971</td>\n",
       "      <td>0.448094</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.143536</td>\n",
       "      <td>0.824857</td>\n",
       "      <td>0.174013</td>\n",
       "      <td>2.591458</td>\n",
       "      <td>0.441763</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.136439</td>\n",
       "      <td>0.787989</td>\n",
       "      <td>0.173148</td>\n",
       "      <td>2.573456</td>\n",
       "      <td>0.441073</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.103291</td>\n",
       "      <td>0.601419</td>\n",
       "      <td>0.171745</td>\n",
       "      <td>2.611743</td>\n",
       "      <td>0.452283</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     img  N  Cin  Cout   K  t_ref_ms  t_int8_ms   speedup   err_max  err_mean  \\\n",
       "242   64  4   16    32   5  0.694116   0.592287  1.171924  2.656394  0.452488   \n",
       "278   64  8   16    32   5  0.509389   0.974933  0.522486  3.487287  0.447657   \n",
       "521  224  4    3     8  11  1.052221   4.354588  0.241635  2.957939  0.452319   \n",
       "557  224  8    3     8  11  2.083238   8.775520  0.237392  2.881968  0.449797   \n",
       "71    32  2   32    64  11  0.165160   0.733606  0.225135  2.454377  0.416937   \n",
       "177   64  1   32    64   7  0.134540   0.608317  0.221167  2.725905  0.447466   \n",
       "556  224  8    3     8   9  1.343330   6.215819  0.216115  3.069810  0.451914   \n",
       "35    32  1   32    64  11  0.118732   0.554172  0.214252  2.567429  0.419619   \n",
       "485  224  2    3     8  11  0.514722   2.420974  0.212609  2.999856  0.459054   \n",
       "558  224  8    8    16   1  0.293859   1.394796  0.210682  3.578571  0.439170   \n",
       "70    32  2   32    64   9  0.114902   0.555968  0.206670  2.680249  0.424279   \n",
       "305  112  1    3     8  11  0.118589   0.585222  0.202640  2.505896  0.453768   \n",
       "377  112  4    3     8  11  0.251714   1.252532  0.200964  2.892299  0.443487   \n",
       "412  112  8    3     8   9  0.322771   1.635051  0.197407  3.040356  0.445423   \n",
       "413  112  8    3     8  11  0.486914   2.536631  0.191953  2.713216  0.446809   \n",
       "484  224  2    3     8   9  0.330313   1.733748  0.190520  2.882931  0.456075   \n",
       "176   64  1   32    64   5  0.095733   0.503429  0.190162  2.900233  0.450494   \n",
       "520  224  4    3     8   9  0.647902   3.421617  0.189356  3.092120  0.459110   \n",
       "390  112  4   32    64   1  0.120791   0.638922  0.189054  2.872022  0.462585   \n",
       "201   64  2    8    16   7  0.097815   0.521016  0.187739  2.591799  0.444313   \n",
       "392  112  4   32    64   5  0.475208   2.578060  0.184328  2.897853  0.454640   \n",
       "322  112  1   32    64   9  0.386079   2.135181  0.180818  2.823719  0.450728   \n",
       "269   64  8    3     8  11  0.164191   0.913302  0.179777  2.610404  0.433493   \n",
       "106   32  4   32    64   9  0.146866   0.818006  0.179541  2.696876  0.425162   \n",
       "376  112  4    3     8   9  0.167505   0.934823  0.179184  2.706684  0.457375   \n",
       "448  224  1    3     8   9  0.168053   0.948842  0.177114  2.792412  0.459720   \n",
       "323  112  1   32    64  11  0.474644   2.695425  0.176092  3.069971  0.448094   \n",
       "178   64  1   32    64   9  0.143536   0.824857  0.174013  2.591458  0.441763   \n",
       "341  112  2    3     8  11  0.136439   0.787989  0.173148  2.573456  0.441073   \n",
       "340  112  2    3     8   9  0.103291   0.601419  0.171745  2.611743  0.452283   \n",
       "\n",
       "     note  \n",
       "242  None  \n",
       "278  None  \n",
       "521  None  \n",
       "557  None  \n",
       "71   None  \n",
       "177  None  \n",
       "556  None  \n",
       "35   None  \n",
       "485  None  \n",
       "558  None  \n",
       "70   None  \n",
       "305  None  \n",
       "377  None  \n",
       "412  None  \n",
       "413  None  \n",
       "484  None  \n",
       "176  None  \n",
       "520  None  \n",
       "390  None  \n",
       "201  None  \n",
       "392  None  \n",
       "322  None  \n",
       "269  None  \n",
       "106  None  \n",
       "376  None  \n",
       "448  None  \n",
       "323  None  \n",
       "178  None  \n",
       "341  None  \n",
       "340  None  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Топ по ускорению среди конфигов, где всё отработало\n",
    "df_valid = df.dropna(subset=[\"t_ref_ms\", \"t_int8_ms\", \"speedup\"])\n",
    "df_top = df_valid.sort_values(\"speedup\", ascending=False).head(30)\n",
    "df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7885ce7-333c-4b88-b679-1f329fc8022e",
   "metadata": {},
   "source": [
    "# base backvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b73e30-0b6b-421b-a3f7-041e864dc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_backward_ms(module, x, iters=50):\n",
    "    \"\"\"\n",
    "    Меряем время полного прохода: forward + backward по loss = y.sum().\n",
    "    Градиенты по весам и входу считаются, но не используются.\n",
    "    \"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        # новый вход с requires_grad, чтобы каждый раз строился граф\n",
    "        x_in = x.detach().clone().requires_grad_(True)\n",
    "        module.zero_grad(set_to_none=True)\n",
    "\n",
    "        y = module(x_in)\n",
    "        loss = y.sum()\n",
    "        loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - t0) * 1000.0 / iters  # ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b121254-bd95-4352-b586-abdecbe4169c",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 38:15:\n            mask=mask_a, other=0\n        )\n        b = tl.load(\n            B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn,\n            mask=mask_b, other=0\n        )\n\n        if USE_FP16:\n            a = a.to(tl.float16)\n            b = b.to(tl.float16)\n        # acc остаётся fp32\n        acc += tl.dot(a, b, allow_tf32=False)\n               ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/language/core.py:42\u001b[0m, in \u001b[0;36mbuiltin.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you forget to add @triton.jit ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`_semantic` argument must be provided outside of JIT functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/language/core.py:2045\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(input, other, acc, input_precision, allow_tf32, max_num_imprecise_acc, out_dtype, _semantic)\u001b[0m\n\u001b[1;32m   2044\u001b[0m acc \u001b[38;5;241m=\u001b[39m _unwrap_if_constexpr(acc)\n\u001b[0;32m-> 2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_semantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_imprecise_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/language/semantic.py:1504\u001b[0m, in \u001b[0;36mTritonSemantic.dot\u001b[0;34m(self, lhs, rhs, acc, input_precision, max_num_imprecise_acc, out_dtype)\u001b[0m\n\u001b[1;32m   1502\u001b[0m min_dot_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mcodegen_fns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_dot_size\u001b[39m\u001b[38;5;124m\"\u001b[39m](lhs\u001b[38;5;241m.\u001b[39mtype, rhs\u001b[38;5;241m.\u001b[39mtype)\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_dot_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_dot_size[\u001b[38;5;241m2\u001b[39m] \\\n\u001b[0;32m-> 1504\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m rhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_dot_size[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[1;32m   1505\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shapes should have M >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_dot_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, N >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_dot_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and K >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_dot_size[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mscalar\u001b[38;5;241m.\u001b[39mis_int():\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input shapes should have M >= 16, N >= 16 and K >= 16",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m loss_int \u001b[38;5;241m=\u001b[39m y_int\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     14\u001b[0m loss_ref\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss_int\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# градиенты по входу\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dx_err \u001b[38;5;241m=\u001b[39m (x_ref\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m-\u001b[39m x_int\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/torch/autograd/function.py:311\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/conv2d-img2col-gemm/conv_gemm/operators/triton_conv2d_int8_fn.py:223\u001b[0m, in \u001b[0;36mTritonConv2dInt8Fn.backward\u001b[0;34m(ctx, gy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m#                               dX\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m    221\u001b[0m W_matT \u001b[38;5;241m=\u001b[39m w32\u001b[38;5;241m.\u001b[39mview(Cout, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m dcols \u001b[38;5;241m=\u001b[39m \u001b[43mtriton_gemm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdy_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_matT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_M\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFP32_GEMM_BLOCK_M\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_N\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFP32_GEMM_BLOCK_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFP32_GEMM_WARPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFP32_GEMM_STAGES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m dx32 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((N, Cin, H, W), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    235\u001b[0m grid_c2i \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     triton\u001b[38;5;241m.\u001b[39mcdiv(M, FP32_C2I_BLOCK_M),\n\u001b[1;32m    237\u001b[0m     triton\u001b[38;5;241m.\u001b[39mcdiv(K, FP32_C2I_BLOCK_K),\n\u001b[1;32m    238\u001b[0m )\n",
      "File \u001b[0;32m~/ITMO/EDLM/conv2d-img2col-gemm/conv_gemm/triton_kernels/fp32/gemm_kernel.py:87\u001b[0m, in \u001b[0;36mtriton_gemm\u001b[0;34m(A, B, use_fp16, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages)\u001b[0m\n\u001b[1;32m     84\u001b[0m c_m, c_n \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mstride()\n\u001b[1;32m     86\u001b[0m grid \u001b[38;5;241m=\u001b[39m (triton\u001b[38;5;241m.\u001b[39mcdiv(M, BLOCK_M), triton\u001b[38;5;241m.\u001b[39mcdiv(N, BLOCK_N))\n\u001b[0;32m---> 87\u001b[0m \u001b[43mgemm_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_M\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_M\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_N\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mUSE_FP16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_warps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stages\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m C\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/runtime/jit.py:390\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/runtime/jit.py:594\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    593\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constexprs, attrs)\n\u001b[0;32m--> 594\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m kernel_cache[key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_hook(knobs\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mjit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n\u001b[1;32m    597\u001b[0m                 warmup)\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/compiler/compiler.py:339\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m module_map \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_module_map()\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    341\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/ITMO/EDLM/venv/lib/python3.10/site-packages/triton/compiler/compiler.py:83\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, module_map, context)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, module_map, context):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ast_to_ttir\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 38:15:\n            mask=mask_a, other=0\n        )\n        b = tl.load(\n            B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn,\n            mask=mask_b, other=0\n        )\n\n        if USE_FP16:\n            a = a.to(tl.float16)\n            b = b.to(tl.float16)\n        # acc остаётся fp32\n        acc += tl.dot(a, b, allow_tf32=False)\n               ^"
     ]
    }
   ],
   "source": [
    "# ==== CHECK GRADS (один прогон) ====\n",
    "x_ref = x.detach().clone().requires_grad_(True)\n",
    "x_int = x.detach().clone().requires_grad_(True)\n",
    "\n",
    "conv_ref.zero_grad(set_to_none=True)\n",
    "conv_int8.zero_grad(set_to_none=True)\n",
    "\n",
    "y_ref = conv_ref(x_ref)\n",
    "y_int = conv_int8(x_int)\n",
    "\n",
    "loss_ref = y_ref.sum()\n",
    "loss_int = y_int.sum()\n",
    "\n",
    "loss_ref.backward()\n",
    "loss_int.backward()\n",
    "\n",
    "# градиенты по входу\n",
    "dx_err = (x_ref.grad - x_int.grad).abs()\n",
    "dx_err_max = dx_err.max().item()\n",
    "dx_err_mean = dx_err.mean().item()\n",
    "\n",
    "# градиенты по весам\n",
    "dw_err = (conv_ref.weight.grad - conv_int8.weight.grad).abs()\n",
    "dw_err_max = dw_err.max().item()\n",
    "dw_err_mean = dw_err.mean().item()\n",
    "\n",
    "print(\"\\n=== GRAD CHECK ===\")\n",
    "print(f\"dX  max err: {dx_err_max:.6e}, mean err: {dx_err_mean:.6e}\")\n",
    "print(f\"dW  max err: {dw_err_max:.6e}, mean err: {dw_err_mean:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687a022-76ca-4554-b6b5-05662a0b18a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
