{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66d6d69-8133-47b2-8635-f561077e9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import copy\n",
    "from conv_gemm.layers.triton_conv2d import TritonConv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cdbc3c-e349-4390-bcfe-fc76f43d3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "device = \"cuda\"\n",
    "\n",
    "# Базовая сеть (обычный ResNet18)\n",
    "base = resnet18(weights=None, num_classes=200).to(device)\n",
    "\n",
    "# Triton-версия (копируем веса и меняем все Conv2d на TritonConv2d)\n",
    "precision_mode   = \"fp32\"        # начнём без fp16, чтобы меньше дебага\n",
    "use_weight_shadow = False\n",
    "BEST_BLOCKS      = (32, 32, 32)\n",
    "BEST_LAUNCH      = (4, 2)\n",
    "\n",
    "tri = copy.deepcopy(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeca42a5-19a5-44b8-a936-ebdf09943b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triton_from_conv(\n",
    "    conv: nn.Conv2d,\n",
    "    *,\n",
    "    precision_mode: str = \"fp32\",\n",
    "    use_weight_shadow: bool = True,\n",
    "    triton_blocks: tuple[int, int, int] = (64, 64, 32),   # (BLOCK_M, BLOCK_N, BLOCK_K)\n",
    "    triton_launch: tuple[int, int] = (4, 2),              # (NUM_WARPS, NUM_STAGES)\n",
    ") -> TritonConv2d:\n",
    "    \"\"\"\n",
    "    Оборачивает один nn.Conv2d в TritonConv2d, копируя все гиперпараметры и веса.\n",
    "\n",
    "    conv            — исходная свёртка PyTorch.\n",
    "    precision_mode  — \"fp32\", \"fp16_runtime\" или \"fp16_infer\".\n",
    "    use_weight_shadow — использовать ли fp16-shadow для веса в режиме fp16_runtime.\n",
    "    triton_blocks   — размеры тайлов GEMM: (BLOCK_M, BLOCK_N, BLOCK_K).\n",
    "    triton_launch   — параметры запуска: (NUM_WARPS, NUM_STAGES).\n",
    "    \"\"\"\n",
    "    tm = TritonConv2d(\n",
    "        in_channels=conv.in_channels,\n",
    "        out_channels=conv.out_channels,\n",
    "        kernel_size=conv.kernel_size,\n",
    "        stride=conv.stride,\n",
    "        padding=conv.padding,\n",
    "        dilation=conv.dilation,\n",
    "        bias=(conv.bias is not None),\n",
    "        BLOCK_M=triton_blocks[0],\n",
    "        BLOCK_N=triton_blocks[1],\n",
    "        BLOCK_K=triton_blocks[2],\n",
    "        NUM_WARPS=triton_launch[0],\n",
    "        NUM_STAGES=triton_launch[1],\n",
    "        precision_mode=precision_mode,\n",
    "        use_weight_shadow=use_weight_shadow,\n",
    "    ).to(conv.weight.device).to(conv.weight.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tm.weight.copy_(conv.weight)\n",
    "        if conv.bias is not None and tm.bias is not None:\n",
    "            tm.bias.copy_(conv.bias)\n",
    "\n",
    "    return tm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cea6d52-45d6-4f4a-be62-78144f4a327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_convs_with_triton(\n",
    "    model: nn.Module,\n",
    "    *,\n",
    "    precision_mode: str = \"fp32\",\n",
    "    use_weight_shadow: bool = True,\n",
    "    triton_blocks: tuple[int, int, int] = (64, 64, 32),\n",
    "    triton_launch: tuple[int, int] = (4, 2),\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Рекурсивно заменяет все nn.Conv2d в модели на TritonConv2d.\n",
    "\n",
    "    Возвращает ту же модель (in-place), но с Triton-свёртками.\n",
    "    \"\"\"\n",
    "    for name, module in list(model.named_children()):\n",
    "        # сначала рекурсивно спускаемся внутрь\n",
    "        replace_convs_with_triton(\n",
    "            module,\n",
    "            precision_mode=precision_mode,\n",
    "            use_weight_shadow=use_weight_shadow,\n",
    "            triton_blocks=triton_blocks,\n",
    "            triton_launch=triton_launch,\n",
    "        )\n",
    "\n",
    "        # потом уже заменяем конкретный Conv2d на этом уровне\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            triton_conv = make_triton_from_conv(\n",
    "                module,\n",
    "                precision_mode=precision_mode,\n",
    "                use_weight_shadow=use_weight_shadow,\n",
    "                triton_blocks=triton_blocks,\n",
    "                triton_launch=triton_launch,\n",
    "            )\n",
    "            setattr(model, name, triton_conv)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b283290c-4b90-413b-9399-6b32c51ec0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tri = replace_all_convs_with_triton(\n",
    "    tri,\n",
    "    precision_mode=precision_mode,\n",
    "    use_weight_shadow=use_weight_shadow,\n",
    "    triton_blocks=BEST_BLOCKS,\n",
    "    triton_launch=BEST_LAUNCH,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d4994-ce2c-47fe-ba9a-a3a2f3450252",
   "metadata": {},
   "source": [
    "# БЕНЧМАРК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea74dcf5-6445-49cb-8960-47802052fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "def benchmark_one_step(\n",
    "    model: nn.Module,\n",
    "    B: int,\n",
    "    H: int,\n",
    "    W: int,\n",
    "    *,\n",
    "    device: str = \"cuda\",\n",
    "    num_classes: int = 200,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    iters_warmup: int = 2,\n",
    "    iters: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "    use_amp: bool = False,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Замеряет один training-step модели (forward + backward + step)\n",
    "    на фиксированном batch size.\n",
    "\n",
    "    Что делает:\n",
    "      - генерит случайный батч (x, y)\n",
    "      - прогревает модель `iters_warmup` шагов\n",
    "      - меряет среднее время по `iters` итерациям через cuda.Event\n",
    "      - возвращает:\n",
    "          avg_ms       — среднее время шага (ms)\n",
    "          peak_mem_mb  — пиковое GPU-потребление (MB) за всё время вызова\n",
    "          last_loss    — последний значение loss (чисто sanity-check)\n",
    "\n",
    "    Параметры:\n",
    "      model        — любая nn.Module (должна принимать [B, 3, H, W])\n",
    "      B, H, W      — batch size и spatial размер входа\n",
    "      device       — \"cuda\" или \"cpu\" (по факту нам нужен cuda для тайминга)\n",
    "      num_classes  — размерность выхода для CrossEntropyLoss\n",
    "      dtype        — тип входного тензора (обычно torch.float32)\n",
    "      iters_warmup — сколько итераций прогреть перед замером\n",
    "      iters        — по скольким итерациям усреднять время\n",
    "      lr           — learning rate для SGD\n",
    "      use_amp      — включать ли torch.cuda.amp.autocast\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x = torch.randn(B, 3, H, W, device=device, dtype=dtype)\n",
    "    y = torch.randint(0, num_classes, (B,), device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    scaler: Optional[torch.cuda.amp.GradScaler] = None\n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device=device)\n",
    "\n",
    "    # --- прогрев ---\n",
    "    for _ in range(iters_warmup):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "            assert scaler is not None\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # --- замер ---\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    total_ms = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for _ in range(iters):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        start_event.record()\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "            assert scaler is not None\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_event.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        iter_ms = start_event.elapsed_time(end_event)\n",
    "        total_ms += iter_ms\n",
    "        last_loss = float(loss.item())\n",
    "\n",
    "    avg_ms = total_ms / iters\n",
    "    peak_mem_bytes = torch.cuda.max_memory_allocated(device=device)\n",
    "    peak_mem_mb = peak_mem_bytes / (1024**2)\n",
    "\n",
    "    return {\n",
    "        \"avg_ms\": avg_ms,\n",
    "        \"peak_mem_mb\": peak_mem_mb,\n",
    "        \"last_loss\": last_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01fe1e26-f655-466c-b116-122c79253903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_batch_size(\n",
    "    model: nn.Module,\n",
    "    candidate_batches: List[int],\n",
    "    H: int,\n",
    "    W: int,\n",
    "    *,\n",
    "    num_classes: int = 200,\n",
    "    label: str = \"\",\n",
    "    benchmark_kwargs: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Подбирает максимальный batch size, который модель выдерживает\n",
    "    при полном шаге обучения (forward + backward + optimizer.step).\n",
    "\n",
    "    Что делает:\n",
    "      - для каждого B из candidate_batches:\n",
    "          * вызывает benchmark_one_step(model, B, H, W, ...)\n",
    "          * логирует время и пиковую память\n",
    "      - если падает по OOM / OutOfResources — помечает B как FAIL\n",
    "      - в конце выбирает 'best' как:\n",
    "          * максимальный B, который успешно прошёл\n",
    "\n",
    "    Параметры:\n",
    "      model             — модель (предполагается уже на нужном device)\n",
    "      candidate_batches — список batch size'ов, которые пробуем (можно в любом порядке)\n",
    "      H, W              — spatial размер входа (3 x H x W)\n",
    "      num_classes       — число классов (для CrossEntropy внутри benchmark_one_step)\n",
    "      label             — метка для логов (например, \"Torch ResNet18\" / \"Triton ResNet18\")\n",
    "      benchmark_kwargs  — доп. аргументы, которые будут проброшены в benchmark_one_step\n",
    "                          (например: {'use_amp': True})\n",
    "\n",
    "    Возвращает:\n",
    "      dict с полями:\n",
    "        - 'best_B'       — максимальный B, который прошёл (или None, если всё упало)\n",
    "        - 'best_stats'   — словарь с метриками для best_B (avg_ms, peak_mem_mb, last_loss)\n",
    "        - 'per_batch'    — список (B, status, stats_or_msg) по всем кандидатам\n",
    "    \"\"\"\n",
    "    if benchmark_kwargs is None:\n",
    "        benchmark_kwargs = {}\n",
    "\n",
    "    print(f\"\\n=== Finding max batch size for {label} ===\")\n",
    "    per_batch = []\n",
    "    best_B: Optional[int] = None\n",
    "    best_stats: Optional[Dict[str, float]] = None\n",
    "\n",
    "    for B in candidate_batches:\n",
    "        print(f\"\\n-- Try B = {B} --\")\n",
    "        try:\n",
    "            res = benchmark_one_step(\n",
    "                model=model,\n",
    "                B=B,\n",
    "                H=H,\n",
    "                W=W,\n",
    "                num_classes=num_classes,\n",
    "                **benchmark_kwargs,\n",
    "            )\n",
    "            print(f\"OK: B={B}, avg_time={res['avg_ms']:.2f} ms, \"\n",
    "                  f\"peak_mem={res['peak_mem_mb']:.1f} MB, \"\n",
    "                  f\"loss={res['last_loss']:.4f}\")\n",
    "\n",
    "            per_batch.append((B, \"OK\", res))\n",
    "\n",
    "            # выбираем максимальный B, который прошёл\n",
    "            if (best_B is None) or (B > best_B):\n",
    "                best_B = B\n",
    "                best_stats = res\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            if (\"out of memory\" in msg.lower()\n",
    "                or \"outofresources\" in msg.lower()\n",
    "                or \"out of resource\" in msg.lower()):\n",
    "                print(f\"FAIL (OOM / OutOfResources) at B={B}\")\n",
    "                per_batch.append((B, \"OOM\", msg))\n",
    "            else:\n",
    "                print(f\"FAIL other error at B={B}: {msg}\")\n",
    "                per_batch.append((B, \"ERR\", msg))\n",
    "\n",
    "    print(\"\\n=== Best that worked for\", label, \"===\")\n",
    "    if best_B is None:\n",
    "        print(\"Nothing worked at all (даже минимальный батч)\")\n",
    "    else:\n",
    "        print(f\"B={best_B}, avg_time={best_stats['avg_ms']:.2f} ms, \"\n",
    "              f\"peak_mem={best_stats['peak_mem_mb']:.1f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"best_B\": best_B,\n",
    "        \"best_stats\": best_stats,\n",
    "        \"per_batch\": per_batch,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51587559-7b11-43c5-b6b1-9a5816968480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Finding max batch size for Torch ResNet18 ===\n",
      "\n",
      "-- Try B = 256 --\n",
      "FAIL (OOM / OutOfResources) at B=256\n",
      "\n",
      "-- Try B = 192 --\n",
      "OK: B=192, avg_time=219.36 ms, peak_mem=4438.6 MB, loss=5.3775\n",
      "\n",
      "-- Try B = 128 --\n",
      "OK: B=128, avg_time=149.69 ms, peak_mem=3032.8 MB, loss=5.3628\n",
      "\n",
      "-- Try B = 96 --\n",
      "OK: B=96, avg_time=117.99 ms, peak_mem=2336.5 MB, loss=5.2413\n",
      "\n",
      "-- Try B = 64 --\n",
      "OK: B=64, avg_time=78.59 ms, peak_mem=1642.9 MB, loss=5.3118\n",
      "\n",
      "-- Try B = 48 --\n",
      "OK: B=48, avg_time=62.58 ms, peak_mem=1290.3 MB, loss=5.1061\n",
      "\n",
      "-- Try B = 32 --\n",
      "OK: B=32, avg_time=45.63 ms, peak_mem=946.1 MB, loss=5.0791\n",
      "\n",
      "-- Try B = 24 --\n",
      "OK: B=24, avg_time=34.13 ms, peak_mem=776.0 MB, loss=4.9017\n",
      "\n",
      "-- Try B = 16 --\n",
      "OK: B=16, avg_time=23.95 ms, peak_mem=604.8 MB, loss=4.8180\n",
      "\n",
      "-- Try B = 8 --\n",
      "OK: B=8, avg_time=12.95 ms, peak_mem=434.1 MB, loss=4.7043\n",
      "\n",
      "-- Try B = 4 --\n",
      "OK: B=4, avg_time=8.62 ms, peak_mem=347.5 MB, loss=3.8065\n",
      "\n",
      "=== Best that worked for Torch ResNet18 ===\n",
      "B=192, avg_time=219.36 ms, peak_mem=4438.6 MB\n",
      "\n",
      "=== Finding max batch size for Triton ResNet18 ===\n",
      "\n",
      "-- Try B = 256 --\n",
      "FAIL (OOM / OutOfResources) at B=256\n",
      "\n",
      "-- Try B = 192 --\n",
      "FAIL (OOM / OutOfResources) at B=192\n",
      "\n",
      "-- Try B = 128 --\n",
      "FAIL (OOM / OutOfResources) at B=128\n",
      "\n",
      "-- Try B = 96 --\n",
      "OK: B=96, avg_time=547.65 ms, peak_mem=3870.1 MB, loss=5.2793\n",
      "\n",
      "-- Try B = 64 --\n",
      "OK: B=64, avg_time=353.52 ms, peak_mem=2674.4 MB, loss=5.2641\n",
      "\n",
      "-- Try B = 48 --\n",
      "OK: B=48, avg_time=257.74 ms, peak_mem=2073.6 MB, loss=5.1577\n",
      "\n",
      "-- Try B = 32 --\n",
      "OK: B=32, avg_time=166.69 ms, peak_mem=1477.9 MB, loss=5.2182\n",
      "\n",
      "-- Try B = 24 --\n",
      "OK: B=24, avg_time=125.38 ms, peak_mem=1180.9 MB, loss=5.1100\n",
      "\n",
      "-- Try B = 16 --\n",
      "OK: B=16, avg_time=78.75 ms, peak_mem=880.5 MB, loss=4.7918\n",
      "\n",
      "-- Try B = 8 --\n",
      "OK: B=8, avg_time=34.70 ms, peak_mem=584.2 MB, loss=4.1619\n",
      "\n",
      "-- Try B = 4 --\n",
      "OK: B=4, avg_time=18.04 ms, peak_mem=432.6 MB, loss=3.8625\n",
      "\n",
      "=== Best that worked for Triton ResNet18 ===\n",
      "B=96, avg_time=547.65 ms, peak_mem=3870.1 MB\n"
     ]
    }
   ],
   "source": [
    "H = 224\n",
    "W = 224\n",
    "candidate_batches = [256, 192, 128, 96, 64, 48, 32, 24, 16, 8, 4]\n",
    "\n",
    "# Для честности — копии моделей, чтобы оптимизатор не вмешивался между прогонками\n",
    "base_for_test = copy.deepcopy(base).to(device)\n",
    "tri_for_test  = copy.deepcopy(tri).to(device)\n",
    "\n",
    "best_base = find_max_batch_size(\n",
    "    model=base_for_test,\n",
    "    candidate_batches=candidate_batches,\n",
    "    H=H,\n",
    "    W=W,\n",
    "    num_classes=200,\n",
    "    label=\"Torch ResNet18\",\n",
    ")\n",
    "\n",
    "best_tri = find_max_batch_size(\n",
    "    model=tri_for_test,\n",
    "    candidate_batches=candidate_batches,\n",
    "    H=H,\n",
    "    W=W,\n",
    "    num_classes=200,\n",
    "    label=\"Triton ResNet18\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30215425-bf6b-42c5-8904-85f4c5bcf8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_TORCH_MAX = 192\n",
    "B_TRITON_MAX = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e0dcd-ae57-4168-b649-95c10ce7082e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe8893-ad58-436b-b739-4f267397d57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d2576-9a26-45d0-8382-5362f4599177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bd8fe-e753-43c8-941e-99aa45daf704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b55dd-894f-442f-9cd7-a5d07d656390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89286ac0-2a04-4aea-9088-9e77fee6fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def bench(fn, warmup=10, iters=100):\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter() - t0) / iters\n",
    "\n",
    "\n",
    "def make_pair(\n",
    "    in_channels=64,\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    dilation=1,\n",
    "    bias=True,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    conv_ref = nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        stride=stride, padding=padding, dilation=dilation, bias=bias\n",
    "    ).to(device)\n",
    "\n",
    "    triton_conv = TritonConv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        stride=stride, padding=padding, dilation=dilation, bias=bias,\n",
    "        BLOCK_M=32, BLOCK_N=32, BLOCK_K=32,\n",
    "        NUM_WARPS=4, NUM_STAGES=2,\n",
    "        precision_mode=\"fp32\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        triton_conv.weight.copy_(conv_ref.weight)\n",
    "        if conv_ref.bias is not None and triton_conv.bias is not None:\n",
    "            triton_conv.bias.copy_(conv_ref.bias)\n",
    "\n",
    "    return conv_ref, triton_conv\n",
    "\n",
    "\n",
    "def run_triton_vs_conv_bench():\n",
    "    device = \"cuda\"\n",
    "\n",
    "    N, Cin, Cout = 32, 64, 64\n",
    "    H, W = 56, 56\n",
    "    ks = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    dilation = 1\n",
    "\n",
    "    x_fp32 = torch.randn(N, Cin, H, W, device=device)\n",
    "\n",
    "    conv_ref_fp32, triton_conv = make_pair(\n",
    "        in_channels=Cin,\n",
    "        out_channels=Cout,\n",
    "        kernel_size=ks,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        dilation=dilation,\n",
    "        bias=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # =========================================================\n",
    "    # FP32\n",
    "    # =========================================================\n",
    "    print(\"=== FP32 режим ===\")\n",
    "    triton_conv.set_precision(\"fp32\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_ref = conv_ref_fp32(x_fp32)\n",
    "        y_triton = triton_conv(x_fp32)\n",
    "\n",
    "    max_err = (y_ref - y_triton).abs().max().item()\n",
    "    print(f\"fp32 max_err = {max_err:.6e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_ref = bench(lambda: conv_ref_fp32(x_fp32))\n",
    "        t_tr = bench(lambda: triton_conv(x_fp32))\n",
    "\n",
    "    print(f\"Conv2d (cuDNN, fp32):    {t_ref*1e3:.3f} ms\")\n",
    "    print(f\"TritonConv2d (fp32):     {t_tr*1e3:.3f} ms\")\n",
    "    print(f\"speedup (Triton/Conv2d): {t_ref/t_tr:.3f}x\\n\")\n",
    "\n",
    "    # =========================================================\n",
    "    # FP16 INFER — отдельная копия conv_ref\n",
    "    # =========================================================\n",
    "    print(\"=== FP16 INFER режим (вся сеть в half) ===\")\n",
    "    triton_conv.set_precision(\"fp16_infer\")\n",
    "\n",
    "    x_fp16 = x_fp32.half()\n",
    "\n",
    "    # делаем deepcopy, чтобы не портить conv_ref_fp32\n",
    "    conv_ref_fp16 = copy.deepcopy(conv_ref_fp32).half()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_ref16 = conv_ref_fp16(x_fp16)      # half\n",
    "        y_triton16 = triton_conv(x_fp16)     # half\n",
    "\n",
    "    max_err16 = (y_ref16.float() - y_triton16.float()).abs().max().item()\n",
    "    print(f\"fp16_infer max_err = {max_err16:.6e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_ref16 = bench(lambda: conv_ref_fp16(x_fp16))\n",
    "        t_tr16 = bench(lambda: triton_conv(x_fp16))\n",
    "\n",
    "    print(f\"Conv2d (cuDNN, fp16):       {t_ref16*1e3:.3f} ms\")\n",
    "    print(f\"TritonConv2d (fp16_infer):  {t_tr16*1e3:.3f} ms\")\n",
    "    print(f\"speedup (Triton/Conv2d):    {t_ref16/t_tr16:.3f}x\\n\")\n",
    "\n",
    "    # =========================================================\n",
    "    # FP16 RUNTIME — опять используем исходный fp32 conv\n",
    "    # =========================================================\n",
    "    print(\"=== FP16 RUNTIME режим (вход fp32, внутри half, наружу fp32) ===\")\n",
    "    triton_conv.set_precision(\"fp16_runtime\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_ref_rt = conv_ref_fp32(x_fp32)        # fp32 эталон\n",
    "        y_triton_rt = triton_conv(x_fp32)       # внутри half, наружу fp32\n",
    "\n",
    "    max_err_rt = (y_ref_rt - y_triton_rt).abs().max().item()\n",
    "    print(f\"fp16_runtime max_err = {max_err_rt:.6e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_ref_rt = bench(lambda: conv_ref_fp32(x_fp32))\n",
    "        t_tr_rt = bench(lambda: triton_conv(x_fp32))\n",
    "\n",
    "    print(f\"Conv2d (cuDNN, fp32):         {t_ref_rt*1e3:.3f} ms\")\n",
    "    print(f\"TritonConv2d (fp16_runtime):  {t_tr_rt*1e3:.3f} ms\")\n",
    "    print(f\"speedup (Triton/Conv2d):      {t_ref_rt/t_tr_rt:.3f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3957ff6a-c463-4fe6-9dca-70941937dce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FP32 режим ===\n",
      "fp32 max_err = 4.053116e-06\n",
      "Conv2d (cuDNN, fp32):    0.717 ms\n",
      "TritonConv2d (fp32):     1.990 ms\n",
      "speedup (Triton/Conv2d): 0.360x\n",
      "\n",
      "=== FP16 INFER режим (вся сеть в half) ===\n",
      "fp16_infer max_err = 1.953125e-03\n",
      "Conv2d (cuDNN, fp16):       0.421 ms\n",
      "TritonConv2d (fp16_infer):  1.706 ms\n",
      "speedup (Triton/Conv2d):    0.247x\n",
      "\n",
      "=== FP16 RUNTIME режим (вход fp32, внутри half, наружу fp32) ===\n",
      "fp16_runtime max_err = 1.393557e-03\n",
      "Conv2d (cuDNN, fp32):         0.701 ms\n",
      "TritonConv2d (fp16_runtime):  1.875 ms\n",
      "speedup (Triton/Conv2d):      0.374x\n"
     ]
    }
   ],
   "source": [
    "run_triton_vs_conv_bench()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5537d-97a4-4119-a8e3-e77b03cd9247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86df9896-31f9-44e1-b103-4ebbf28c9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_gemm.layers.triton_conv2d import TritonConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c66a0c1-ac7b-4f0c-b394-b11fae589a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: img=1024, N=1, Cin=1, Cout=1, ks=1\n",
      "DONE: img=1024, N=1, Cin=1, Cout=1, ks=3\n",
      "DONE: img=1024, N=1, Cin=1, Cout=1, ks=7\n",
      "DONE: img=1024, N=1, Cin=1, Cout=1, ks=9\n",
      "DONE: img=1024, N=1, Cin=1, Cout=1, ks=11\n",
      "DONE: img=1024, N=1, Cin=1, Cout=3, ks=1\n",
      "DONE: img=1024, N=1, Cin=1, Cout=3, ks=3\n",
      "DONE: img=1024, N=1, Cin=1, Cout=3, ks=7\n",
      "DONE: img=1024, N=1, Cin=1, Cout=3, ks=9\n",
      "DONE: img=1024, N=1, Cin=1, Cout=3, ks=11\n",
      "DONE: img=1024, N=1, Cin=3, Cout=8, ks=1\n",
      "DONE: img=1024, N=1, Cin=3, Cout=8, ks=3\n",
      "DONE: img=1024, N=1, Cin=3, Cout=8, ks=7\n",
      "DONE: img=1024, N=1, Cin=3, Cout=8, ks=9\n",
      "DONE: img=1024, N=1, Cin=3, Cout=8, ks=11\n",
      "DONE: img=1024, N=1, Cin=32, Cout=64, ks=1\n",
      "DONE: img=1024, N=1, Cin=32, Cout=64, ks=3\n",
      "DONE: img=1024, N=1, Cin=32, Cout=64, ks=7\n",
      "DONE: img=1024, N=1, Cin=32, Cout=64, ks=9\n",
      "DONE: img=1024, N=1, Cin=32, Cout=64, ks=11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>N</th>\n",
       "      <th>Cin</th>\n",
       "      <th>Cout</th>\n",
       "      <th>K</th>\n",
       "      <th>cuDNN FP16 ms</th>\n",
       "      <th>cuDNN FP32 ms</th>\n",
       "      <th>Triton FP16 ms</th>\n",
       "      <th>Triton INT8 ms</th>\n",
       "      <th>Speed FP16</th>\n",
       "      <th>Speed INT8</th>\n",
       "      <th>Err FP16</th>\n",
       "      <th>Err INT8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.200</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.427x</td>\n",
       "      <td>-</td>\n",
       "      <td>9.77e-04</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.570</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.174x</td>\n",
       "      <td>-</td>\n",
       "      <td>9.77e-04</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.000</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.144x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.415</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.138x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.272</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.931</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.141x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.509</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.232x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.935</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.216x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.624</td>\n",
       "      <td>1.417</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.282x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.499</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.843</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.271x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.788</td>\n",
       "      <td>1.372</td>\n",
       "      <td>2.415</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.326x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.328</td>\n",
       "      <td>1.321</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.205x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.516</td>\n",
       "      <td>1.497</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.256x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1.661</td>\n",
       "      <td>1.714</td>\n",
       "      <td>3.501</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.475x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2.535</td>\n",
       "      <td>3.794</td>\n",
       "      <td>5.068</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.500x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>3.642</td>\n",
       "      <td>5.754</td>\n",
       "      <td>7.868</td>\n",
       "      <td>UNSUP</td>\n",
       "      <td>0.463x</td>\n",
       "      <td>-</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1.242</td>\n",
       "      <td>3.132</td>\n",
       "      <td>9.127</td>\n",
       "      <td>14.773</td>\n",
       "      <td>0.136x</td>\n",
       "      <td>0.084x</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>4.77e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>2.824</td>\n",
       "      <td>4.446</td>\n",
       "      <td>12.258</td>\n",
       "      <td>17.462</td>\n",
       "      <td>0.230x</td>\n",
       "      <td>0.162x</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>4.47e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>7.069</td>\n",
       "      <td>19.759</td>\n",
       "      <td>36.097</td>\n",
       "      <td>41.899</td>\n",
       "      <td>0.196x</td>\n",
       "      <td>0.169x</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>4.35e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OOM</td>\n",
       "      <td>OOM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OOM</td>\n",
       "      <td>OOM</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     img  N  Cin  Cout   K cuDNN FP16 ms cuDNN FP32 ms Triton FP16 ms  \\\n",
       "0   1024  1    1     1   1         0.085         0.063          0.200   \n",
       "1   1024  1    1     1   3         0.099         0.119          0.570   \n",
       "2   1024  1    1     1   7         0.144         0.142          1.000   \n",
       "3   1024  1    1     1   9         0.195         1.002          1.415   \n",
       "4   1024  1    1     1  11         0.272         1.330          1.931   \n",
       "5   1024  1    1     3   1         0.118         0.167          0.509   \n",
       "6   1024  1    1     3   3         0.202         0.325          0.935   \n",
       "7   1024  1    1     3   7         0.399         0.624          1.417   \n",
       "8   1024  1    1     3   9         0.499         1.033          1.843   \n",
       "9   1024  1    1     3  11         0.788         1.372          2.415   \n",
       "10  1024  1    3     8   1         0.271         0.328          1.321   \n",
       "11  1024  1    3     8   3         0.383         0.516          1.497   \n",
       "12  1024  1    3     8   7         1.661         1.714          3.501   \n",
       "13  1024  1    3     8   9         2.535         3.794          5.068   \n",
       "14  1024  1    3     8  11         3.642         5.754          7.868   \n",
       "15  1024  1   32    64   1         1.242         3.132          9.127   \n",
       "16  1024  1   32    64   3         2.824         4.446         12.258   \n",
       "17  1024  1   32    64   7         7.069        19.759         36.097   \n",
       "18  1024  1   32    64   9            OK            OK            OOM   \n",
       "19  1024  1   32    64  11            OK            OK            OOM   \n",
       "\n",
       "   Triton INT8 ms Speed FP16 Speed INT8  Err FP16  Err INT8  \n",
       "0           UNSUP     0.427x          -  9.77e-04         -  \n",
       "1           UNSUP     0.174x          -  9.77e-04         -  \n",
       "2           UNSUP     0.144x          -  1.95e-03         -  \n",
       "3           UNSUP     0.138x          -  1.95e-03         -  \n",
       "4           UNSUP     0.141x          -  1.95e-03         -  \n",
       "5           UNSUP     0.232x          -  1.95e-03         -  \n",
       "6           UNSUP     0.216x          -  1.95e-03         -  \n",
       "7           UNSUP     0.282x          -  1.95e-03         -  \n",
       "8           UNSUP     0.271x          -  1.95e-03         -  \n",
       "9           UNSUP     0.326x          -  1.95e-03         -  \n",
       "10          UNSUP     0.205x          -  1.95e-03         -  \n",
       "11          UNSUP     0.256x          -  1.95e-03         -  \n",
       "12          UNSUP     0.475x          -  1.95e-03         -  \n",
       "13          UNSUP     0.500x          -  1.95e-03         -  \n",
       "14          UNSUP     0.463x          -  1.95e-03         -  \n",
       "15         14.773     0.136x     0.084x  1.95e-03  4.77e-02  \n",
       "16         17.462     0.230x     0.162x  1.95e-03  4.47e-02  \n",
       "17         41.899     0.196x     0.169x  1.95e-03  4.35e-02  \n",
       "18            OOM          -          -         -         -  \n",
       "19            OOM          -          -         -         -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from conv_gemm.layers.triton_conv2d import TritonConv2d\n",
    "from conv_gemm.layers.triton_conv2d_int8 import TritonConv2dInt8\n",
    "\n",
    "\n",
    "def bench_mean(fn, iters: int = 50) -> float:\n",
    "    \"\"\"Среднее время выполнения fn() за iters прогонов (в секундах).\"\"\"\n",
    "    # небольшой прогрев\n",
    "    for _ in range(5):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter() - t0) / iters\n",
    "\n",
    "\n",
    "def safe_forward(fn):\n",
    "    \"\"\"Выполнить forward и аккуратно поймать OOM/другие RuntimeError.\"\"\"\n",
    "    try:\n",
    "        out = fn()\n",
    "        return out, None\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e).lower()\n",
    "        if \"out of memory\" in msg:\n",
    "            torch.cuda.empty_cache()\n",
    "            return None, \"OOM\"\n",
    "        return None, \"ERR\"\n",
    "\n",
    "\n",
    "def show_results_as_table(results):\n",
    "    cols = [\n",
    "        \"img\", \"N\", \"Cin\", \"Cout\", \"K\",\n",
    "        \"cuDNN FP16 ms\", \"cuDNN FP32 ms\",\n",
    "        \"Triton FP16 ms\", \"Triton INT8 ms\",\n",
    "        \"Speed FP16\", \"Speed INT8\",\n",
    "        \"Err FP16\", \"Err INT8\",\n",
    "    ]\n",
    "    df = pd.DataFrame(results, columns=cols)\n",
    "    display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_full_conv_bench_fp16_int8(\n",
    "    image_sizes=(224, 112, 56),\n",
    "    batch_sizes=(1, 2, 4, 8),\n",
    "    channels=((3, 32), (32, 32), (32, 64), (64, 64), (64, 128)),\n",
    "    kernels=(3, 5, 7, 9),\n",
    "    stride=1,\n",
    "    iters=200,\n",
    "):\n",
    "    device = \"cuda\"\n",
    "    results = []\n",
    "\n",
    "    for img in image_sizes:\n",
    "        H = W = img\n",
    "\n",
    "        for N in batch_sizes:\n",
    "            for Cin, Cout in channels:\n",
    "                for ks in kernels:\n",
    "                    print(f\"DONE: img={img}, N={N}, Cin={Cin}, Cout={Cout}, ks={ks}\")\n",
    "\n",
    "                    x_fp16 = torch.randn(\n",
    "                        N, Cin, H, W, device=device, dtype=torch.float16\n",
    "                    )\n",
    "\n",
    "                    # === БАЗОВЫЕ свёртки: cuDNN FP32 + cuDNN FP16 ===\n",
    "                    conv_ref_fp32 = nn.Conv2d(\n",
    "                        Cin, Cout, ks,\n",
    "                        stride=stride, padding=ks // 2, dilation=1, bias=True,\n",
    "                    ).to(device)          # fp32\n",
    "                    conv_ref_fp32.eval()\n",
    "\n",
    "                    conv_ref_fp16 = nn.Conv2d(\n",
    "                        Cin, Cout, ks,\n",
    "                        stride=stride, padding=ks // 2, dilation=1, bias=True,\n",
    "                    ).to(device).half()   # fp16\n",
    "                    conv_ref_fp16.eval()\n",
    "\n",
    "                    # одинаковые веса в обоих референсах\n",
    "                    with torch.no_grad():\n",
    "                        conv_ref_fp16.weight.copy_(conv_ref_fp32.weight.half())\n",
    "                        if conv_ref_fp32.bias is not None:\n",
    "                            conv_ref_fp16.bias.copy_(conv_ref_fp32.bias.half())\n",
    "\n",
    "                    # === Triton FP16 ===\n",
    "                    triton_conv_fp16 = TritonConv2d(\n",
    "                        Cin, Cout, ks,\n",
    "                        stride=stride, padding=ks // 2, dilation=1, bias=True,\n",
    "                        BLOCK_M=64, BLOCK_N=64, BLOCK_K=32,\n",
    "                        NUM_WARPS=4, NUM_STAGES=2,\n",
    "                        precision_mode=\"fp16_infer\",  # вход/веса half, выход half\n",
    "                    ).to(device)\n",
    "                    triton_conv_fp16.eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        triton_conv_fp16.weight.copy_(conv_ref_fp16.weight)\n",
    "                        if triton_conv_fp16.bias is not None:\n",
    "                            triton_conv_fp16.bias.copy_(conv_ref_fp16.bias)\n",
    "\n",
    "                    # ---------- FORWARD / ERRORS ----------\n",
    "\n",
    "                    # cuDNN FP16 baseline (этот выход используем как \"истину\")\n",
    "                    y_ref, err_ref = safe_forward(lambda: conv_ref_fp16(x_fp16))\n",
    "                    if err_ref:\n",
    "                        results.append((\n",
    "                            img, N, Cin, Cout, ks,\n",
    "                            \"OOM\",      # cuDNN FP16 ms\n",
    "                            \"OOM\",      # cuDNN FP32 ms\n",
    "                            \"OOM\",      # Triton FP16 ms\n",
    "                            \"OOM\",      # Triton INT8 ms\n",
    "                            \"-\", \"-\", \"-\", \"-\",\n",
    "                        ))\n",
    "                        continue\n",
    "\n",
    "                    # Triton FP16\n",
    "                    y_tri_fp16, err_tri_fp16 = safe_forward(\n",
    "                        lambda: triton_conv_fp16(x_fp16)\n",
    "                    )\n",
    "                    if err_tri_fp16:\n",
    "                        results.append((\n",
    "                            img, N, Cin, Cout, ks,\n",
    "                            \"OK\",       # cuDNN FP16 живой, но Triton упал\n",
    "                            \"OK\",\n",
    "                            \"OOM\",\n",
    "                            \"OOM\",\n",
    "                            \"-\", \"-\", \"-\", \"-\",\n",
    "                        ))\n",
    "                        continue\n",
    "\n",
    "                    # Можно ли вообще делать INT8? (K кратно 4)\n",
    "                    K = Cin * ks * ks\n",
    "                    supports_int8 = (K % 4 == 0)\n",
    "\n",
    "                    # ---------- Ветвь: INT8 не поддержан (UNSUP) ----------\n",
    "                    if not supports_int8:\n",
    "                        err_fp16 = (y_ref - y_tri_fp16).float().abs().max().item()\n",
    "\n",
    "                        t_ref_fp16 = bench_mean(lambda: conv_ref_fp16(x_fp16), iters)\n",
    "                        t_ref_fp32 = bench_mean(lambda: conv_ref_fp32(x_fp16.float()), iters)\n",
    "                        t_tri_fp16 = bench_mean(lambda: triton_conv_fp16(x_fp16), iters)\n",
    "\n",
    "                        speed_fp16 = t_ref_fp16 / t_tri_fp16\n",
    "\n",
    "                        results.append((\n",
    "                            img, N, Cin, Cout, ks,\n",
    "                            f\"{t_ref_fp16 * 1e3:.3f}\",   # cuDNN FP16 ms\n",
    "                            f\"{t_ref_fp32 * 1e3:.3f}\",   # cuDNN FP32 ms\n",
    "                            f\"{t_tri_fp16 * 1e3:.3f}\",   # Triton FP16 ms\n",
    "                            \"UNSUP\",                     # Triton INT8 ms\n",
    "                            f\"{speed_fp16:.3f}x\",\n",
    "                            \"-\",\n",
    "                            f\"{err_fp16:.2e}\",\n",
    "                            \"-\",\n",
    "                        ))\n",
    "                        continue\n",
    "\n",
    "                    # === Triton INT8 ===\n",
    "                    triton_conv_int8 = TritonConv2dInt8(\n",
    "                        Cin, Cout, ks,\n",
    "                        stride=stride, padding=ks // 2, dilation=1, bias=True,\n",
    "                    ).to(device)\n",
    "                    triton_conv_int8.eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        triton_conv_int8.init_from_conv(conv_ref_fp16)\n",
    "\n",
    "                    y_tri_int8, err_tri_int8 = safe_forward(\n",
    "                        lambda: triton_conv_int8(x_fp16)\n",
    "                    )\n",
    "\n",
    "                    # ---------- Ветвь: INT8 упал (ERR) ----------\n",
    "                    if err_tri_int8:\n",
    "                        err_fp16 = (y_ref - y_tri_fp16).float().abs().max().item()\n",
    "\n",
    "                        t_ref_fp16 = bench_mean(lambda: conv_ref_fp16(x_fp16), iters)\n",
    "                        t_ref_fp32 = bench_mean(lambda: conv_ref_fp32(x_fp16.float()), iters)\n",
    "                        t_tri_fp16 = bench_mean(lambda: triton_conv_fp16(x_fp16), iters)\n",
    "\n",
    "                        speed_fp16 = t_ref_fp16 / t_tri_fp16\n",
    "\n",
    "                        results.append((\n",
    "                            img, N, Cin, Cout, ks,\n",
    "                            f\"{t_ref_fp16 * 1e3:.3f}\",\n",
    "                            f\"{t_ref_fp32 * 1e3:.3f}\",\n",
    "                            f\"{t_tri_fp16 * 1e3:.3f}\",\n",
    "                            \"ERR\",\n",
    "                            f\"{speed_fp16:.3f}x\",\n",
    "                            \"-\",\n",
    "                            f\"{err_fp16:.2e}\",\n",
    "                            \"-\",\n",
    "                        ))\n",
    "                        continue\n",
    "\n",
    "                    # ---------- Основная ветка: всё отработало ----------\n",
    "\n",
    "                    err_fp16 = (y_ref - y_tri_fp16).float().abs().max().item()\n",
    "                    err_int8 = (y_ref - y_tri_int8).float().abs().max().item()\n",
    "\n",
    "                    t_ref_fp16 = bench_mean(lambda: conv_ref_fp16(x_fp16), iters)\n",
    "                    t_ref_fp32 = bench_mean(lambda: conv_ref_fp32(x_fp16.float()), iters)\n",
    "                    t_tri_fp16 = bench_mean(lambda: triton_conv_fp16(x_fp16), iters)\n",
    "                    t_tri_int8 = bench_mean(lambda: triton_conv_int8(x_fp16), iters)\n",
    "\n",
    "                    speed_fp16 = t_ref_fp16 / t_tri_fp16\n",
    "                    speed_int8 = t_ref_fp16 / t_tri_int8\n",
    "\n",
    "                    results.append((\n",
    "                        img, N, Cin, Cout, ks,\n",
    "                        f\"{t_ref_fp16 * 1e3:.3f}\",   # cuDNN FP16 ms\n",
    "                        f\"{t_ref_fp32 * 1e3:.3f}\",   # cuDNN FP32 ms\n",
    "                        f\"{t_tri_fp16 * 1e3:.3f}\",   # Triton FP16 ms\n",
    "                        f\"{t_tri_int8 * 1e3:.3f}\",   # Triton INT8 ms\n",
    "                        f\"{speed_fp16:.3f}x\",\n",
    "                        f\"{speed_int8:.3f}x\",\n",
    "                        f\"{err_fp16:.2e}\",\n",
    "                        f\"{err_int8:.2e}\",\n",
    "                    ))\n",
    "\n",
    "    return show_results_as_table(results)\n",
    "\n",
    "\n",
    "# пример вызова\n",
    "df = run_full_conv_bench_fp16_int8(\n",
    "    image_sizes=[1024],\n",
    "    # image_sizes=[224, 112, 56],\n",
    "    batch_sizes=[1, ],\n",
    "    # batch_sizes=[1, 2, 4, 8],\n",
    "    channels=[(1, 1),(1, 3), (3, 8), (32, 64)],\n",
    "    # channels=[(1, 3), (3, 16), (32, 64), (64, 64), (64, 128)],\n",
    "    kernels=[1, 3, 7, 9, 11],\n",
    "    # kernels=[3, 5, 7, 9],\n",
    "    stride=1,\n",
    "    iters=300,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d33d88-1f46-4dac-8684-88d43801c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "from IPython.display import display\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6c415-a1eb-4685-94bd-305dca92f253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d410bf-0e6c-4323-a367-2802059c0fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15254e47-8cc5-40a6-9c70-f63d02df5913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766b7a1-bfb5-43de-9388-28306cb9d49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2e862-6954-4791-a328-d9b6ab72913d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8ca8f-3b27-4e95-bfa2-e0feaff55079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632596d-636b-4fc3-8841-1f03e2157d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f709f7-ca84-4e85-98c8-535c8e17620f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
